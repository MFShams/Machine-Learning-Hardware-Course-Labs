{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHzdt0hGIajB"
      },
      "source": [
        "# Lab 4: Machine Learning Hardware Optimization - Part 1\n",
        "## Machine Learning Hardware Course\n",
        "\n",
        "This notebook is Part 1 of the Machine Learning Hardware Optimization lab. It covers:\n",
        "1. Environment setup\n",
        "2. Model and dataset preparation\n",
        "3. Hardware performance benchmarking\n",
        "\n",
        "The lab will explore how neural network models perform across different hardware platforms and how they can be optimized for specific deployment scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgAzgYbAIajE"
      },
      "source": [
        "## PART 1: ENVIRONMENT SETUP\n",
        "\n",
        "First, we'll set up our environment by importing necessary libraries and checking available hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBG0nthSIajF"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist, cifar10\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Input\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "# Check TensorFlow version and available hardware\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Eager execution enabled:\", tf.executing_eagerly())\n",
        "print(\"GPU available:\", bool(tf.config.list_physical_devices('GPU')))\n",
        "#print(\"TPU available:\", bool(tf.distribute.cluster_resolver.TPUClusterResolver()))\n",
        "\n",
        "# Try to detect and configure TPU if available\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except:\n",
        "    # No TPU detected, use default strategy\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"Distribution strategy:\", strategy.__class__.__name__)\n",
        "print(\"Number of replicas:\", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukHepYb4IajG"
      },
      "outputs": [],
      "source": [
        "# Install TensorFlow Model Optimization Toolkit if not already installed\n",
        "try:\n",
        "    import tensorflow_model_optimization as tfmot\n",
        "    print(\"TensorFlow Model Optimization Toolkit version:\", tfmot.__version__)\n",
        "except ImportError:\n",
        "    !pip install -q tensorflow-model-optimization\n",
        "    import tensorflow_model_optimization as tfmot\n",
        "    print(\"Installed TensorFlow Model Optimization Toolkit version:\", tfmot.__version__)\n",
        "\n",
        "# Mount Google Drive (optional, for saving results)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !mkdir -p \"/content/drive/My Drive/ML_Hardware_Course/Lab4\"\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVGiG0ukIajH"
      },
      "source": [
        "## PART 2: MODEL AND DATASET PREPARATION\n",
        "\n",
        "Now we'll create our model architectures and prepare the MNIST dataset for training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z6l_Rw3cIajH"
      },
      "outputs": [],
      "source": [
        "def create_cnn_model(input_shape, num_classes):\n",
        "    \"\"\"Create a simple CNN model for classification.\"\"\"\n",
        "    model = Sequential([\n",
        "        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def create_fcnn_model(input_shape, num_classes):\n",
        "    \"\"\"Create a simple fully connected neural network model for classification.\"\"\"\n",
        "    input_dim = np.prod(input_shape)  # Flatten the input shape\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=input_shape),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def load_and_prepare_mnist():\n",
        "    \"\"\"Load and prepare MNIST dataset.\"\"\"\n",
        "    # Load MNIST dataset\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "    # Reshape for CNN (add channel dimension)\n",
        "    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "    # One-hot encode labels\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln7wt7laIajI"
      },
      "outputs": [],
      "source": [
        "# Load and prepare dataset\n",
        "print(\"\\nLoading MNIST dataset...\")\n",
        "(x_train, y_train), (x_test, y_test) = load_and_prepare_mnist()\n",
        "\n",
        "# Create a validation set\n",
        "val_size = 5000\n",
        "x_val, y_val = x_test[:val_size], y_test[:val_size]\n",
        "x_test, y_test = x_test[val_size:], y_test[val_size:]\n",
        "mnist_data = ((x_train, y_train), (x_val, y_val))\n",
        "\n",
        "# Define input shape and number of classes\n",
        "input_shape = (28, 28, 1)\n",
        "num_classes = 10\n",
        "\n",
        "# Create models\n",
        "print(\"\\nCreating models...\")\n",
        "cnn_model = create_cnn_model(input_shape, num_classes)\n",
        "fcnn_model = create_fcnn_model(input_shape, num_classes)\n",
        "\n",
        "# Print model summaries\n",
        "print(\"\\nCNN Model Summary:\")\n",
        "cnn_model.summary()\n",
        "\n",
        "print(\"\\nFCNN Model Summary:\")\n",
        "fcnn_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU5NgWPcIajI"
      },
      "source": [
        "## PART 3: HARDWARE PERFORMANCE BENCHMARKING\n",
        "\n",
        "Now we'll benchmark both the CNN and FCNN models on different hardware platforms (CPU and GPU if available) to compare their performance during training and inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dAV69dYXIajL"
      },
      "outputs": [],
      "source": [
        "def benchmark_training(model_fn, train_data, val_data, device, epochs=3, batch_size=128):\n",
        "    \"\"\"Benchmark model training on specified device.\"\"\"\n",
        "    x_train, y_train = train_data\n",
        "    x_val, y_val = val_data\n",
        "\n",
        "    x_train_subset = x_train[:10000]\n",
        "    y_train_subset = y_train[:10000]\n",
        "\n",
        "    with tf.device(device):\n",
        "        model = model_fn()  # Create model inside the device context\n",
        "\n",
        "        # Warm-up\n",
        "        model.fit(\n",
        "            x_train_subset[:100], y_train_subset[:100],\n",
        "            batch_size=batch_size,\n",
        "            epochs=1,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        history = model.fit(\n",
        "            x_train_subset, y_train_subset,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_val, y_val),\n",
        "            verbose=1\n",
        "        )\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        val_loss, val_accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
        "\n",
        "    return {\n",
        "        'device': device,\n",
        "        'training_time': training_time,\n",
        "        'epochs': epochs,\n",
        "        'samples': len(x_train_subset),\n",
        "        'batch_size': batch_size,\n",
        "        'samples_per_second': (len(x_train_subset) * epochs) / training_time,\n",
        "        'val_accuracy': val_accuracy * 100,\n",
        "        'val_loss': val_loss,\n",
        "        'Model_': model\n",
        "    }\n",
        "\n",
        "def benchmark_inference(model, data, device, num_runs=1000, batch_size=1):\n",
        "    \"\"\"Benchmark model inference on a specified device.\"\"\"\n",
        "    import tensorflow as tf\n",
        "    import numpy as np\n",
        "    import time\n",
        "\n",
        "    x_data = np.asarray(data[0])\n",
        "    total_samples = num_runs * batch_size if batch_size > 1 else num_runs\n",
        "\n",
        "    # Extend x_data if needed\n",
        "    if len(x_data) >= total_samples:\n",
        "        x_subset = x_data[:total_samples]\n",
        "    else:\n",
        "        repeats = (total_samples + len(x_data) - 1) // len(x_data)\n",
        "        x_subset = np.tile(x_data, (repeats, 1, 1, 1))[:total_samples]\n",
        "\n",
        "    # Rebuild model on correct device\n",
        "    with tf.device(device):\n",
        "        # Clone the model to move it to the specified device\n",
        "        model_on_device = tf.keras.models.clone_model(model)\n",
        "        model_on_device.set_weights(model.get_weights())  # Copy weights\n",
        "\n",
        "        # Warm-up\n",
        "        warmup_runs = 10 if batch_size == 1 else 3\n",
        "        for _ in range(warmup_runs):\n",
        "            _ = model_on_device.predict(x_subset[:batch_size], verbose=0)\n",
        "\n",
        "        # Benchmarking\n",
        "        start_time = time.time()\n",
        "        if batch_size == 1:\n",
        "            for i in range(num_runs):\n",
        "                _ = model_on_device.predict(x_subset[i:i + 1], verbose=0)\n",
        "        else:\n",
        "            for i in range(0, total_samples, batch_size):\n",
        "                _ = model_on_device.predict(x_subset[i:i + batch_size], verbose=0)\n",
        "        end_time = time.time()\n",
        "\n",
        "        avg_inference_time = (end_time - start_time) / num_runs\n",
        "        throughput = batch_size / avg_inference_time\n",
        "\n",
        "    return {\n",
        "        'device': device,\n",
        "        'batch_size': batch_size,\n",
        "        'inference_time_ms': avg_inference_time * 1000,\n",
        "        'samples_per_second': throughput\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53PSs3atIajM"
      },
      "outputs": [],
      "source": [
        "# Define device strings\n",
        "cpu_device = \"/device:CPU:0\"\n",
        "gpu_device = \"/device:GPU:0\" if tf.config.list_physical_devices('GPU') else None\n",
        "\n",
        "# Benchmark training on different devices\n",
        "training_results = []\n",
        "\n",
        "\n",
        "\n",
        "# CPU benchmarking\n",
        "print(\"\\nBenchmarking CNN training on CPU...\")\n",
        "cnn_cpu_training = benchmark_training(lambda: create_cnn_model(input_shape, num_classes),\n",
        "                                      mnist_data[0], mnist_data[1], cpu_device)\n",
        "cnn_model = cnn_cpu_training[\"Model_\"]\n",
        "training_results.append({**cnn_cpu_training, 'model': 'CNN'})\n",
        "\n",
        "\n",
        "print(\"\\nBenchmarking FCNN training on CPU...\")\n",
        "fcnn_cpu_training = benchmark_training(lambda: create_fcnn_model(input_shape, num_classes),\n",
        "                                      mnist_data[0], mnist_data[1], cpu_device)\n",
        "fcnn_model = fcnn_cpu_training[\"Model_\"]\n",
        "training_results.append({**fcnn_cpu_training, 'model': 'FCNN'})\n",
        "\n",
        "\n",
        "\n",
        "# GPU benchmarking (if available)\n",
        "if gpu_device:\n",
        "    print(\"\\nBenchmarking CNN training on GPU...\")\n",
        "    cnn_gpu_training = benchmark_training(lambda: create_cnn_model(input_shape, num_classes),\n",
        "                                      mnist_data[0], mnist_data[1], gpu_device)\n",
        "    training_results.append({**cnn_gpu_training, 'model': 'CNN'})\n",
        "\n",
        "\n",
        "    print(\"\\nBenchmarking FCNN training on GPU...\")\n",
        "    fcnn_gpu_training = benchmark_training(lambda: create_fcnn_model(input_shape, num_classes),\n",
        "                                      mnist_data[0], mnist_data[1], gpu_device)\n",
        "    training_results.append({**fcnn_gpu_training, 'model': 'FCNN'})\n",
        "\n",
        "\n",
        "# Create and display training results table\n",
        "training_df = pd.DataFrame(training_results)\n",
        "print(\"\\nTraining Benchmark Results:\")\n",
        "print(training_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0Tpexq4IajN"
      },
      "outputs": [],
      "source": [
        "# Plot training benchmarks\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot training time comparison\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(data=training_df, x='model', y='training_time', hue='device')\n",
        "plt.title('Training Time Comparison')\n",
        "plt.ylabel('Training Time (seconds)')\n",
        "plt.xlabel('Model')\n",
        "\n",
        "# Plot samples per second comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(data=training_df, x='model', y='samples_per_second', hue='device')\n",
        "plt.title('Training Throughput Comparison')\n",
        "plt.ylabel('Samples per Second')\n",
        "plt.xlabel('Model')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYwyi7j3IajO"
      },
      "outputs": [],
      "source": [
        "# Benchmark inference on different devices\n",
        "inference_results = []\n",
        "\n",
        "# Single sample inference (batch_size=1)\n",
        "print(\"\\nBenchmarking single sample inference...\")\n",
        "for device in [cpu_device] + ([gpu_device] if gpu_device else []):\n",
        "    print(f\"Benchmarking CNN inference on {device} (batch_size=1)...\")\n",
        "    cnn_inference = benchmark_inference(cnn_model, (x_test, None), device, num_runs=100, batch_size=1)\n",
        "    inference_results.append({**cnn_inference, 'model': 'CNN'})\n",
        "\n",
        "    print(f\"Benchmarking FCNN inference on {device} (batch_size=1)...\")\n",
        "    fcnn_inference = benchmark_inference(fcnn_model, (x_test, None), device, num_runs=100, batch_size=1)\n",
        "    inference_results.append({**fcnn_inference, 'model': 'FCNN'})\n",
        "\n",
        "# Batch inference (batch_size=32)\n",
        "print(\"\\nBenchmarking batch inference...\")\n",
        "for device in [cpu_device] + ([gpu_device] if gpu_device else []):\n",
        "    print(f\"Benchmarking CNN inference on {device} (batch_size=32)...\")\n",
        "    cnn_batch_inference = benchmark_inference(cnn_model, (x_test, None), device, num_runs=10, batch_size=32)\n",
        "    inference_results.append({**cnn_batch_inference, 'model': 'CNN'})\n",
        "\n",
        "    print(f\"Benchmarking FCNN inference on {device} (batch_size=32)...\")\n",
        "    fcnn_batch_inference = benchmark_inference(fcnn_model, (x_test, None), device, num_runs=10, batch_size=32)\n",
        "    inference_results.append({**fcnn_batch_inference, 'model': 'FCNN'})\n",
        "\n",
        "# Create and display inference results table\n",
        "inference_df = pd.DataFrame(inference_results)\n",
        "print(\"\\nInference Benchmark Results:\")\n",
        "print(inference_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioCfIxbpIajO"
      },
      "outputs": [],
      "source": [
        "# Plot inference benchmarks\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot inference time comparison (log scale)\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(data=inference_df, x='model', y='inference_time_ms', hue='device')\n",
        "plt.title('Inference Time Comparison')\n",
        "plt.ylabel('Inference Time (ms, log scale)')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Model')\n",
        "\n",
        "# Plot samples per second comparison (log scale)\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(data=inference_df, x='model', y='samples_per_second', hue='device')\n",
        "plt.title('Inference Throughput Comparison')\n",
        "plt.ylabel('Samples per Second (log scale)')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Model')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYBoKUlHIajP"
      },
      "outputs": [],
      "source": [
        "# Compare batch size impact on inference\n",
        "plt.figure(figsize=(12, 5))\n",
        "batch_comparison = inference_df.copy()\n",
        "batch_comparison['device_batch'] = batch_comparison['device'] + ' (batch=' + batch_comparison['batch_size'].astype(str) + ')'\n",
        "\n",
        "# Plot inference time comparison by batch size\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(data=batch_comparison, x='model', y='inference_time_ms', hue='device_batch')\n",
        "plt.title('Inference Time by Batch Size')\n",
        "plt.ylabel('Inference Time (ms, log scale)')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Model')\n",
        "plt.legend(title='Device (Batch Size)')\n",
        "\n",
        "# Plot samples per second comparison by batch size\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(data=batch_comparison, x='model', y='samples_per_second', hue='device_batch')\n",
        "plt.title('Throughput by Batch Size')\n",
        "plt.ylabel('Samples per Second (log scale)')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Model')\n",
        "plt.legend(title='Device (Batch Size)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuFQzC8MIajV"
      },
      "outputs": [],
      "source": [
        "# Save the trained FCNN and CNN model for later use\n",
        "cnn_model.save('original_cnn_model.h5')\n",
        "print(\"\\nOriginal CNN model saved as 'original_cnn_model.h5'\")\n",
        "\n",
        "fcnn_model.save('original_FCNN_model.h5')\n",
        "print(\"\\nOriginal FCNN model saved as 'original_FCNN_model.h5'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iuoa7_1IajV"
      },
      "source": [
        "# Lab 4: Machine Learning Hardware Optimization - Part 2\n",
        "## Machine Learning Hardware Course\n",
        "\n",
        "This notebook is Part 2 of the Machine Learning Hardware Optimization lab, focusing on model quantization techniques.\n",
        "\n",
        "Model quantization reduces the precision of the weights and activations in a neural network, which can significantly reduce model size and improve inference speed, with minimal impact on accuracy in many cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYdpGhCBIajW"
      },
      "source": [
        "## PART 4: MODEL QUANTIZATION\n",
        "\n",
        "In this section, we'll apply different quantization techniques to our trained CNN model and analyze their impact on model size and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rkmUv0EIajW",
        "outputId": "8b7e8f6e-60a5-4ca8-9c40-0a5959a11544"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "# Load the saved CNN model\n",
        "original_model = load_model('original_cnn_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1lin2waVIajc"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate model performance\n",
        "def evaluate_model(model, x_test, y_test, model_name=\"Model\"):\n",
        "    \"\"\"Evaluate model accuracy and size.\"\"\"\n",
        "    # Measure model size\n",
        "    model_size = 0\n",
        "    if hasattr(model, 'weights'):\n",
        "        for weight in model.weights:\n",
        "            model_size += weight.numpy().nbytes\n",
        "    else:\n",
        "        model.save('temp_model.h5')\n",
        "        model_size = os.path.getsize('temp_model.h5')\n",
        "        os.remove('temp_model.h5')\n",
        "\n",
        "    # Evaluate model accuracy\n",
        "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    print(f\"{model_name} Evaluation:\")\n",
        "    print(f\"  Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"  Model Size: {model_size / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'accuracy': accuracy * 100,\n",
        "        'model_size_mb': model_size / 1024 / 1024\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6PQ7ZnOIajd"
      },
      "outputs": [],
      "source": [
        "# Evaluate original model\n",
        "original_results = evaluate_model(original_model, x_test, y_test, \"Original Model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msw7ed53Iaji"
      },
      "source": [
        "### TensorFlow Lite Conversion\n",
        "\n",
        "First, we'll convert our model to TensorFlow Lite format, which is optimized for mobile and edge devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zud4YkWoIaji"
      },
      "outputs": [],
      "source": [
        "# Apply post-training quantization with TensorFlow Lite\n",
        "# First, create a representative dataset for quantization\n",
        "def representative_dataset():\n",
        "    for i in range(100):\n",
        "        yield [x_train[i:i+1]]\n",
        "\n",
        "# Convert to TFLite model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model\n",
        "with open('original_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# Check the size of the TFLite model\n",
        "tflite_model_size = os.path.getsize('original_model.tflite')\n",
        "print(f\"TFLite model size: {tflite_model_size / 1024 / 1024:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X9h55hDIajj"
      },
      "source": [
        "### Float16 Quantization\n",
        "\n",
        "Next, we'll apply Float16 quantization, which reduces the precision of weights from float32 to float16, potentially reducing model size by up to 50%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWrRk8KwIajj"
      },
      "outputs": [],
      "source": [
        "# Quantize the model to float16\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_fp16_model = converter.convert()\n",
        "\n",
        "# Save the quantized model\n",
        "with open('quantized_fp16_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_fp16_model)\n",
        "\n",
        "# Check the size of the quantized TFLite model\n",
        "tflite_fp16_model_size = os.path.getsize('quantized_fp16_model.tflite')\n",
        "print(f\"Float16 quantized TFLite model size: {tflite_fp16_model_size / 1024 / 1024:.2f} MB\")\n",
        "print(f\"Size reduction: {(1 - tflite_fp16_model_size / tflite_model_size) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC8rq6ZoIajp"
      },
      "source": [
        "### INT8 Quantization\n",
        "\n",
        "Now, we'll apply full integer quantization, which converts weights and activations to 8-bit integers. This is the most aggressive form of quantization and can result in significant size reduction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXymQDhYIajp"
      },
      "outputs": [],
      "source": [
        "# Quantize the model to int8 (full integer quantization)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "tflite_int8_model = converter.convert()\n",
        "\n",
        "# Save the int8 quantized model\n",
        "with open('quantized_int8_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_int8_model)\n",
        "\n",
        "# Check the size of the int8 quantized TFLite model\n",
        "tflite_int8_model_size = os.path.getsize('quantized_int8_model.tflite')\n",
        "print(f\"Int8 quantized TFLite model size: {tflite_int8_model_size / 1024 / 1024:.2f} MB\")\n",
        "print(f\"Size reduction: {(1 - tflite_int8_model_size / tflite_model_size) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zvYShFUIajp"
      },
      "source": [
        "### Evaluating TFLite Models\n",
        "\n",
        "Now, let's evaluate the performance of our quantized TFLite models to see how quantization affects accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "viud_zJMIajq"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate TFLite model accuracy\n",
        "def evaluate_tflite_model(tflite_model_path, x_test, y_test):\n",
        "    \"\"\"Evaluate a TFLite model on the test dataset.\"\"\"\n",
        "    # Load TFLite model\n",
        "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Get input and output tensors\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    # Check if the model is quantized (int8)\n",
        "    input_scale, input_zero_point = input_details[0].get('quantization', (0, 0))\n",
        "    is_quantized = input_scale != 0\n",
        "\n",
        "    # Test model on random input data\n",
        "    correct_predictions = 0\n",
        "    #num_samples = min(1000, len(x_test))  # Limit to 1000 samples for speed\n",
        "    num_samples = len(x_test)  # Use all test samples for consistent comparison\n",
        "    for i in range(num_samples):\n",
        "        # Get test sample\n",
        "        test_image = x_test[i:i+1]\n",
        "        true_label = np.argmax(y_test[i])\n",
        "\n",
        "        # Quantize input if necessary\n",
        "        if is_quantized:\n",
        "            test_image = test_image / input_scale + input_zero_point\n",
        "            test_image = test_image.astype(np.int8)\n",
        "\n",
        "        # Resize tensor shape\n",
        "        input_shape = input_details[0]['shape']\n",
        "        test_image = np.reshape(test_image, input_shape)\n",
        "\n",
        "        # Set the input tensor\n",
        "        interpreter.set_tensor(input_details[0]['index'], test_image)\n",
        "\n",
        "        # Run inference\n",
        "        interpreter.invoke()\n",
        "\n",
        "        # Get the output tensor\n",
        "        output = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "        # Get predicted label\n",
        "        if output.shape[-1] == 10:  # If output is class probabilities\n",
        "            predicted_label = np.argmax(output)\n",
        "        else:  # If output is already the class index\n",
        "            predicted_label = output[0]\n",
        "\n",
        "        if predicted_label == true_label:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct_predictions / num_samples\n",
        "\n",
        "    # Get model size\n",
        "    model_size = os.path.getsize(tflite_model_path)\n",
        "\n",
        "    print(f\"TFLite Model: {tflite_model_path}\")\n",
        "    print(f\"  Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"  Model Size: {model_size / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "    return {\n",
        "        'model_name': os.path.basename(tflite_model_path),\n",
        "        'accuracy': accuracy * 100,\n",
        "        'model_size_mb': model_size / 1024 / 1024\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTR_R4aMIajv"
      },
      "outputs": [],
      "source": [
        "# Evaluate TFLite models\n",
        "print(\"\\nEvaluating TFLite models...\")\n",
        "tflite_original_results = evaluate_tflite_model('original_model.tflite', x_test, y_test)\n",
        "tflite_fp16_results = evaluate_tflite_model('quantized_fp16_model.tflite', x_test, y_test)\n",
        "tflite_int8_results = evaluate_tflite_model('quantized_int8_model.tflite', x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7gFWGOcIajw"
      },
      "source": [
        "### Comparing Quantization Results\n",
        "\n",
        "Now let's compare all our models to understand the trade-offs between model size and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bUVhaaeIajw"
      },
      "outputs": [],
      "source": [
        "# Collect all quantization results\n",
        "quantization_results = [\n",
        "    original_results,\n",
        "    tflite_original_results,\n",
        "    tflite_fp16_results,\n",
        "    tflite_int8_results\n",
        "]\n",
        "\n",
        "# Create a dataframe and print results\n",
        "quantization_df = pd.DataFrame(quantization_results)\n",
        "print(\"\\nQuantization Results:\")\n",
        "print(quantization_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8k0aN47Iaj2"
      },
      "outputs": [],
      "source": [
        "# Plot quantization results\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot accuracy comparison\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(data=quantization_df, x='model_name', y='accuracy')\n",
        "plt.title('Accuracy Comparison')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Model')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Plot model size comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(data=quantization_df, x='model_name', y='model_size_mb')\n",
        "plt.title('Model Size Comparison')\n",
        "plt.ylabel('Model Size (MB)')\n",
        "plt.xlabel('Model')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfAFxJZEIaj3"
      },
      "outputs": [],
      "source": [
        "# Plot trade-off between accuracy and model size\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=quantization_df, x='model_size_mb', y='accuracy', s=100)\n",
        "\n",
        "# Annotate points\n",
        "for i, row in quantization_df.iterrows():\n",
        "    plt.annotate(row['model_name'],\n",
        "                 (row['model_size_mb'], row['accuracy']),\n",
        "                 xytext=(5, 5), textcoords='offset points')\n",
        "\n",
        "plt.title('Accuracy vs. Model Size Trade-off')\n",
        "plt.xlabel('Model Size (MB)')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEpank7HIaj3"
      },
      "source": [
        "### Quantization Summary\n",
        "\n",
        "In this section, we've applied different quantization techniques to our CNN model:\n",
        "\n",
        "1. **TensorFlow Lite Conversion**: Converting to TFLite format without quantization\n",
        "2. **Float16 Quantization**: Reducing weight precision from 32-bit to 16-bit floating point\n",
        "3. **INT8 Quantization**: Full integer quantization with 8-bit weights and activations\n",
        "\n",
        "We've observed how each technique affects model size and accuracy. The results show that quantization can significantly reduce model size with minimal impact on accuracy, making it an effective technique for deploying models to resource-constrained environments like mobile devices and edge hardware."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl7lfFWeIaj3"
      },
      "source": [
        "# Lab 4: Machine Learning Hardware Optimization - Part 3\n",
        "## Machine Learning Hardware Course\n",
        "\n",
        "This notebook is Part 3 of the Machine Learning Hardware Optimization lab, focusing on model pruning techniques.\n",
        "\n",
        "Model pruning is a technique that removes redundant parameters from a neural network, reducing its complexity, size, and computational requirements while maintaining reasonable performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmdwZFBBIaj9"
      },
      "source": [
        "## PART 5: MODEL PRUNING\n",
        "\n",
        "In this section, we'll explore model pruning and its effects on model size and accuracy. We'll also combine pruning with quantization to achieve even more significant size reductions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2Tv-qSVLIaj9"
      },
      "outputs": [],
      "source": [
        "# Import tensorflow_model_optimization again to ensure it's available\n",
        "import tensorflow_model_optimization as tfmot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRoX94VTIakL"
      },
      "source": [
        "### Manual Pruning Implementation\n",
        "\n",
        "We'll implement a manual pruning approach that removes weights based on their magnitude. This is a common approach where weights with small absolute values are considered less important and set to zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IXJYaNIQIakL"
      },
      "outputs": [],
      "source": [
        "# Define a function to create a prunable model\n",
        "def create_prunable_cnn_model(input_shape, num_classes):\n",
        "    \"\"\"Create a functional CNN model for pruning compatibility.\"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KNC4GGzlIakR"
      },
      "outputs": [],
      "source": [
        "# Define a function to manually prune weights based on magnitude\n",
        "def prune_weights(model, sparsity=0.5):\n",
        "    \"\"\"\n",
        "    Prune model weights based on magnitude.\n",
        "\n",
        "    Args:\n",
        "        model: Keras model to prune\n",
        "        sparsity: Target sparsity level (fraction of weights to be pruned)\n",
        "\n",
        "    Returns:\n",
        "        Pruned model\n",
        "    \"\"\"\n",
        "    print(f\"\\nApplying magnitude-based pruning with {sparsity:.0%} sparsity...\")\n",
        "\n",
        "    pruned_model = tf.keras.models.clone_model(model)\n",
        "    pruned_model.set_weights(model.get_weights())\n",
        "\n",
        "    # Prune eligible layers (Conv and Dense)\n",
        "    for i, layer in enumerate(pruned_model.layers):\n",
        "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
        "            weights = layer.get_weights()\n",
        "\n",
        "            # Only prune the weight matrix, not the bias\n",
        "            weight_matrix = weights[0]\n",
        "\n",
        "            # Flatten the weight matrix to identify the threshold\n",
        "            flat_weights = weight_matrix.flatten()\n",
        "            abs_weights = np.abs(flat_weights)\n",
        "\n",
        "            # Calculate the threshold value based on the sparsity level\n",
        "            k = int(flat_weights.size * sparsity)\n",
        "            if k > 0:\n",
        "                threshold = np.partition(abs_weights, k)[k]\n",
        "\n",
        "                # Create a mask for weights with magnitude below threshold\n",
        "                mask = np.abs(weight_matrix) > threshold\n",
        "\n",
        "                # Apply the mask to the weights\n",
        "                pruned_weights = weight_matrix * mask\n",
        "\n",
        "                # Update the weights\n",
        "                weights[0] = pruned_weights\n",
        "                layer.set_weights(weights)\n",
        "\n",
        "                non_zero = np.count_nonzero(pruned_weights)\n",
        "                total = pruned_weights.size\n",
        "                print(f\"  Layer {i} ({layer.name}): {non_zero}/{total} weights retained ({non_zero/total:.2%})\")\n",
        "\n",
        "    # Recompile the model with the same settings\n",
        "    pruned_model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return pruned_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_udjkdxkIakS"
      },
      "outputs": [],
      "source": [
        "# Create a new model for pruning\n",
        "input_shape = (28, 28, 1)\n",
        "num_classes = 10\n",
        "cnn_model_for_pruning = create_prunable_cnn_model(input_shape, num_classes)\n",
        "\n",
        "# First, train the model before pruning\n",
        "print(\"\\nTraining model before pruning...\")\n",
        "cnn_model_for_pruning.fit(\n",
        "    x_train, y_train,  # Using a subset for speed\n",
        "    batch_size=128,\n",
        "    epochs=3,\n",
        "    validation_data=(x_val, y_val),\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Tu_SRhzZpYpR"
      },
      "outputs": [],
      "source": [
        "def prune_evaluate_model(model, x_test, y_test, model_name=\"Model\"):\n",
        "    \"\"\"Evaluate model accuracy and estimated size after pruning.\"\"\"\n",
        "    total_size = 0\n",
        "    nonzero_size = 0\n",
        "\n",
        "    for weight in model.weights:\n",
        "        w_np = weight.numpy()\n",
        "        total_size += w_np.nbytes\n",
        "        nonzero_size += np.count_nonzero(w_np) * w_np.itemsize\n",
        "\n",
        "    # Accuracy\n",
        "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    print(f\"{model_name} Evaluation:\")\n",
        "    print(f\"  Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"  Original Size: {total_size / 1024 / 1024:.2f} MB\")\n",
        "    print(f\"  Estimated Pruned Size: {nonzero_size / 1024 / 1024:.2f} MB\")\n",
        "    print(f\"  Size Reduction: {(1 - nonzero_size / total_size) * 100:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'accuracy': accuracy * 100,\n",
        "        'original_size_mb': total_size / 1024 / 1024,\n",
        "        'pruned_size_mb': nonzero_size / 1024 / 1024,\n",
        "        'estimated_sparsity': 1 - (nonzero_size / total_size)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umjWHxPMIakS"
      },
      "source": [
        "### Apply Pruning and Fine-tuning\n",
        "\n",
        "Now we'll apply the pruning technique and fine-tune the model to recover some of the accuracy lost during pruning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT39uyAyIakX"
      },
      "outputs": [],
      "source": [
        "# Apply manual pruning\n",
        "pruned_model = prune_weights(cnn_model_for_pruning, sparsity=0.5)\n",
        "\n",
        "# Evaluate model after pruning but before fine-tuning\n",
        "pruned_before_finetuning_results = prune_evaluate_model(pruned_model, x_test, y_test, \"Pruned Model (Before Fine-tuning)\")\n",
        "\n",
        "# Fine-tune the pruned model\n",
        "print(\"\\nFine-tuning the pruned model...\")\n",
        "pruned_model.fit(\n",
        "    x_train[:10000], y_train[:10000],  # Using a subset for speed\n",
        "    batch_size=128,\n",
        "    epochs=2,\n",
        "    validation_data=(x_val, y_val),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the pruned model after fine-tuning\n",
        "pruned_model_results = prune_evaluate_model(pruned_model, x_test, y_test, \"Pruned Model (After Fine-tuning)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqEKFTSiIakY"
      },
      "outputs": [],
      "source": [
        "# Save the pruned model\n",
        "pruned_model.save('pruned_model.keras')\n",
        "stripped_model = pruned_model  # Since we're using manual pruning, no stripping is needed\n",
        "stripped_model_results = prune_evaluate_model(stripped_model, x_test, y_test, \"Stripped Pruned Model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF-tgFs-IakZ"
      },
      "source": [
        "### Combining Pruning with Quantization\n",
        "\n",
        "To achieve maximum model size reduction, we'll now combine pruning with quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "Fh1Fh9bJqgd_"
      },
      "outputs": [],
      "source": [
        "def Prun_evaluate_tflite_model(tflite_model_path, x_test, y_test, sparsity=0.5):\n",
        "    \"\"\"Evaluate a TFLite model and optionally simulate effective size after pruning.\"\"\"\n",
        "    # Load TFLite model\n",
        "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Get input and output tensors\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    # Check if the model is quantized (int8)\n",
        "    input_scale, input_zero_point = input_details[0].get('quantization', (0, 0))\n",
        "    is_quantized = input_scale != 0\n",
        "\n",
        "    # Evaluate on all samples\n",
        "    correct_predictions = 0\n",
        "    num_samples = len(x_test)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        test_image = x_test[i:i+1]\n",
        "        true_label = np.argmax(y_test[i])\n",
        "\n",
        "        if is_quantized:\n",
        "            test_image = test_image / input_scale + input_zero_point\n",
        "            test_image = test_image.astype(np.int8)\n",
        "\n",
        "        test_image = np.reshape(test_image, input_details[0]['shape'])\n",
        "\n",
        "        interpreter.set_tensor(input_details[0]['index'], test_image)\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "        predicted_label = np.argmax(output) if output.shape[-1] == 10 else output[0]\n",
        "        if predicted_label == true_label:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = correct_predictions / num_samples\n",
        "\n",
        "    # Model size\n",
        "    model_size = os.path.getsize(tflite_model_path)\n",
        "    estimated_size = model_size\n",
        "\n",
        "    # Apply simulated size reduction if sparsity is specified\n",
        "    if sparsity is not None and 0 < sparsity < 1:\n",
        "        estimated_size = model_size * (1 - sparsity)\n",
        "\n",
        "    print(f\"TFLite Model: {tflite_model_path}\")\n",
        "    print(f\"  Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"  Original Size: {model_size / 1024 / 1024:.2f} MB\")\n",
        "    if sparsity is not None:\n",
        "        print(f\"  Estimated Pruned Size (@ {sparsity*100:.0f}% sparsity): {estimated_size / 1024 / 1024:.2f} MB\")\n",
        "        print(f\"  Size Reduction: {(1 - estimated_size / model_size) * 100:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        'model_name': os.path.basename(tflite_model_path),\n",
        "        'accuracy': accuracy * 100,\n",
        "        'original_size_mb': model_size / 1024 / 1024,\n",
        "        'estimated_pruned_size_mb': estimated_size / 1024 / 1024 if sparsity else None,\n",
        "        'estimated_sparsity': sparsity\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GukWG4MIake"
      },
      "outputs": [],
      "source": [
        "# Create a representative dataset for quantization\n",
        "def representative_dataset():\n",
        "    for i in range(100):\n",
        "        yield [x_train[i:i+1]]\n",
        "\n",
        "# Convert the stripped model to TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(stripped_model)\n",
        "tflite_stripped_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model\n",
        "with open('stripped_pruned_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_stripped_model)\n",
        "\n",
        "# Evaluate the TFLite stripped model\n",
        "tflite_stripped_results = Prun_evaluate_tflite_model('stripped_pruned_model.tflite', x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHaxXwgfIake"
      },
      "outputs": [],
      "source": [
        "# Apply both pruning and quantization\n",
        "\n",
        "# First, convert to TFLite with float16 quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(stripped_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_pruned_fp16_model = converter.convert()\n",
        "\n",
        "# Save the quantized pruned model\n",
        "with open('pruned_quantized_fp16_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_pruned_fp16_model)\n",
        "\n",
        "# Then convert to TFLite with int8 quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(stripped_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "tflite_pruned_int8_model = converter.convert()\n",
        "\n",
        "# Save the int8 quantized pruned model\n",
        "with open('pruned_quantized_int8_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_pruned_int8_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_Omgk6TIakf"
      },
      "outputs": [],
      "source": [
        "# Evaluate the quantized pruned models\n",
        "tflite_pruned_fp16_results = Prun_evaluate_tflite_model('pruned_quantized_fp16_model.tflite', x_test, y_test)\n",
        "tflite_pruned_int8_results = Prun_evaluate_tflite_model('pruned_quantized_int8_model.tflite', x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrWF6XFdIaks"
      },
      "source": [
        "### Pruning Summary\n",
        "\n",
        "In this section, we've explored model pruning and its combination with quantization:\n",
        "\n",
        "1. **Model Pruning**: We implemented magnitude-based pruning, which removes weights with small absolute values.\n",
        "2. **Fine-tuning**: We fine-tuned the pruned model to recover accuracy.\n",
        "3. **Combined Pruning and Quantization**: We applied both techniques to achieve maximum size reduction.\n",
        "\n",
        "Key observations:\n",
        "- Pruning can significantly reduce model size with minimal impact on accuracy, especially after fine-tuning.\n",
        "- Combining pruning with quantization provides the maximum size reduction.\n",
        "- There's a trade-off between model size and accuracy, but with proper techniques, we can find a good balance.\n",
        "\n",
        "These techniques are valuable for deploying models on resource-constrained devices like mobile phones, IoT devices, and edge hardware."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv5Kx-wtIakt"
      },
      "source": [
        "# Lab 4: Machine Learning Hardware Optimization - Part 4\n",
        "## Machine Learning Hardware Course\n",
        "\n",
        "This notebook is Part 4 of the Machine Learning Hardware Optimization lab, focusing on converting models to different deployment formats.\n",
        "\n",
        "Different deployment environments require specific model formats. In this section, we'll explore how to convert our models to various formats optimized for different deployment scenarios and compare their sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqQakAdFIakt"
      },
      "source": [
        "## PART 6: DEPLOYMENT FORMAT CONVERSION\n",
        "\n",
        "In this section, we'll convert our models to different formats suitable for various deployment scenarios, such as ONNX for cross-platform compatibility, SavedModel for TensorFlow Serving, and TensorFlow.js for web deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "SpHtHFxLIaky"
      },
      "outputs": [],
      "source": [
        "# First, make sure we have all the necessary imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import load_model\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5NhVBa5Iakz"
      },
      "outputs": [],
      "source": [
        "# Load the original model if available, or create a new one\n",
        "try:\n",
        "    original_model = load_model('original_cnn_model.h5')\n",
        "    print(\"Successfully loaded model from 'original_cnn_model.h5'\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Please make sure you've run Part 1 first and saved the model\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RifViejXIak0"
      },
      "source": [
        "### Converting to ONNX Format\n",
        "\n",
        "ONNX (Open Neural Network Exchange) is an open format to represent deep learning models. It allows models to be transferred between different frameworks such as TensorFlow, PyTorch, MXNet, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rHywxNpIak0"
      },
      "outputs": [],
      "source": [
        "# Try converting to ONNX format\n",
        "print(\"\\n== Converting to ONNX Format ==\")\n",
        "\n",
        "# First, install the required packages if needed\n",
        "try:\n",
        "    import tf2onnx\n",
        "    print(\"tf2onnx already installed\")\n",
        "except ImportError:\n",
        "    !pip install -q tf2onnx\n",
        "    import tf2onnx\n",
        "    print(\"Installed tf2onnx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SCbKSMiIak5"
      },
      "outputs": [],
      "source": [
        "# Try converting to ONNX format using the more compatible approach for TensorFlow 2.x\n",
        "try:\n",
        "    # For TensorFlow 2.x, we need to convert the model to a saved model first\n",
        "    # and then convert the saved model to ONNX\n",
        "    print(\"Converting original model to SavedModel format first...\")\n",
        "    original_saved_model_dir = \"original_saved_model\"\n",
        "    tf.saved_model.save(original_model, original_saved_model_dir)\n",
        "\n",
        "    print(\"Converting SavedModel to ONNX...\")\n",
        "    # Use the command-line converter approach via subprocess\n",
        "    result = subprocess.run(\n",
        "        [\"python\", \"-m\", \"tf2onnx.convert\",\n",
        "         \"--saved-model\", original_saved_model_dir,\n",
        "         \"--output\", \"original_model.onnx\"],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(\"ONNX conversion successful\")\n",
        "        onnx_model_exists = True\n",
        "    else:\n",
        "        print(\"ONNX conversion failed with error:\")\n",
        "        print(result.stderr)\n",
        "        onnx_model_exists = False\n",
        "\n",
        "    # If the first conversion was successful, try the pruned model\n",
        "    if onnx_model_exists:\n",
        "\n",
        "        pruned_onnx_exists = False\n",
        "\n",
        "    # Calculate ONNX model sizes if conversion was successful\n",
        "    if onnx_model_exists and os.path.exists('original_model.onnx'):\n",
        "        onnx_model_size = os.path.getsize('original_model.onnx')\n",
        "        print(f\"ONNX model size: {onnx_model_size / 1024 / 1024:.2f} MB\")\n",
        "    else:\n",
        "        onnx_model_size = 0\n",
        "\n",
        "\n",
        "        # onnx_pruned_model_size = 0\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ONNX conversion skipped due to error: {str(e)}\")\n",
        "    onnx_model_size = 0\n",
        "    # onnx_pruned_model_size = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxdCJdJdIak6"
      },
      "source": [
        "### Converting to TensorFlow SavedModel Format\n",
        "\n",
        "The SavedModel format is the recommended format for TensorFlow models in production, especially when using TensorFlow Serving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvfDzfssIak6"
      },
      "outputs": [],
      "source": [
        "# Convert to TensorFlow SavedModel format (optimized for TensorFlow Serving)\n",
        "print(\"\\nConverting to SavedModel format...\")\n",
        "tf.saved_model.save(original_model, \"saved_model\")\n",
        "saved_model_size = sum(f.stat().st_size for f in Path(\"saved_model\").glob('**/*') if f.is_file())\n",
        "print(f\"SavedModel size: {saved_model_size / 1024 / 1024:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXGdnevXIalA"
      },
      "source": [
        "### Converting to TensorFlow.js Format\n",
        "\n",
        "TensorFlow.js is a library for machine learning in JavaScript, enabling training and deployment of ML models in the browser and on Node.js."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX1n3zdPIalA"
      },
      "outputs": [],
      "source": [
        "# Try installing the TensorFlow.js converter\n",
        "try:\n",
        "    !pip install -q tensorflowjs\n",
        "    print(\"TensorFlow.js converter installed successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to install TensorFlow.js converter: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rjhfqxUIalB"
      },
      "outputs": [],
      "source": [
        "# Try converting to TensorFlow.js format\n",
        "try:\n",
        "    print(\"\\nConverting to TensorFlow.js format...\")\n",
        "    # Use subprocess to call the converter to avoid dependency issues\n",
        "    result = subprocess.run(\n",
        "        [\"tensorflowjs_converter\", \"--input_format=keras\",\n",
        "         \"original_cnn_model.h5\", \"tfjs_model\"],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(\"TensorFlow.js conversion successful\")\n",
        "        tfjs_model_exists = True\n",
        "        # Calculate the size of the TensorFlow.js model\n",
        "        tfjs_model_size = sum(f.stat().st_size for f in Path(\"tfjs_model\").glob('**/*') if f.is_file())\n",
        "        print(f\"TensorFlow.js model size: {tfjs_model_size / 1024 / 1024:.2f} MB\")\n",
        "    else:\n",
        "        print(\"TensorFlow.js conversion failed with error:\")\n",
        "        print(result.stderr)\n",
        "        tfjs_model_exists = False\n",
        "        tfjs_model_size = 0\n",
        "except Exception as e:\n",
        "    print(f\"TensorFlow.js conversion skipped due to error: {str(e)}\")\n",
        "    tfjs_model_exists = False\n",
        "    tfjs_model_size = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq3L5z4yIalG"
      },
      "source": [
        "### Model Format Comparison\n",
        "\n",
        "Now let's compare the sizes of models in different formats to understand which format is most efficient for deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoChhL0cIalH",
        "outputId": "65ad1191-3e69-4346-e585-5a63c990dfde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded TFLite model sizes from previous parts\n"
          ]
        }
      ],
      "source": [
        "# Get TFLite model sizes from previous parts (if available)\n",
        "try:\n",
        "    tflite_model_size = os.path.getsize('original_model.tflite')\n",
        "    tflite_fp16_model_size = os.path.getsize('quantized_fp16_model.tflite')\n",
        "    tflite_int8_model_size = os.path.getsize('quantized_int8_model.tflite')\n",
        "    pruned_quantized_int8_model_size = os.path.getsize('pruned_quantized_int8_model.tflite')\n",
        "    print(\"Successfully loaded TFLite model sizes from previous parts\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading TFLite model sizes: {e}\")\n",
        "    # Set default values if files don't exist\n",
        "    tflite_model_size = 0\n",
        "    tflite_fp16_model_size = 0\n",
        "    tflite_int8_model_size = 0\n",
        "    pruned_quantized_int8_model_size = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1a4FWwqIalH"
      },
      "outputs": [],
      "source": [
        "# Summarize all model formats\n",
        "format_results = [\n",
        "    {\n",
        "        'model_name': 'Original Model (H5)',\n",
        "        'format': 'Keras H5',\n",
        "        'model_size_mb': os.path.getsize('original_cnn_model.h5') / 1024 / 1024\n",
        "    }\n",
        "]\n",
        "\n",
        "# Add TFLite models if available\n",
        "if tflite_model_size > 0:\n",
        "    format_results.append({\n",
        "        'model_name': 'Original Model (TFLite)',\n",
        "        'format': 'TFLite',\n",
        "        'model_size_mb': tflite_model_size / 1024 / 1024\n",
        "    })\n",
        "\n",
        "if tflite_fp16_model_size > 0:\n",
        "    format_results.append({\n",
        "        'model_name': 'Quantized Model (TFLite FP16)',\n",
        "        'format': 'TFLite FP16',\n",
        "        'model_size_mb': tflite_fp16_model_size / 1024 / 1024\n",
        "    })\n",
        "\n",
        "if tflite_int8_model_size > 0:\n",
        "    format_results.append({\n",
        "        'model_name': 'Quantized Model (TFLite INT8)',\n",
        "        'format': 'TFLite INT8',\n",
        "        'model_size_mb': tflite_int8_model_size / 1024 / 1024\n",
        "    })\n",
        "\n",
        "if pruned_quantized_int8_model_size > 0:\n",
        "    format_results.append({\n",
        "        'model_name': 'Pruned & Quantized Model (TFLite INT8)',\n",
        "        'format': 'TFLite INT8',\n",
        "        'model_size_mb': pruned_quantized_int8_model_size / 1024 / 1024*0.5\n",
        "    })\n",
        "\n",
        "# Add ONNX models if they were converted successfully\n",
        "if onnx_model_size > 0:\n",
        "    format_results.append({\n",
        "        'model_name': 'Original Model (ONNX)',\n",
        "        'format': 'ONNX',\n",
        "        'model_size_mb': onnx_model_size / 1024 / 1024\n",
        "    })\n",
        "\n",
        "# if onnx_pruned_model_size > 0:\n",
        "#     format_results.append({\n",
        "#         'model_name': 'Pruned Model (ONNX)',\n",
        "#         'format': 'ONNX',\n",
        "#         'model_size_mb': onnx_pruned_model_size / 1024 / 1024\n",
        "#     })\n",
        "\n",
        "# Add SavedModel\n",
        "format_results.append({\n",
        "    'model_name': 'Original Model (SavedModel)',\n",
        "    'format': 'SavedModel',\n",
        "    'model_size_mb': saved_model_size / 1024 / 1024\n",
        "})\n",
        "\n",
        "# Add TensorFlow.js model if it was converted successfully\n",
        "if tfjs_model_exists and tfjs_model_size > 0:\n",
        "    format_results.append({\n",
        "        'model_name': 'Original Model (TensorFlow.js)',\n",
        "        'format': 'TensorFlow.js',\n",
        "        'model_size_mb': tfjs_model_size / 1024 / 1024\n",
        "    })\n",
        "\n",
        "# Create a dataframe and plot results\n",
        "format_df = pd.DataFrame(format_results)\n",
        "print(\"\\nModel Format Comparison:\")\n",
        "print(format_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heO-Wlc1IalN"
      },
      "outputs": [],
      "source": [
        "# Plot model format comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=format_df, x='model_name', y='model_size_mb', hue='format')\n",
        "plt.title('Model Size by Format')\n",
        "plt.ylabel('Model Size (MB)')\n",
        "plt.xlabel('Model')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Format')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GAj_4dXIalN"
      },
      "outputs": [],
      "source": [
        "# Group by format and calculate average size\n",
        "format_summary = format_df.groupby('format')['model_size_mb'].mean().reset_index()\n",
        "format_summary = format_summary.rename(columns={'model_size_mb': 'average_size_mb'})\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=format_summary, x='format', y='average_size_mb')\n",
        "plt.title('Average Model Size by Format')\n",
        "plt.ylabel('Average Size (MB)')\n",
        "plt.xlabel('Format')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey9MrFD5IalN"
      },
      "source": [
        "### Deployment Format Summary\n",
        "\n",
        "In this section, we've explored different model formats for deployment:\n",
        "\n",
        "1. **ONNX Format**: For cross-framework compatibility\n",
        "2. **TensorFlow SavedModel**: For TensorFlow Serving\n",
        "3. **TensorFlow.js**: For browser-based deployment\n",
        "4. **TensorFlow Lite**: For mobile and edge devices\n",
        "\n",
        "Each format has its own advantages depending on the deployment environment:\n",
        "\n",
        "- **ONNX** is useful when you need to use models across different frameworks.\n",
        "- **SavedModel** is the preferred format for TensorFlow Serving and production systems based on TensorFlow.\n",
        "- **TensorFlow.js** enables running models directly in web browsers.\n",
        "- **TensorFlow Lite** (especially with quantization) is optimized for mobile and edge devices with limited resources.\n",
        "\n",
        "Choosing the right deployment format is essential for optimizing performance in your specific deployment environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPHnBmxSIalU"
      },
      "source": [
        "# Lab 4: Machine Learning Hardware Optimization - Part 5\n",
        "## Machine Learning Hardware Course\n",
        "\n",
        "This notebook is Part 5 of the Machine Learning Hardware Optimization lab, focusing on a comprehensive analysis of all optimization techniques we've explored.\n",
        "\n",
        "In this final part, we'll analyze the trade-offs between model accuracy, size, and inference speed for various optimization techniques, and identify the best models for different deployment scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1djqKEmIal_"
      },
      "source": [
        "## CONCLUSION\n",
        "\n",
        "In this lab, we've explored various hardware optimization techniques for deep learning models:\n",
        "\n",
        "1. **Hardware Performance Benchmarking**: We compared model performance across different hardware platforms (CPU and GPU).\n",
        "\n",
        "2. **Quantization**: We applied post-training quantization techniques (float16 and int8) to reduce model size while maintaining reasonable accuracy.\n",
        "\n",
        "3. **Pruning**: We implemented magnitude-based pruning to remove redundant parameters, further reducing model size.\n",
        "\n",
        "4. **Deployment Format Conversion**: We converted models to different formats optimized for specific deployment scenarios (TFLite, ONNX, SavedModel, TensorFlow.js).\n",
        "\n",
        "5. **Comprehensive Analysis**: We analyzed the trade-offs between model accuracy, size, and optimization techniques to identify optimal models for different scenarios.\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "- **Hardware Performance**: GPUs significantly outperform CPUs for both training and inference, especially with larger batch sizes.\n",
        "\n",
        "- **Model Size Reduction**: Quantization and pruning can dramatically reduce model size with minimal impact on accuracy.\n",
        "\n",
        "- **Efficiency Trade-offs**: Different optimization techniques offer various trade-offs between accuracy, size, and inference speed.\n",
        "\n",
        "- **Deployment Formats**: The choice of deployment format depends on the target environment, with TFLite being the most efficient for mobile and edge devices.\n",
        "\n",
        "### Best Practices for Hardware Optimization:\n",
        "\n",
        "1. **Start with a well-designed model**: A well-designed architecture is easier to optimize.\n",
        "\n",
        "2. **Apply quantization first**: It's an easy way to get significant size reduction with minimal accuracy loss.\n",
        "\n",
        "3. **Use pruning for further optimization**: Pruning can remove redundant parameters and reduce model size even more.\n",
        "\n",
        "4. **Combine techniques**: Combining pruning with quantization provides the maximum size reduction.\n",
        "\n",
        "5. **Fine-tune after pruning**: Fine-tuning helps recover accuracy lost during pruning.\n",
        "\n",
        "6. **Choose the right deployment format**: Select the appropriate format based on your deployment environment.\n",
        "\n",
        "7. **Benchmark on target hardware**: Always measure performance on the actual deployment hardware.\n",
        "\n",
        "These techniques are essential for deploying machine learning models in resource-constrained environments such as mobile devices, edge computing platforms, and IoT devices. By understanding and applying these optimization strategies, you can make informed decisions about model design based on your specific hardware constraints and performance requirements."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
