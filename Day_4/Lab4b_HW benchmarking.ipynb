{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"markdown-google-sans\">\n",
    "  <h2>Machine Learning Hardware Courseâ€‹</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"markdown-google-sans\">\n",
    "  <h2>Lab 4b: HW benchmarking</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below code twice. One time using a CPU and the other one use a GPU. Before re-running the code for the different HW, record your results with this notebook TO AVOID LOSING YOUR PROGRESS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision import datasets, models\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load an image from COCO dataset\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Define preprocessing for AlexNet\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),  # Resize to 224x224 as expected by AlexNet\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalization for ImageNet\n",
    "])\n",
    "\n",
    "# Apply preprocessing\n",
    "image = transform(image).unsqueeze(0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to GPU/MPS if available\n",
    "device, dev_name = (torch.device(\"mps\"), \"mps\") if torch.backends.mps.is_available() else \\\n",
    "         (torch.device(\"cuda\"), \"cuda\") if torch.cuda.is_available() else (torch.device(\"cpu\"), \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_workload(model, device, dev_name, image, iterations=100):\n",
    "    try:\n",
    "        model_name = type(model).__name__\n",
    "        print(f\"profiling {model_name} on {dev_name}...\")\n",
    "    except:\n",
    "        print(f\"profiling on {dev_name}...\")\n",
    "    model.to(device)\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Run inference\n",
    "    if dev_name==\"cpu\":\n",
    "        start_time = time.time()\n",
    "        for _ in tqdm(range(iterations), desc ='profiling latency is in progress...'):\n",
    "            with torch.no_grad():\n",
    "              output = model(image)\n",
    "        elapsed_time = time.time()-start_time\n",
    "        latency = elapsed_time/iterations*1000\n",
    "    elif dev_name==\"cuda\":\n",
    "        torch.cuda.synchronize()  # Ensure any pending tasks are done\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        for _ in tqdm(range(iterations), desc ='profiling latency is in progress...'):\n",
    "            with torch.no_grad():\n",
    "              output = model(image)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()  # Wait for all kernels to finish\n",
    "        latency = start.elapsed_time(end)/iterations\n",
    "    elif dev_name==\"mps\":\n",
    "        torch.mps.synchronize()  # Ensure all pending tasks are complete before starting\n",
    "        start_time = time.time()\n",
    "        for _ in tqdm(range(iterations), desc ='profiling latency is in progress...'):\n",
    "            with torch.no_grad():\n",
    "              output = model(image)\n",
    "        elapsed_time = time.time()-start_time\n",
    "        torch.mps.synchronize()  # Ensure all pending tasks are complete before starting\n",
    "        latency = elapsed_time/iterations*1000  \n",
    "    # Get predicted class\n",
    "    predicted_class = output.argmax(dim=1).item()\n",
    "    # print(f\"Predicted Class: {predicted_class}\")\n",
    "    return latency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_workload_on_ViT(device, dev_name, iterations=100):\n",
    "    # Load an image from COCO dataset\n",
    "    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    # Use ViTImageProcessor instead of the deprecated ViTFeatureExtractor\n",
    "    processor = ViTImageProcessor.from_pretrained('google/vit-large-patch16-224')\n",
    "    ViT_large = ViTForImageClassification.from_pretrained('google/vit-large-patch16-224')\n",
    "    # Apply feature extractor directly on the raw image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    ViT_large.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Run inference\n",
    "    if dev_name==\"cpu\":\n",
    "        start_time = time.time()\n",
    "        for _ in tqdm(range(iterations), desc ='profiling latency is in progress...'):\n",
    "            with torch.no_grad():\n",
    "              outputs = ViT_large(**inputs)\n",
    "        elapsed_time = time.time()-start_time\n",
    "        latency = elapsed_time/iterations*1000\n",
    "    elif dev_name==\"cuda\":\n",
    "        torch.cuda.synchronize()  # Ensure any pending tasks are done\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        for _ in tqdm(range(iterations), desc ='profiling latency is in progress...'):\n",
    "            with torch.no_grad():\n",
    "              outputs = ViT_large(**inputs)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()  # Wait for all kernels to finish\n",
    "        latency = start.elapsed_time(end)/iterations\n",
    "    elif dev_name==\"mps\":\n",
    "        torch.mps.synchronize()  # Ensure all pending tasks are complete before starting\n",
    "        start_time = time.time()\n",
    "        for _ in tqdm(range(iterations), desc ='profiling latency is in progress...'):\n",
    "            with torch.no_grad():\n",
    "              outputs = ViT_large(**inputs)\n",
    "        elapsed_time = time.time()-start_time\n",
    "        torch.mps.synchronize()  # Ensure all pending tasks are complete before starting\n",
    "        latency = elapsed_time/iterations*1000  \n",
    "    # Get predicted class\n",
    "    return latency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "alexnet_inference_latency = profile_workload(alexnet, device, dev_name, image, iterations=100)\n",
    "print(f\"\\n\\nAlexNet inference latency: {alexnet_inference_latency:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet152 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
    "resnet152_inference_latency = profile_workload(resnet152, device, dev_name, image, iterations=100)\n",
    "print(f\"\\n\\nResNet152 inference latency: {resnet152_inference_latency:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "ViTLarge_inference_latency = profile_workload_on_ViT(device, dev_name, iterations=100)\n",
    "print(f\"\\n\\nViTLarge inference latency: {ViTLarge_inference_latency:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record the profiled latancy on:\n",
    "- CPU AlexNet latency: ## ms\n",
    "- CPU ResNet152 latency: ## ms\n",
    "- CPU ViT-Large latency: ## ms\n",
    "- GPU AlexNet latency: ## ms\n",
    "- GPU ResNet152 latency: ## ms\n",
    "- GPU ViT-Large latency: ## ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your results for the different DNN models you profiles on the different hardware:\n",
    "\n",
    "Why did you get different latencies for each DNN model?\n",
    "\n",
    "Why did you get different latencies for different hardware?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
