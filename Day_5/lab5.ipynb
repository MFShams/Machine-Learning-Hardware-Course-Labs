{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHzdt0hGIajB"
      },
      "source": [
        "# Lab 5: Arithmetic and Number Representations\n",
        "## Machine Learning Hardware Course\n",
        "\n",
        "This notebook is Part 1 of the Arithmetic and Number Representations lab. It covers:\n",
        "1. Environment setup\n",
        "2. Quantize from FP32 to int8/6/4, FP16, BF16, FP8_E4M3, FP8_E5M2. Then present the result in bin, and reconstruct back to FP32."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgAzgYbAIajE"
      },
      "source": [
        "## PART 1: ENVIRONMENT SETUP\n",
        "\n",
        "First, we'll set up our environment by installing necessary libraries and checking available hardware.\n",
        "\n",
        "Install specific versions of PyTorch, NumPy, and torchaudio/vision to ensure compatibility.\n",
        "  \n",
        "This ensures consistent results across different systems and environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBG0nthSIajF",
        "outputId": "c7cc9054-f8b9-4148-9504-15d0ca24ebf4"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "!pip uninstall -y torch torchaudio torchvision\n",
        "!pip install torch==2.1.0 torchaudio==2.1.0 torchvision==0.16.0\n",
        "!pip uninstall numpy -y\n",
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuUWZ_ZQc18L"
      },
      "source": [
        "## Check the version of numpy and pytorch\n",
        "\n",
        "Check the versions of installed libraries and GPU availability.  \n",
        "Verifies that the correct software stack and hardware resources are ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4XqkBuknBoV",
        "outputId": "9da69fa5-4296-49fe-b82d-e208eab5538a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.quantization as quant\n",
        "\n",
        "# Check Numpy, Pytorch version and available hardware\n",
        "print(\"Numpy version:\", np.__version__)\n",
        "print(\"Pytorch version:\", torch.__version__)\n",
        "print(\"CUDA version:\", torch.version.cuda)\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlhacX5rb3Qe"
      },
      "source": [
        "It should show as below.  \n",
        "```\n",
        "Numpy version: 1.26.4.     \n",
        "Pytorch version: 2.1.0+cu121.     \n",
        "CUDA version: 12.1.    \n",
        "GPU available: False\n",
        "```\n",
        "\n",
        "If numpy version is 2.0.2 instead of 1.26.4, click Runtime->Restart session and re-run the Version checking block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv4xXq79qlT1",
        "outputId": "5059579f-8ee4-4157-d5f2-7ae16c867fb7"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/liuhao-97/microxcaling.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFjm49Lhvi1a"
      },
      "source": [
        "### Quantize from FP32 to int8, int6, int4\n",
        "\n",
        "Define functions to quantize float32 to int8, int6, int4 and reconstruct back.  \n",
        "Includes MSE computation and bit-level binary formatting.  \n",
        "\n",
        "\n",
        "#### Takeaway: Shows the effect of reducing precision on numeric accuracy.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7cCbV89NZdh",
        "outputId": "888c2613-1be2-477f-a66a-184aead6a9fd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Quantizes a float32 numpy array `x` into integer values using uniform quantization\n",
        "# with the specified bit-width. Returns the quantized values and the scale used.\n",
        "def quantize(x, bits):\n",
        "    if bits < 2:\n",
        "        raise ValueError(\"Bits must be >= 2\")\n",
        "    qmax = 2**(bits - 1) - 1\n",
        "    qmin = -2**(bits - 1)\n",
        "    max_abs = np.max(np.abs(x))\n",
        "    scale = max_abs / qmax if max_abs > 0 else 1e-10 # Avoid division by zero\n",
        "    q = np.round(x / scale).astype(int) # Quantize: scale and round\n",
        "    q = np.clip(q, qmin, qmax)\n",
        "    return q, scale\n",
        "\n",
        "# Reconstructs approximate float32 values from quantized integers using the scale\n",
        "def dequantize(q, scale):\n",
        "    return q * scale\n",
        "\n",
        "# Converts a signed integer `x` into a binary string of length `bits`, two's complement format\n",
        "def int_to_bin(x, bits):\n",
        "    qmin = -2**(bits - 1)\n",
        "    qmax = 2**(bits - 1) - 1\n",
        "    x = np.clip(x, qmin, qmax)\n",
        "    if x < 0:\n",
        "        x = (1 << bits) + x\n",
        "    return format(x, f'0{bits}b')\n",
        "\n",
        "# Example float32 values\n",
        "fp32_values = np.array([-0.3006, -12.2760,   0.7851,   6.6808,  -2.2207,   0.9973, -10.8371,\n",
        "          13.1378,   1.3044,   5.2754], dtype=np.float32)\n",
        "\n",
        "# Quantize and evaluate using different bit-widths\n",
        "for bits in [8, 6, 4,]:\n",
        "    print(f\"\\n==== Quantizing to INT{bits} ====\")\n",
        "    q_vals, scale = quantize(fp32_values, bits)\n",
        "    dq_vals = dequantize(q_vals, scale)\n",
        "    mse = np.mean((fp32_values - dq_vals) ** 2)\n",
        "    mae = np.mean(np.abs(fp32_values - dq_vals))\n",
        "    print(f\"{'FP32':>10} {'INT':>6} {'BINARY':>{bits+2}} {'DEQUANT_FP32':>15}\")\n",
        "    for f, q, dq in zip(fp32_values, q_vals, dq_vals):\n",
        "        binary = int_to_bin(q, bits)\n",
        "        print(f\"{f:10.4f} {q:6} {binary:>{bits+2}} {dq:15.4f}\")\n",
        "    # Print error metrics\n",
        "    print(f\"Quantization MAE: {mae:.6f}\")\n",
        "    print(f\"\\nQuantization MSE: {mse:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixWN1sWYwEgY"
      },
      "source": [
        "### Quantize from FP32 to FP16, BF16, FP8_E4M3, FP8_E5M2\n",
        "\n",
        "Utility functions for converting between FP32 and various formats (FP16, BF16, FP8)     \n",
        "Takeaway: Understand IEEE754-style bit representations for reduced-precision floats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B4fu5I9TNbx0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import struct\n",
        "\n",
        "# ==== Utility Functions to Convert Between Float and Bit Representations ====\n",
        "\n",
        "# Convert float32 to 32-bit integer representation\n",
        "def float32_to_bits(f):\n",
        "    return struct.unpack('>I', struct.pack('>f', f))[0]\n",
        "\n",
        "# Convert 32-bit integer bits back to float32\n",
        "def bits_to_float32(b):\n",
        "    return struct.unpack('>f', struct.pack('>I', b))[0]\n",
        "\n",
        "# Convert float32 to IEEE 754 half-precision (fp16) 16-bit binary\n",
        "def float32_to_fp16(f):\n",
        "    return np.float16(f).view(np.uint16)\n",
        "\n",
        "# Convert fp16 binary back to float32\n",
        "def fp16_to_float32(bits):\n",
        "    return np.float16(bits.view(np.float16)).astype(np.float32)\n",
        "\n",
        "# Convert float32 to bfloat16 (high 16 bits of float32)\n",
        "def float32_to_bf16(f):\n",
        "    bits = float32_to_bits(f)\n",
        "    return bits >> 16\n",
        "\n",
        "# Convert bfloat16 (high 16 bits) back to float32\n",
        "def bf16_to_float32(bf16_bits):\n",
        "    return bits_to_float32(bf16_bits << 16)\n",
        "\n",
        "# ==== Manual FP8 Conversions ====\n",
        "\n",
        "# Convert float32 to FP8 E4M3 (1 sign bit, 4 exponent bits, 3 mantissa bits)\n",
        "def float32_to_fp8_e4m3(f):\n",
        "    f_bits = float32_to_bits(f)\n",
        "    sign = (f_bits >> 31) & 0x1\n",
        "    exponent = (f_bits >> 23) & 0xFF\n",
        "    mantissa = f_bits & 0x7FFFFF\n",
        "\n",
        "    fp32_bias = 127\n",
        "    fp8_bias = 7\n",
        "    exp_unbiased = exponent - fp32_bias + fp8_bias\n",
        "\n",
        "    if exponent == 0 or exp_unbiased <= 0:\n",
        "        return sign << 7  # zero\n",
        "    elif exp_unbiased >= 0b1111:\n",
        "        return (sign << 7) | (0b1111 << 3)  # inf/nan\n",
        "    else:\n",
        "        mant = mantissa >> (23 - 3)\n",
        "        return (sign << 7) | (exp_unbiased << 3) | (mant & 0b111)\n",
        "\n",
        "# Convert FP8 E4M3 8-bit value back to float32\n",
        "def fp8_e4m3_to_float32(fp8):\n",
        "    sign = (fp8 >> 7) & 0x1\n",
        "    exponent = (fp8 >> 3) & 0xF\n",
        "    mantissa = fp8 & 0b111\n",
        "\n",
        "    if exponent == 0 and mantissa == 0:\n",
        "        return -0.0 if sign else 0.0\n",
        "    elif exponent == 0b1111:\n",
        "        return float('-inf') if sign else float('inf')\n",
        "\n",
        "    fp8_bias = 7\n",
        "    fp32_bias = 127\n",
        "    exp = exponent - fp8_bias + fp32_bias\n",
        "    mant = mantissa << (23 - 3)\n",
        "    bits = (sign << 31) | (exp << 23) | mant\n",
        "    return bits_to_float32(bits)\n",
        "\n",
        "# Convert float32 to FP8 E5M2 (1 sign bit, 5 exponent bits, 2 mantissa bits)\n",
        "def float32_to_fp8_e5m2(f):\n",
        "    f_bits = float32_to_bits(f)\n",
        "    sign = (f_bits >> 31) & 0x1\n",
        "    exponent = (f_bits >> 23) & 0xFF\n",
        "    mantissa = f_bits & 0x7FFFFF\n",
        "\n",
        "    fp32_bias = 127\n",
        "    fp8_bias = 15\n",
        "    exp_unbiased = exponent - fp32_bias + fp8_bias\n",
        "\n",
        "    if exponent == 0 or exp_unbiased <= 0:\n",
        "        return sign << 7  # zero\n",
        "    elif exp_unbiased >= 0b11111:\n",
        "        return (sign << 7) | (0b11111 << 2)  # inf/nan\n",
        "    else:\n",
        "        mant = mantissa >> (23 - 2)\n",
        "        return (sign << 7) | (exp_unbiased << 2) | (mant & 0b11)\n",
        "\n",
        "# Convert FP8 E5M2 8-bit value back to float32\n",
        "def fp8_e5m2_to_float32(fp8):\n",
        "    sign = (fp8 >> 7) & 0x1\n",
        "    exponent = (fp8 >> 2) & 0x1F\n",
        "    mantissa = fp8 & 0b11\n",
        "\n",
        "    if exponent == 0 and mantissa == 0:\n",
        "        return -0.0 if sign else 0.0\n",
        "    elif exponent == 0b11111:\n",
        "        return float('-inf') if sign else float('inf')\n",
        "\n",
        "    fp8_bias = 15\n",
        "    fp32_bias = 127\n",
        "    exp = exponent - fp8_bias + fp32_bias\n",
        "    mant = mantissa << (23 - 2)\n",
        "    bits = (sign << 31) | (exp << 23) | mant\n",
        "    return bits_to_float32(bits)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qzJk6xzx0w6"
      },
      "source": [
        "### Run the conversion on sample FP32 values\n",
        "Run the conversion on sample FP32 values and display their bit-level layout. Helpful for visualizing how exponent and mantissa bits change across formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dcthVixxtpF",
        "outputId": "c8df43e1-6082-4000-b9e2-ed3c4da59e2f"
      },
      "outputs": [],
      "source": [
        "# Sample values\n",
        "# ==== Run Example on Sample Float Values ====\n",
        "values = [-0.3006, -12.2760,   0.7851,   6.6808,  -2.2207,   0.9973, -10.8371,\n",
        "          13.1378,   1.3044,   5.2754]\n",
        "\n",
        "# Print original and quantized bit representations\n",
        "# Print header\n",
        "print(f\"{'Value':>8} | {'FP32 (S|E|M)':>32} | {'FP16 (S|E|M)':>20} | {'BF16 (S|E|M)':>20} | {'FP8-E4M3 (S|E|M)':>20} | {'FP8-E5M2 (S|E|M)':>20}\")\n",
        "print(\"-\"*120)\n",
        "for val in values:\n",
        "    fp32_bits = float32_to_bits(val)\n",
        "    fp16_bits = float32_to_fp16(val)\n",
        "    bf16_bits = float32_to_bf16(val)\n",
        "    fp8_e4m3 = float32_to_fp8_e4m3(val)\n",
        "    fp8_e5m2 = float32_to_fp8_e5m2(val)\n",
        "\n",
        "    # Extract sign, exponent, mantissa bits\n",
        "    fp32_s = (fp32_bits >> 31) & 0x1\n",
        "    fp32_e = (fp32_bits >> 23) & 0xFF\n",
        "    fp32_m = fp32_bits & 0x7FFFFF\n",
        "\n",
        "    fp16_s = (fp16_bits >> 15) & 0x1\n",
        "    fp16_e = (fp16_bits >> 10) & 0x1F\n",
        "    fp16_m = fp16_bits & 0x3FF\n",
        "\n",
        "    bf16_s = (bf16_bits >> 15) & 0x1\n",
        "    bf16_e = (bf16_bits >> 7) & 0xFF\n",
        "    bf16_m = bf16_bits & 0x7F\n",
        "\n",
        "    fp8_4_s = (fp8_e4m3 >> 7) & 0x1\n",
        "    fp8_4_e = (fp8_e4m3 >> 3) & 0xF\n",
        "    fp8_4_m = fp8_e4m3 & 0x7\n",
        "\n",
        "    fp8_5_s = (fp8_e5m2 >> 7) & 0x1\n",
        "    fp8_5_e = (fp8_e5m2 >> 2) & 0x1F\n",
        "    fp8_5_m = fp8_e5m2 & 0x3\n",
        "\n",
        "    print(f\"{val:8.4f} | \"\n",
        "          f\"{fp32_s:1b} {fp32_e:08b} {fp32_m:023b} | \"\n",
        "          f\"{fp16_s:1b} {fp16_e:05b} {fp16_m:010b} | \"\n",
        "          f\"{bf16_s:1b} {bf16_e:08b} {bf16_m:07b} | \"\n",
        "          f\"{fp8_4_s:1b} {fp8_4_e:04b} {fp8_4_m:03b}       | \"\n",
        "          f\"{fp8_5_s:1b} {fp8_5_e:05b} {fp8_5_m:02b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTdXvakEyEGC"
      },
      "source": [
        "### Convert reduced-precision values back to FP32\n",
        "Convert reduced-precision values back to FP32 and compare them\n",
        "Takeaway: Observe how rounding and truncation impact numerical accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jcT0APTx8a1",
        "outputId": "887cb61b-56a1-4721-a613-a0ad676f78d7"
      },
      "outputs": [],
      "source": [
        "# Print reconstruction results and compute average error\n",
        "print(\"\\nReconstruction to FP32:\")\n",
        "print(f\"{'Value':>8} | {'From FP16':>10} | {'From BF16':>10} | {'From FP8_E4M3':>10} | {'From FP8_E5M2':>10}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "# Track total errors\n",
        "err_fp16 = 0\n",
        "err_bf16 = 0\n",
        "err_e4m3 = 0\n",
        "err_e5m2 = 0\n",
        "\n",
        "for val in values:\n",
        "    fp16_bits = float32_to_fp16(val)\n",
        "    bf16_bits = float32_to_bf16(val)\n",
        "    fp8_e4m3 = float32_to_fp8_e4m3(val)\n",
        "    fp8_e5m2 = float32_to_fp8_e5m2(val)\n",
        "\n",
        "    f_fp16 = fp16_to_float32(fp16_bits)\n",
        "    f_bf16 = bf16_to_float32(bf16_bits)\n",
        "    f_e4m3 = fp8_e4m3_to_float32(fp8_e4m3)\n",
        "    f_e5m2 = fp8_e5m2_to_float32(fp8_e5m2)\n",
        "\n",
        "    # Print reconstruction\n",
        "    print(f\"{val:8.4f} | {f_fp16:10.4f} | {f_bf16:10.4f} | {f_e4m3:10.4f} | {f_e5m2:10.4f}\")\n",
        "\n",
        "    # Accumulate absolute errors\n",
        "    err_fp16 += abs(val - f_fp16)\n",
        "    err_bf16 += abs(val - f_bf16)\n",
        "    err_e4m3 += abs(val - f_e4m3)\n",
        "    err_e5m2 += abs(val - f_e5m2)\n",
        "\n",
        "# Compute averages\n",
        "n = len(values)\n",
        "print(\"\\nAverage Absolute Errors:\")\n",
        "print(f\"{'FP16':>10}: {err_fp16 / n:.6f}\")\n",
        "print(f\"{'BF16':>10}: {err_bf16 / n:.6f}\")\n",
        "print(f\"{'FP8-E4M3':>10}: {err_e4m3 / n:.6f}\")\n",
        "print(f\"{'FP8-E5M2':>10}: {err_e5m2 / n:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6l_K6dawP2i"
      },
      "source": [
        "## PART 2: Exploring Microscaling (MX) data formats for DNN\n",
        "\n",
        "This notebook is Part 2 of the Arithmetic and Number Representations lab.\n",
        "\n",
        "1. Train a Simple MNIST Linear Model and obtain the original accuracy.\n",
        "2. Quantize and evaluate the output matrix with different MX data formats.\n",
        "3. Quantize and evaluate the matrix multiplication accumulation with different MX data formats.\n",
        "4. Evaluate quantized DNN with different MX data formats.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEyQ95B51cid"
      },
      "source": [
        "### Train a Simple MNIST Linear Model and obtain the original accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1swerV6hVRS",
        "outputId": "54a1443c-743a-48f1-aab1-e20de9b42871"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1. Load and preprocess MNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# 2. Define the model\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = nn.Linear(28 * 28, 10,bias=True)  # 784 inputs (flattened 28x28), 10 outputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the 28x28 images\n",
        "        return self.linear(x)\n",
        "\n",
        "model = LinearModel().to(device)\n",
        "\n",
        "# 3. Set loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# 4. Train the model\n",
        "def train_model(num_epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# 5. Evaluate the model\n",
        "def test_model():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Run training and testing\n",
        "train_model(num_epochs=5)\n",
        "test_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MquA__q5iRm3",
        "outputId": "1c6bef3d-1981-42c5-913d-199046b36a4c"
      },
      "outputs": [],
      "source": [
        "model.eval()  # Set model to evaluation mode\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "      if i>=1:\n",
        "        break\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      print(f\"Sample {i+1}:\")\n",
        "      print(\"Label:\", labels)\n",
        "      print(\"Outputs:\", outputs)\n",
        "      print(\"Predicted:\", predicted)\n",
        "      print(\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9i4FBZdiIHJ",
        "outputId": "cc8b2d11-e2cb-4c74-d863-166b43bddf7a"
      },
      "outputs": [],
      "source": [
        "# Assuming `model` is the trained LinearModel from the previous code\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# 1. Extract weight matrix and bias\n",
        "weight_matrix = model.linear.weight  # Shape: (10, 784)\n",
        "bias_vector = model.linear.bias      # Shape: (10,)\n",
        "\n",
        "# 2. Convert to NumPy (optional)\n",
        "weight_matrix_np = weight_matrix.detach().cpu().numpy()  # Convert to NumPy array\n",
        "bias_vector_np = bias_vector.detach().cpu().numpy()      # Convert to NumPy array\n",
        "# 3. Print shapes and example values\n",
        "print(\"Weight matrix shape:\", weight_matrix.shape)\n",
        "print(\"Bias vector shape:\", bias_vector.shape)\n",
        "print(\"First row of weight matrix (weights for class 0):\\n\", weight_matrix_np[0, :10])  # First 10 weights for class 0\n",
        "print(\"Bias vector:\\n\", bias_vector_np)\n",
        "print(\" \")\n",
        "\n",
        "# Process first samples\n",
        "with torch.no_grad():\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "        if i >= 1:\n",
        "            break\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Flatten images for torch.nn.functional.linear\n",
        "        images_flat = images.view(-1, 28*28)  # Shape: (1, 784)\n",
        "\n",
        "        # Compute output using torch.nn.functional.linear\n",
        "        outputs = torch.nn.functional.linear(input=images_flat, weight=weight_matrix, bias=bias_vector)\n",
        "\n",
        "        # Get predicted class\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Print results in the same format as the second code\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(\"Label:\", labels)\n",
        "        print(\"Outputs:\", outputs)\n",
        "        print(\"Predicted:\", predicted)\n",
        "        print(\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbvOZxrG7qd5"
      },
      "source": [
        "\n",
        "### Quantize and evaluate the output matrix with different MX data formats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "k3E00_qHZbDk"
      },
      "outputs": [],
      "source": [
        "import mx\n",
        "from mx import mx_mapping, MxSpecs, linear, matmul, Linear\n",
        "from mx.specs import finalize_mx_specs\n",
        "from mx.elemwise_ops import _quantize_bfloat\n",
        "from mx.formats import _get_format_params\n",
        "from mx.mx_ops import _quantize_mx, quantize_mx_op\n",
        "import pytest\n",
        "import torch\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYr2M7ojkcPC",
        "outputId": "20a6701a-c60a-4278-efbc-52465b415906"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(43)\n",
        "torch.manual_seed(43)\n",
        "\n",
        "for i, (images, labels) in enumerate(test_loader):\n",
        "    if i == 0:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      # Flatten images for torch.nn.functional.linear\n",
        "      images_flat = images.view(-1, 28*28)  # Shape: (1, 784)\n",
        "      A = images_flat.detach().cpu().numpy()  # Shape: (1, 784)\n",
        "      break\n",
        "\n",
        "# print(A.shape)\n",
        "B = weight_matrix_np\n",
        "C = bias_vector_np\n",
        "\n",
        "# print(B.shape) # Shape:(10, 784)\n",
        "# print(C.shape) # Shape:(10,)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "A_torch = torch.from_numpy(A) # Shape: (1, 784)\n",
        "B_torch = torch.from_numpy(B) # Shape:(10, 784)\n",
        "C_torch = torch.from_numpy(C) # Shape:(10,)\n",
        "\n",
        "# Ground truth with NumPy (FP32)\n",
        "with torch.no_grad():\n",
        "  D_torch = torch.nn.functional.linear(input=images_flat, weight=weight_matrix, bias=bias_vector)\n",
        "D_ground_truth = D_torch.detach().cpu().numpy()\n",
        "print(\"D_ground_truth: \\n\",D_ground_truth)\n",
        "print(\"D_torch: \\n\",D_ground_truth)\n",
        "\n",
        "_, predicted = torch.max(D_torch.data, 1)\n",
        "print(\"Label:\", labels)\n",
        "print(\"Outputs:\", outputs)\n",
        "print(\"Predicted:\", predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjUDayHF2DTH"
      },
      "source": [
        "### Quantize and evaluate the output matrix with different MX data formats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pYEk0FbgkhfK",
        "outputId": "faf01204-b158-4b2c-bb14-7d12fd19ad9e"
      },
      "outputs": [],
      "source": [
        "quant_list = ['int8', 'int4', 'int2', 'fp8_e5m2', 'fp8_e4m3', 'fp6_e3m2', 'fp6_e2m3', 'fp4_e2m1', 'fp16', 'bf16']\n",
        "mae_list = []\n",
        "mse_list = []\n",
        "\n",
        "print(\"Quantization Error Metrics:\")\n",
        "print(f\"{'Method':<12} {'MAE':>12} {'MSE':>12}\")\n",
        "print(\"-\" * 36)\n",
        "\n",
        "for quant_method in quant_list:\n",
        "    D_mx = _quantize_mx(D_torch, 8, quant_method,\n",
        "                        block_size=8,\n",
        "                        axes=[-1],\n",
        "                        round='nearest',\n",
        "                        flush_fp32_subnorms=False,\n",
        "                        custom_cuda=False)\n",
        "\n",
        "    _, mx_predicted = torch.max(D_mx.data, 1)\n",
        "    print(f\"mx_{quant_method} Truth Label: {labels}, Predicted: {mx_predicted}\")\n",
        "\n",
        "    mae = torch.mean(torch.abs(D_mx - D_torch)).item()\n",
        "    mse = torch.mean((D_mx - D_torch) ** 2).item()\n",
        "\n",
        "    mae_list.append(mae)\n",
        "    mse_list.append(mse)\n",
        "\n",
        "    print(f\"mx{quant_method:<12} {mae:12.6f} {mse:12.6f}\")\n",
        "    print()\n",
        "\n",
        "# Visualize the metrics\n",
        "plt.figure(figsize=(17, 10))\n",
        "\n",
        "# MAE\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(quant_list, mae_list)\n",
        "plt.title('MAE Comparison')\n",
        "plt.grid(axis='y')\n",
        "plt.yscale('log')\n",
        "\n",
        "# MSE\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(quant_list, mse_list)\n",
        "plt.title('MSE Comparison')\n",
        "plt.grid(axis='y')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyYs6W3r13_H"
      },
      "source": [
        "### Compare MX_int8 with int8 matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vFTxbd7fkhhg",
        "outputId": "f3d3e77b-5662-40e1-8fef-ab5b9d64f42b"
      },
      "outputs": [],
      "source": [
        "D_mxint8 = _quantize_mx(D_torch, 8, 'int8',\n",
        "                block_size=8,\n",
        "                axes=[-1],\n",
        "                round='nearest',\n",
        "                flush_fp32_subnorms=False,\n",
        "                custom_cuda=False)\n",
        "\n",
        "_, mxint8_predicted = torch.max(D_mxint8.data, 1)\n",
        "print(f\"mxint8 Truth Label: {labels}, Predicted: {mxint8_predicted}\")\n",
        "print(\"D_mxint8:\\n\", D_mxint8 )\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "mae_mxint8=torch.mean(torch.abs(D_mxint8 - D_torch))\n",
        "mse_mxint8=torch.mean((D_mxint8 - D_torch) ** 2)\n",
        "print(\"MAE_mxint8: \",mae_mxint8)\n",
        "print(\"MSE_mxint8: \",mse_mxint8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "D_int8 = D_torch.type(torch.int8)\n",
        "_, int8_predicted = torch.max(D_mxint8.data, 1)\n",
        "print(f\"int8 Truth Label: {labels}, Predicted: {int8_predicted}\")\n",
        "print(\"D_int8:\\n\", D_int8)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "mae_int8=torch.mean(torch.abs(D_int8 - D_torch))\n",
        "mse_int8=torch.mean((D_int8 - D_torch) ** 2)\n",
        "print(\"MAE_int8: \", mae_int8)\n",
        "print(\"MSE_int8: \", mse_int8)\n",
        "\n",
        "\n",
        "# Visualize the metrics\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# MAE\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(['mxint8', 'int8'],[mae_mxint8,mae_int8])\n",
        "plt.title('MAE Comparison')\n",
        "plt.grid(axis='y')\n",
        "plt.yscale('log')\n",
        "\n",
        "# Training time comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(['mxint8', 'int8'],[mse_mxint8,mse_int8])\n",
        "plt.title('MSE Comparison')\n",
        "plt.grid(axis='y')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-19c-hP85au"
      },
      "source": [
        "### Test MX_int8 for various block sizes\n",
        "Plot and analyze how error trends vary with block size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9CplcObtkhjz",
        "outputId": "84ad9771-c3d5-4454-a2b7-abc031e9c831"
      },
      "outputs": [],
      "source": [
        "blocksize_list = [1,2,3,4,6,7,8,9,10,]\n",
        "mae_list = []\n",
        "mse_list = []\n",
        "\n",
        "for blocksize in blocksize_list:\n",
        "  D_mx = _quantize_mx(D_torch, 32, 'int8',\n",
        "                  block_size=blocksize,\n",
        "                  axes=[-1],\n",
        "                  round='nearest',\n",
        "                  flush_fp32_subnorms=False,\n",
        "                  custom_cuda=False)\n",
        "  _, mx_predicted = torch.max(D_mx.data, 1)\n",
        "  print(f\"mxint8 blocksize: {blocksize}, Truth Label: {labels}, Predicted: {mx_predicted}\")\n",
        "  # print(\"D_mx:\\n\", D_mx)\n",
        "  print(\"\\n\")\n",
        "\n",
        "  mae_list.append(torch.mean(torch.abs(D_mx - D_torch)))\n",
        "  mse_list.append(torch.mean((D_mx - D_torch) ** 2))\n",
        "  print(f\"blocksize {blocksize}: \")\n",
        "  print(f\"MAE: {torch.mean(torch.abs(D_mx - D_torch))}\")\n",
        "  print(f\"MSE: {torch.mean((D_mx - D_torch) ** 2)}\\n\")\n",
        "\n",
        "# Visualize the metrics\n",
        "plt.figure(figsize=(17, 10))\n",
        "\n",
        "# MAE\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(blocksize_list,mae_list,markersize=10, marker='*')\n",
        "plt.xlabel('block size', size=20)\n",
        "plt.ylabel('MAE', size=20)\n",
        "plt.title('MAE Comparison')\n",
        "# plt.ylim(0.06, 0.067)  # Adjust as needed\n",
        "plt.grid(axis='y')\n",
        "\n",
        "# Training time comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(blocksize_list,mse_list,markersize=10, marker='*')\n",
        "plt.title('MSE Comparison')\n",
        "plt.xlabel('block size', size=20)\n",
        "plt.ylabel('MSE', size=20)\n",
        "# plt.ylim(0.005, 0.0062)  # Adjust as needed\n",
        "plt.grid(axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Kv30hH4CkhmP"
      },
      "outputs": [],
      "source": [
        "# Define helper function for matrix multiplication accumulation\n",
        "def matmul_accumulate(A, B, C, mx_specs=None):\n",
        "    if mx_specs is None:\n",
        "        # Native PyTorch FP32\n",
        "        # return torch.matmul(A, B) + C\n",
        "        return torch.nn.functional.linear(A, B, C)\n",
        "    else:\n",
        "        # Use microxcaling's linear function for quantized computation\n",
        "        return mx.linear(A, B, C, mx_specs=mx_specs)\n",
        "\n",
        "# Compute accuracy metrics\n",
        "def compute_errors(ground_truth, result):\n",
        "    result_np = result.cpu().numpy()  # Move back to CPU for comparison\n",
        "    abs_error = np.abs(ground_truth - result_np)\n",
        "    mae = np.mean(abs_error)\n",
        "    maxae = np.max(abs_error)\n",
        "    return mae, maxae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p30kCWgekhoW",
        "outputId": "30963854-192a-409b-9698-3fd96e69fc80"
      },
      "outputs": [],
      "source": [
        "# Run with different data types\n",
        "# FP32 (no quantization)\n",
        "fp32_result = matmul_accumulate(A_torch, B_torch, C_torch, mx_specs=None)\n",
        "\n",
        "fp32_mae, fp32_maxae = compute_errors(D_ground_truth, fp32_result)\n",
        "print(\"Accuracy Comparison:\")\n",
        "print(f\"FP32 - MAE: {fp32_mae:.6f}, MaxAE: {fp32_maxae:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2X0-w579PQH"
      },
      "source": [
        "### Test mx_int quantization (mx_int8/mx_int4/mx_int2) for various block sizes\n",
        "   \n",
        "Plot and analyze how error trends vary with block size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GOgfj5JPdZ3M",
        "outputId": "e7ae1318-b042-4c92-d8f4-7f537cff2901"
      },
      "outputs": [],
      "source": [
        "# Visualize the metrics\n",
        "plt.figure(figsize=(17, 10))\n",
        "quant_list = ['int8', 'int4', 'int2']\n",
        "blocksize_list = [2,4,8,12,16,24,32,40,48,56,64]\n",
        "for quant_method in quant_list:\n",
        "  mae_list=[]\n",
        "  mse_list=[]\n",
        "  for blocksize in blocksize_list:\n",
        "    mxint_specs=MxSpecs()\n",
        "    mxint_specs['w_elem_format'] = quant_method\n",
        "    mxint_specs['a_elem_format'] = quant_method\n",
        "    mxint_specs['block_size'] = blocksize\n",
        "    mxint_specs['shared_exp_method'] = 'max'\n",
        "\n",
        "    mxint_result = matmul_accumulate(A_torch, B_torch, C_torch, mx_specs=mxint_specs)\n",
        "    mae_list.append(torch.mean(torch.abs(mxint_result - D_torch)))\n",
        "    mse_list.append(torch.mean((mxint_result - D_torch) ** 2))\n",
        "\n",
        "    print(f\"mx_{quant_method} blocksize {blocksize}\")\n",
        "    mae = torch.mean(torch.abs(mxint_result - D_torch)).item()\n",
        "    mse = torch.mean((mxint_result - D_torch) ** 2).item()\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"MSE: {mse:.4f}\\n\")\n",
        "\n",
        "    _, mxint_predicted = torch.max(mxint_result.data, 1)\n",
        "    # print(f\"mx{quant_method} blocksize: {blocksize}, Truth Label: {labels}, Predicted: {mxint_predicted}\")\n",
        "    # print(f\"mx{quant_method}_result:\\n\", mxint_result)\n",
        "    print(\"\\n\")\n",
        "\n",
        "  # MAE\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(blocksize_list,mae_list,markersize=10, marker='*', label=quant_method)\n",
        "  plt.xlabel('block size', size=20)\n",
        "  plt.ylabel('MAE', size=20)\n",
        "  plt.title('MAE Comparison')\n",
        "  plt.grid(axis='y')\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(blocksize_list,mse_list,markersize=10, marker='*', label=quant_method)\n",
        "  plt.xlabel('block size', size=20)\n",
        "  plt.ylabel('MSE', size=20)\n",
        "  plt.title('MSE Comparison')\n",
        "  plt.grid(axis='y')\n",
        "\n",
        "plt.legend(fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JwNDwZe9djj"
      },
      "source": [
        "### Test mx_fp quantization mx_(fp8_e5m2, fp8_e4m3, fp6_e3m2, fp6_e2m3, fp4_e2m1,  fp16, bf16) for various block sizes\n",
        "   \n",
        "Plot and analyze how error trends vary with block size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KlBmAcOQet-l",
        "outputId": "0fff9df1-5efd-4569-8b00-e48c5813d633"
      },
      "outputs": [],
      "source": [
        "# Visualize the metrics\n",
        "plt.figure(figsize=(17, 10))\n",
        "quant_list = ['fp8_e5m2', 'fp8_e4m3', 'fp6_e3m2', 'fp6_e2m3', 'fp4_e2m1',  'fp16', 'bf16']\n",
        "\n",
        "blocksize_list = [4, 8,16,24,32,40,48,56,64,]\n",
        "for quant_method in quant_list:\n",
        "  mae_list=[]\n",
        "  mse_list=[]\n",
        "  for blocksize in blocksize_list:\n",
        "    mxfp_specs=MxSpecs()\n",
        "    mxfp_specs['scale_bits']=8\n",
        "    mxfp_specs['w_elem_format'] = quant_method\n",
        "    mxfp_specs['a_elem_format'] = quant_method\n",
        "    mxfp_specs['block_size'] = blocksize\n",
        "    mxfp_specs['shared_exp_method'] = 'max'\n",
        "    mxfp_specs = finalize_mx_specs(mxfp_specs)\n",
        "\n",
        "    mxfp_result = matmul_accumulate(A_torch, B_torch, C_torch, mx_specs=mxfp_specs)\n",
        "    mae_list.append(torch.mean(torch.abs(mxfp_result - D_torch)))\n",
        "    mse_list.append(torch.mean((mxfp_result - D_torch) ** 2))\n",
        "\n",
        "    print(f\"mx_{quant_method} blocksize {blocksize}\")\n",
        "    print(f\"MAE: {torch.mean(torch.abs(mxint_result - D_torch))}\")\n",
        "    print(f\"MSE: {torch.mean((mxint_result - D_torch) ** 2)}\\n\")\n",
        "\n",
        "    _, mxfp_predicted = torch.max(mxfp_result.data, 1)\n",
        "    # print(f\"mx{quant_method} blocksize: {blocksize}, Truth Label: {labels}, Predicted: {mxfp_predicted}\")\n",
        "    # print(f\"mx{quant_method}_result:\\n\", mxfp_result)\n",
        "    print(\"\\n\")\n",
        "\n",
        "  # MAE\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(blocksize_list,mae_list,markersize=10, marker='*', label=quant_method)\n",
        "  plt.title('MAE Comparison')\n",
        "  plt.grid(axis='y')\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(blocksize_list,mse_list,markersize=10, marker='*', label=quant_method)\n",
        "  plt.title('MSE Comparison')\n",
        "  plt.grid(axis='y')\n",
        "\n",
        "# plt.grid(axis='y')\n",
        "plt.legend(fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rnSFIgX9wD1"
      },
      "source": [
        "### Evaluate quantized DNN with different MX data formats.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k6l4vj8X61NU",
        "outputId": "969492ee-0d17-4901-fe97-6e3563fbef6a"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "def test_model():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Original Test Accuracy: {accuracy:.2f}\")\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def test_model_mx(mx_specs):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for i, (images, labels) in enumerate(test_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Flatten images for torch.nn.functional.linear\n",
        "        images_flat = images.view(-1, 28*28)  # Shape: (1, 784)\n",
        "\n",
        "        # Compute output using torch.nn.functional.linear\n",
        "        # outputs = torch.nn.functional.linear(input=images_flat, weight=weight_matrix, bias=bias_vector)\n",
        "        outputs = mx.linear(images_flat, B_torch, C_torch, mx_specs=mx_specs)\n",
        "        # Get predicted class\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "  accuracy = correct / total\n",
        "  # print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "  return accuracy\n",
        "\n",
        "# Re-evaluate model accuracy using original (full-precision) weights\n",
        "# This is the baseline for assessing quantized inference accuracy\n",
        "original_accuracy = test_model()\n",
        "\n",
        "# Visualize the metrics\n",
        "plt.figure(figsize=(17, 10))\n",
        "\n",
        "quant_list = ['int8', 'int4', 'int2','fp8_e5m2', 'fp8_e4m3', 'fp6_e3m2', 'fp6_e2m3', 'fp4_e2m1',  'fp16', 'bf16']\n",
        "blocksize_list = [16,32,]\n",
        "\n",
        "# Define test_model_mx: inference with quantized formats using microxcaling\n",
        "# Used to simulate low-bit inference performance\n",
        "for blocksize in blocksize_list:\n",
        "  accuracy_list=[]\n",
        "  for quant_method in quant_list:\n",
        "    mxint_specs=MxSpecs()\n",
        "    mxint_specs['w_elem_format'] = quant_method\n",
        "    mxint_specs['a_elem_format'] = quant_method\n",
        "    mxint_specs['block_size'] = blocksize\n",
        "    mxint_specs['shared_exp_method'] = 'max'\n",
        "    accuracy = test_model_mx(mxint_specs)\n",
        "    accuracy_list.append(accuracy)\n",
        "    print(f\"quant method:mx{quant_method}, blocksize: {blocksize}, accuracy: {accuracy}\")\n",
        "\n",
        "  if blocksize == 16:\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.bar(['original_accuracy']+quant_list, [original_accuracy] + accuracy_list, )\n",
        "    plt.xlabel('quant method', size=20)\n",
        "    plt.ylabel('accuracy', size=20)\n",
        "    plt.title(f'Accuracy Comparison (blocksize {blocksize})')\n",
        "    plt.grid(axis='y')\n",
        "    # plt.ylim(0.84,1)\n",
        "\n",
        "  if blocksize == 32:\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.bar(['original_accuracy']+quant_list, [original_accuracy] + accuracy_list, )\n",
        "    plt.xlabel('quant method', size=20)\n",
        "    plt.ylabel('accuracy', size=20)\n",
        "    plt.title(f'Accuracy Comparison (blocksize {blocksize})')\n",
        "    plt.grid(axis='y')\n",
        "    # plt.ylim(0.7,1)\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
