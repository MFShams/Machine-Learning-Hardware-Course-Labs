{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Fully Connected Neural Networks (FCNN)\n",
    "## Machine Learning Hardware Course\n",
    "\n",
    "This lab focuses on the architecture, implementation, and hardware implications of Fully Connected Neural Networks (FCNNs). You will implement FCNNs of varying depths and widths, experiment with different activation functions and regularization techniques, and analyze how architectural choices impact performance, computational requirements, and hardware efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: ENVIRONMENT SETUP\n",
    "\n",
    "In this section, we'll set up our environment and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Check TensorFlow version and GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# If psutil is not installed, uncomment this line:\n",
    "# !pip install psutil\n",
    "\n",
    "# Mount Google Drive (uncomment when running in Colab)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !mkdir -p \"/content/drive/My Drive/ML_Hardware_Course/Lab2\"\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: DATASET PREPARATION\n",
    "\n",
    "In this section, we'll load and prepare the MNIST and Fashion MNIST datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_and_prepare_mnist():\n",
    "    \"\"\"\n",
    "    Load and prepare the MNIST dataset for training FCNNs.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Training, validation, and test data along with test labels\n",
    "    \"\"\"\n",
    "    # Load MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Print original data shapes\n",
    "    print(\"Original MNIST shapes:\")\n",
    "    print(f\"  X_train: {x_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"  X_test: {x_test.shape}, y_test: {y_test.shape}\")\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    \n",
    "    # Reshape data (flatten images)\n",
    "    x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
    "    x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    y_train_encoded = to_categorical(y_train, 10)\n",
    "    y_test_encoded = to_categorical(y_test, 10)\n",
    "    \n",
    "    # Create validation set (10% of training data)\n",
    "    val_size = 6000\n",
    "    x_val = x_train_flat[-val_size:]\n",
    "    y_val = y_train_encoded[-val_size:]\n",
    "    x_train_final = x_train_flat[:-val_size]\n",
    "    y_train_final = y_train_encoded[:-val_size]\n",
    "    \n",
    "    print(\"\\nProcessed MNIST dataset:\")\n",
    "    print(f\"  Training set: {x_train_final.shape}\")\n",
    "    print(f\"  Validation set: {x_val.shape}\")\n",
    "    print(f\"  Test set: {x_test_flat.shape}\")\n",
    "    \n",
    "    return (x_train_final, y_train_final), (x_val, y_val), (x_test_flat, y_test_encoded), y_test\n",
    "\n",
    "def load_and_prepare_fashion_mnist():\n",
    "    \"\"\"\n",
    "    Load and prepare the Fashion MNIST dataset for training FCNNs.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Training, validation, and test data along with test labels\n",
    "    \"\"\"\n",
    "    # Load Fashion MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    \n",
    "    # Print original data shapes\n",
    "    print(\"\\nOriginal Fashion MNIST shapes:\")\n",
    "    print(f\"  X_train: {x_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"  X_test: {x_test.shape}, y_test: {y_test.shape}\")\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    \n",
    "    # Reshape data (flatten images)\n",
    "    x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
    "    x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    y_train_encoded = to_categorical(y_train, 10)\n",
    "    y_test_encoded = to_categorical(y_test, 10)\n",
    "    \n",
    "    # Create validation set (10% of training data)\n",
    "    val_size = 6000\n",
    "    x_val = x_train_flat[-val_size:]\n",
    "    y_val = y_train_encoded[-val_size:]\n",
    "    x_train_final = x_train_flat[:-val_size]\n",
    "    y_train_final = y_train_encoded[:-val_size:]\n",
    "    \n",
    "    print(\"\\nProcessed Fashion MNIST dataset:\")\n",
    "    print(f\"  Training set: {x_train_final.shape}\")\n",
    "    print(f\"  Validation set: {x_val.shape}\")\n",
    "    print(f\"  Test set: {x_test_flat.shape}\")\n",
    "    \n",
    "    return (x_train_final, y_train_final), (x_val, y_val), (x_test_flat, y_test_encoded), y_test\n",
    "\n",
    "# Define class names for Fashion MNIST\n",
    "fashion_class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                      'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Define class names for MNIST\n",
    "mnist_class_names = [str(i) for i in range(10)]\n",
    "\n",
    "# Load both datasets\n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist_train, mnist_val, mnist_test, mnist_y_test = load_and_prepare_mnist()\n",
    "\n",
    "print(\"\\nLoading Fashion MNIST dataset...\")\n",
    "fashion_train, fashion_val, fashion_test, fashion_y_test = load_and_prepare_fashion_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display sample images from both datasets\n",
    "def display_samples(dataset_name, x_data, y_data, class_names, num_samples=5):\n",
    "    \"\"\"\n",
    "    Display sample images from a dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset\n",
    "        x_data: Image data (flattened)\n",
    "        y_data: One-hot encoded labels\n",
    "        class_names: List of class names\n",
    "        num_samples: Number of samples to display\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(num_samples):\n",
    "        idx = np.random.randint(0, len(x_data))\n",
    "        plt.subplot(1, num_samples, i+1)\n",
    "        plt.imshow(x_data[idx].reshape(28, 28), cmap='gray')\n",
    "        class_idx = np.argmax(y_data[idx])\n",
    "        plt.title(f\"{class_names[class_idx]}\")\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f\"{dataset_name} Samples\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display sample images\n",
    "print(\"\\nDisplaying MNIST samples:\")\n",
    "display_samples(\"MNIST\", mnist_train[0], mnist_train[1], mnist_class_names)\n",
    "\n",
    "print(\"\\nDisplaying Fashion MNIST samples:\")\n",
    "display_samples(\"Fashion MNIST\", fashion_train[0], fashion_train[1], fashion_class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: FULLY CONNECTED NEURAL NETWORK IMPLEMENTATION\n",
    "\n",
    "In this section, we'll implement functions to create and train Fully Connected Neural Networks with various architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_fcnn(input_dim, hidden_layers, hidden_units, activation='relu', dropout_rate=0.0):\n",
    "    \"\"\"\n",
    "    Create a Fully Connected Neural Network with specified architecture.\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Input dimension (e.g., 784 for MNIST)\n",
    "        hidden_layers: Number of hidden layers\n",
    "        hidden_units: List or int specifying neurons in each hidden layer\n",
    "        activation: Activation function to use\n",
    "        dropout_rate: Dropout rate (0 = no dropout)\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential(name=f\"FCNN_L{hidden_layers}_U{hidden_units if isinstance(hidden_units, int) else '-'.join(map(str, hidden_units))}\")\n",
    "    \n",
    "    # Convert hidden_units to list if it's an integer\n",
    "    if isinstance(hidden_units, int):\n",
    "        hidden_units = [hidden_units] * hidden_layers\n",
    "    \n",
    "    # Ensure we have enough hidden_units specified\n",
    "    if len(hidden_units) < hidden_layers:\n",
    "        hidden_units = hidden_units + [hidden_units[-1]] * (hidden_layers - len(hidden_units))\n",
    "    \n",
    "    # Add input layer\n",
    "    model.add(Dense(hidden_units[0], activation=activation, input_shape=(input_dim,),\n",
    "                   name=f'dense_1_{hidden_units[0]}'))\n",
    "    \n",
    "    # Add dropout if specified\n",
    "    if dropout_rate > 0:\n",
    "        model.add(Dropout(dropout_rate, name=f'dropout_1_{dropout_rate}'))\n",
    "    \n",
    "    # Add additional hidden layers\n",
    "    for i in range(1, hidden_layers):\n",
    "        model.add(Dense(hidden_units[i], activation=activation, \n",
    "                       name=f'dense_{i+1}_{hidden_units[i]}'))\n",
    "        \n",
    "        # Add dropout if specified\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate, name=f'dropout_{i+1}_{dropout_rate}'))\n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(Dense(10, activation='softmax', name='output'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_model(model, train_data, val_data, test_data, model_name, \n",
    "                            batch_size=128, epochs=20, patience=3, verbose=1):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model, and calculate performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Compiled Keras model\n",
    "        train_data: Tuple of (x_train, y_train)\n",
    "        val_data: Tuple of (x_val, y_val)\n",
    "        test_data: Tuple of (x_test, y_test)\n",
    "        model_name: Name for logging\n",
    "        batch_size: Batch size for training\n",
    "        epochs: Maximum number of epochs\n",
    "        patience: Early stopping patience\n",
    "        verbose: Verbosity level for training\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary of results and metrics\n",
    "    \"\"\"\n",
    "    x_train, y_train = train_data\n",
    "    x_val, y_val = val_data\n",
    "    x_test, y_test = test_data\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=(1 if verbose > 0 else 0)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_val, y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Calculate training time\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    # Measure inference time (average over 1000 samples)\n",
    "    inference_samples = min(1000, len(x_test))\n",
    "    start_time = time.time()\n",
    "    _ = model.predict(x_test[:inference_samples], verbose=0)\n",
    "    inference_time = (time.time() - start_time) / inference_samples  # per sample\n",
    "    \n",
    "    # Count parameters\n",
    "    trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    non_trainable_params = np.sum([np.prod(v.shape) for v in model.non_trainable_weights])\n",
    "    total_params = trainable_params + non_trainable_params\n",
    "    \n",
    "    # Calculate train-validation gap (for overfitting analysis)\n",
    "    train_acc = max(history.history['accuracy'])\n",
    "    val_acc = max(history.history['val_accuracy'])\n",
    "    train_val_gap = train_acc - val_acc\n",
    "    \n",
    "    # Calculate efficiency metrics\n",
    "    params_per_second = total_params / training_time\n",
    "    accuracy_per_million_params = test_accuracy * 100 / (total_params / 1e6)\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'history': history,\n",
    "        'training_time': training_time,\n",
    "        'test_accuracy': test_accuracy * 100,  # convert to percentage\n",
    "        'test_loss': test_loss,\n",
    "        'inference_time': inference_time * 1000,  # convert to milliseconds\n",
    "        'total_params': total_params,\n",
    "        'trainable_params': trainable_params,\n",
    "        'params_per_second': params_per_second,\n",
    "        'accuracy_per_million_params': accuracy_per_million_params,\n",
    "        'epochs_trained': len(history.history['accuracy']),\n",
    "        'train_val_gap': train_val_gap,\n",
    "        'batch_size': batch_size\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n--- {model_name} Results ---\")\n",
    "    print(f\"Test Accuracy: {results['test_accuracy']:.2f}%\")\n",
    "    print(f\"Training Time: {results['training_time']:.2f} seconds\")\n",
    "    print(f\"Inference Time: {results['inference_time']:.4f} ms\")\n",
    "    print(f\"Total Parameters: {results['total_params']:,}\")\n",
    "    print(f\"Epochs Trained: {results['epochs_trained']}\")\n",
    "    print(f\"Train-Val Gap: {results['train_val_gap']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4: VISUALIZATION AND ANALYSIS FUNCTIONS\n",
    "\n",
    "Here we implement functions for visualizing and analyzing model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_training_history(histories, labels=None):\n",
    "    \"\"\"\n",
    "    Plot training history for multiple models.\n",
    "    \n",
    "    Args:\n",
    "        histories: List of history objects or results dictionaries\n",
    "        labels: List of model names\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i, hist in enumerate(histories):\n",
    "        history = hist['history'] if isinstance(hist, dict) else hist\n",
    "        label = labels[i] if labels else f\"Model {i+1}\"\n",
    "        plt.plot(history.history['accuracy'], label=f\"{label} - Train\")\n",
    "        plt.plot(history.history['val_accuracy'], label=f\"{label} - Val\")\n",
    "    \n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i, hist in enumerate(histories):\n",
    "        history = hist['history'] if isinstance(hist, dict) else hist\n",
    "        label = labels[i] if labels else f\"Model {i+1}\"\n",
    "        plt.plot(history.history['loss'], label=f\"{label} - Train\")\n",
    "        plt.plot(history.history['val_loss'], label=f\"{label} - Val\")\n",
    "    \n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(model, x_test, y_test_true, class_names, title):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix for model predictions.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        x_test: Test inputs\n",
    "        y_test_true: True labels (not one-hot encoded)\n",
    "        class_names: List of class names\n",
    "        title: Plot title\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Confusion matrix and most confused pair\n",
    "    \"\"\"\n",
    "    # Generate predictions\n",
    "    y_pred = model.predict(x_test, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test_true, y_pred_classes)\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find most confused pairs\n",
    "    cm_copy = cm.copy()\n",
    "    np.fill_diagonal(cm_copy, 0)  # Ignore correct classifications\n",
    "    max_confusion = np.unravel_index(np.argmax(cm_copy), cm_copy.shape)\n",
    "    print(f\"Most confused pair: {class_names[max_confusion[0]]} mistaken for {class_names[max_confusion[1]]} ({cm_copy[max_confusion]} times)\")\n",
    "    \n",
    "    return cm, max_confusion\n",
    "\n",
    "def profile_memory_usage(model, x_input, batch_size=32):\n",
    "    \"\"\"\n",
    "    Profile memory usage during model inference.\n",
    "    \n",
    "    Args:\n",
    "        model: Keras model\n",
    "        x_input: Input data\n",
    "        batch_size: Batch size for inference\n",
    "    \n",
    "    Returns:\n",
    "        dict: Memory usage metrics\n",
    "    \"\"\"\n",
    "    # Record baseline memory usage\n",
    "    baseline_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # Warm-up run\n",
    "    _ = model.predict(x_input[:batch_size], verbose=0)\n",
    "    \n",
    "    # Record memory usage during inference\n",
    "    peak_memory = baseline_memory\n",
    "    \n",
    "    for i in range(0, min(1000, len(x_input)), batch_size):\n",
    "        batch = x_input[i:i+batch_size]\n",
    "        _ = model.predict(batch, verbose=0)\n",
    "        current_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB\n",
    "        peak_memory = max(peak_memory, current_memory)\n",
    "    \n",
    "    memory_results = {\n",
    "        'baseline_memory_mb': baseline_memory,\n",
    "        'peak_memory_mb': peak_memory,\n",
    "        'memory_increase_mb': peak_memory - baseline_memory\n",
    "    }\n",
    "    \n",
    "    print(f\"Memory profiling results:\")\n",
    "    print(f\"  Baseline Memory: {memory_results['baseline_memory_mb']:.2f} MB\")\n",
    "    print(f\"  Peak Memory: {memory_results['peak_memory_mb']:.2f} MB\")\n",
    "    print(f\"  Memory Increase: {memory_results['memory_increase_mb']:.2f} MB\")\n",
    "    \n",
    "    return memory_results\n",
    "\n",
    "def is_pareto_efficient(costs):\n",
    "    \"\"\"\n",
    "    Find the Pareto-efficient points.\n",
    "    \n",
    "    Args:\n",
    "        costs: An (n_points, n_costs) array\n",
    "    \n",
    "    Returns:\n",
    "        np.array: A boolean array of Pareto-efficient points\n",
    "    \"\"\"\n",
    "    is_efficient = np.ones(costs.shape[0], dtype=bool)\n",
    "    for i, c in enumerate(costs):\n",
    "        if is_efficient[i]:\n",
    "            # Keep any point with a lower cost in at least one dimension\n",
    "            is_efficient[is_efficient] = np.any(costs[is_efficient] < c, axis=1) | np.all(costs[is_efficient] == c, axis=1)\n",
    "    return is_efficient\n",
    "\n",
    "def plot_metric_comparison(results_df, x_metric, y_metric, title, annotate=True):\n",
    "    \"\"\"\n",
    "    Plot a comparison of two metrics across models.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with results\n",
    "        x_metric: Column name for x-axis\n",
    "        y_metric: Column name for y-axis\n",
    "        title: Plot title\n",
    "        annotate: Whether to annotate points with model names\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(results_df[x_metric], results_df[y_metric], s=100, alpha=0.7)\n",
    "    \n",
    "    if annotate:\n",
    "        for i, row in results_df.iterrows():\n",
    "            model_name = row['Model'].replace('FCNN_', '')\n",
    "            plt.annotate(model_name, \n",
    "                        (row[x_metric], row[y_metric]),\n",
    "                        xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_metric)\n",
    "    plt.ylabel(y_metric)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 5: EXPERIMENT - NETWORK DEPTH VARIATION\n",
    "\n",
    "In this experiment, we'll examine how varying the number of hidden layers affects model performance and computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENT 1: VARYING NETWORK DEPTH\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Experiment with different network depths\n",
    "depth_results = []\n",
    "depth_histories = []\n",
    "depth_models = []\n",
    "depth_names = []\n",
    "\n",
    "# Create models with different depths (1, 2, 3, 4 hidden layers)\n",
    "for num_layers in [1, 2, 3, 4]:\n",
    "    model_name = f\"FCNN_Depth_{num_layers}\"\n",
    "    \n",
    "    model = create_fcnn(\n",
    "        input_dim=784,\n",
    "        hidden_layers=num_layers,\n",
    "        hidden_units=128,\n",
    "        activation='relu',\n",
    "        dropout_rate=0.2\n",
    "    )\n",
    "    \n",
    "    result = train_and_evaluate_model(\n",
    "        model=model,\n",
    "        train_data=mnist_train,\n",
    "        val_data=mnist_val,\n",
    "        test_data=mnist_test,\n",
    "        model_name=model_name,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    depth_results.append(result)\n",
    "    depth_histories.append(result['history'])\n",
    "    depth_models.append(model)\n",
    "    depth_names.append(model_name)\n",
    "\n",
    "# Plot training history for different depths\n",
    "print(\"\\nTraining history comparison for different network depths:\")\n",
    "plot_training_history(depth_histories, depth_names)\n",
    "\n",
    "# Create results table\n",
    "depth_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': result['model_name'],\n",
    "        'Depth': i+1,\n",
    "        'Accuracy (%)': result['test_accuracy'],\n",
    "        'Training Time (s)': result['training_time'],\n",
    "        'Inference Time (ms)': result['inference_time'],\n",
    "        'Parameters': result['total_params'],\n",
    "        'Params/Second': result['params_per_second'],\n",
    "        'Accuracy/Million Params': result['accuracy_per_million_params'],\n",
    "        'Train-Val Gap': result['train_val_gap']\n",
    "    }\n",
    "    for i, result in enumerate(depth_results)\n",
    "])\n",
    "\n",
    "print(\"\\nDepth Experiment Results:\")\n",
    "print(depth_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 6: EXPERIMENT - NETWORK WIDTH VARIATION\n",
    "\n",
    "In this experiment, we'll examine how varying the number of neurons in each layer affects model performance and computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENT 2: VARYING NETWORK WIDTH\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Experiment with different network widths\n",
    "width_results = []\n",
    "width_histories = []\n",
    "width_models = []\n",
    "width_names = []\n",
    "\n",
    "# Create models with different widths (64, 128, 256, 512 neurons)\n",
    "for width in [64, 128, 256, 512]:\n",
    "    model_name = f\"FCNN_Width_{width}\"\n",
    "    \n",
    "    model = create_fcnn(\n",
    "        input_dim=784,\n",
    "        hidden_layers=2,\n",
    "        hidden_units=width,\n",
    "        activation='relu',\n",
    "        dropout_rate=0.2\n",
    "    )\n",
    "    \n",
    "    result = train_and_evaluate_model(\n",
    "        model=model,\n",
    "        train_data=mnist_train,\n",
    "        val_data=mnist_val,\n",
    "        test_data=mnist_test,\n",
    "        model_name=model_name,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    width_results.append(result)\n",
    "    width_histories.append(result['history'])\n",
    "    width_models.append(model)\n",
    "    width_names.append(model_name)\n",
    "\n",
    "# Plot training history for different widths\n",
    "print(\"\\nTraining history comparison for different network widths:\")\n",
    "plot_training_history(width_histories, width_names)\n",
    "\n",
    "# Create results table\n",
    "width_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': result['model_name'],\n",
    "        'Width': [64, 128, 256, 512][i],\n",
    "        'Accuracy (%)': result['test_accuracy'],\n",
    "        'Training Time (s)': result['training_time'],\n",
    "        'Inference Time (ms)': result['inference_time'],\n",
    "        'Parameters': result['total_params'],\n",
    "        'Params/Second': result['params_per_second'],\n",
    "        'Accuracy/Million Params': result['accuracy_per_million_params'],\n",
    "        'Train-Val Gap': result['train_val_gap']\n",
    "    }\n",
    "    for i, result in enumerate(width_results)\n",
    "])\n",
    "\n",
    "print(\"\\nWidth Experiment Results:\")\n",
    "print(width_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 7: EXPERIMENT - ACTIVATION FUNCTIONS\n",
    "\n",
    "In this experiment, we'll examine how different activation functions affect model performance and training dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENT 3: VARYING ACTIVATION FUNCTIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Experiment with different activation functions\n",
    "activation_results = []\n",
    "activation_histories = []\n",
    "activation_models = []\n",
    "activation_names = []\n",
    "\n",
    "# Create models with different activation functions\n",
    "for activation in ['relu', 'sigmoid', 'tanh', 'elu']:\n",
    "    model_name = f\"FCNN_Activation_{activation}\"\n",
    "    \n",
    "    model = create_fcnn(\n",
    "        input_dim=784,\n",
    "        hidden_layers=2,\n",
    "        hidden_units=128,\n",
    "        activation=activation,\n",
    "        dropout_rate=0.2\n",
    "    )\n",
    "    \n",
    "    result = train_and_evaluate_model(\n",
    "        model=model,\n",
    "        train_data=mnist_train,\n",
    "        val_data=mnist_val,\n",
    "        test_data=mnist_test,\n",
    "        model_name=model_name,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    activation_results.append(result)\n",
    "    activation_histories.append(result['history'])\n",
    "    activation_models.append(model)\n",
    "    activation_names.append(model_name)\n",
    "\n",
    "# Plot training history for different activation functions\n",
    "print(\"\\nTraining history comparison for different activation functions:\")\n",
    "plot_training_history(activation_histories, activation_names)\n",
    "\n",
    "# Create results table\n",
    "activation_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': result['model_name'],\n",
    "        'Activation': ['relu', 'sigmoid', 'tanh', 'elu'][i],\n",
    "        'Accuracy (%)': result['test_accuracy'],\n",
    "        'Training Time (s)': result['training_time'],\n",
    "        'Epochs': result['epochs_trained'],\n",
    "        'Inference Time (ms)': result['inference_time'],\n",
    "        'Parameters': result['total_params'],\n",
    "        'Params/Second': result['params_per_second'],\n",
    "        'Train-Val Gap': result['train_val_gap']\n",
    "    }\n",
    "    for i, result in enumerate(activation_results)\n",
    "])\n",
    "\n",
    "print(\"\\nActivation Function Experiment Results:\")\n",
    "print(activation_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 8: EXPERIMENT - DROPOUT REGULARIZATION\n",
    "\n",
    "In this experiment, we'll examine how dropout regularization affects model performance and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENT 4: VARYING DROPOUT REGULARIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Experiment with different dropout rates\n",
    "dropout_results = []\n",
    "dropout_histories = []\n",
    "dropout_models = []\n",
    "dropout_names = []\n",
    "\n",
    "# Create models with different dropout rates\n",
    "for dropout_rate in [0.0, 0.2, 0.4, 0.6]:\n",
    "    model_name = f\"FCNN_Dropout_{dropout_rate}\"\n",
    "    \n",
    "    model = create_fcnn(\n",
    "        input_dim=784,\n",
    "        hidden_layers=2,\n",
    "        hidden_units=128,\n",
    "        activation='relu',\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    \n",
    "    result = train_and_evaluate_model(\n",
    "        model=model,\n",
    "        train_data=mnist_train,\n",
    "        val_data=mnist_val,\n",
    "        test_data=mnist_test,\n",
    "        model_name=model_name,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    dropout_results.append(result)\n",
    "    dropout_histories.append(result['history'])\n",
    "    dropout_models.append(model)\n",
    "    dropout_names.append(model_name)\n",
    "\n",
    "# Plot training history for different dropout rates\n",
    "print(\"\\nTraining history comparison for different dropout rates:\")\n",
    "plot_training_history(dropout_histories, dropout_names)\n",
    "\n",
    "# Create results table\n",
    "dropout_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': result['model_name'],\n",
    "        'Dropout Rate': [0.0, 0.2, 0.4, 0.6][i],\n",
    "        'Accuracy (%)': result['test_accuracy'],\n",
    "        'Training Time (s)': result['training_time'],\n",
    "        'Epochs': result['epochs_trained'],\n",
    "        'Train-Val Gap': result['train_val_gap'],\n",
    "        'Parameters': result['total_params']\n",
    "    }\n",
    "    for i, result in enumerate(dropout_results)\n",
    "])\n",
    "\n",
    "print(\"\\nDropout Experiment Results:\")\n",
    "print(dropout_df.to_string(index=False))"
   ]
  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 9: MEMORY PROFILING\n",
    "\n",
    "In this section, we'll analyze the memory requirements of different neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENT 5: MEMORY PROFILING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Profile memory usage for models with different sizes\n",
    "memory_results = []\n",
    "\n",
    "# Select representative models from width experiment\n",
    "for i, width in enumerate([64, 128, 256, 512]):\n",
    "    model = width_models[i]\n",
    "    model_name = width_names[i]\n",
    "    \n",
    "    print(f\"\\nProfiling memory usage for {model_name}...\")\n",
    "    memory_profile = profile_memory_usage(model, mnist_test[0][:1000])\n",
    "    \n",
    "    memory_results.append({\n",
    "        'Model': model_name,\n",
    "        'Width': width,\n",
    "        'Parameters': width_results[i]['total_params'],\n",
    "        'Baseline Memory (MB)': memory_profile['baseline_memory_mb'],\n",
    "        'Peak Memory (MB)': memory_profile['peak_memory_mb'],\n",
    "        'Memory Increase (MB)': memory_profile['memory_increase_mb']\n",
    "    })\n",
    "\n",
    "# Create memory results table\n",
    "memory_df = pd.DataFrame(memory_results)\n",
    "print(\"\\nMemory Usage Results:\")\n",
    "print(memory_df.to_string(index=False))\n",
    "\n",
    "# Plot memory vs. model size\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(memory_df['Parameters'], memory_df['Memory Increase (MB)'], s=100)\n",
    "for i, row in memory_df.iterrows():\n",
    "    plt.annotate(f\"Width={row['Width']}\", \n",
    "                 (row['Parameters'], row['Memory Increase (MB)']),\n",
    "                 xytext=(10, 5), textcoords='offset points')\n",
    "\n",
    "plt.title('Memory Usage vs. Model Size')\n",
    "plt.xlabel('Number of Parameters')\n",
    "plt.ylabel('Memory Increase (MB)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 10: DATASET COMPARISON\n",
    "\n",
    "In this section, we'll compare model performance on MNIST vs. Fashion MNIST datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENT 6: DATASET COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Compare performance on different datasets\n",
    "dataset_results = []\n",
    "dataset_histories = []\n",
    "dataset_names = []\n",
    "\n",
    "# Create a standard model architecture\n",
    "model_architecture = {\n",
    "    'input_dim': 784,\n",
    "    'hidden_layers': 2,\n",
    "    'hidden_units': 128,\n",
    "    'activation': 'relu',\n",
    "    'dropout_rate': 0.2\n",
    "}\n",
    "\n",
    "# Train on MNIST\n",
    "print(\"\\nTraining on MNIST dataset...\")\n",
    "mnist_model = create_fcnn(**model_architecture)\n",
    "mnist_result = train_and_evaluate_model(\n",
    "    model=mnist_model,\n",
    "    train_data=mnist_train,\n",
    "    val_data=mnist_val,\n",
    "    test_data=mnist_test,\n",
    "    model_name=\"FCNN_MNIST\",\n",
    "    verbose=1\n",
    ")\n",
    "dataset_results.append(mnist_result)\n",
    "dataset_histories.append(mnist_result['history'])\n",
    "dataset_names.append(\"MNIST\")\n",
    "\n",
    "# Train on Fashion MNIST\n",
    "print(\"\\nTraining on Fashion MNIST dataset...\")\n",
    "fashion_model = create_fcnn(**model_architecture)\n",
    "fashion_result = train_and_evaluate_model(\n",
    "    model=fashion_model,\n",
    "    train_data=fashion_train,\n",
    "    val_data=fashion_val,\n",
    "    test_data=fashion_test,\n",
    "    model_name=\"FCNN_Fashion\",\n",
    "    verbose=1\n",
    ")\n",
    "dataset_results.append(fashion_result)\n",
    "dataset_histories.append(fashion_result['history'])\n",
    "dataset_names.append(\"Fashion MNIST\")\n",
    "\n",
    "# Plot training history comparison\n",
    "print(\"\\nTraining history comparison between datasets:\")\n",
    "plot_training_history(dataset_histories, dataset_names)\n",
    "\n",
    "# Create results table\n",
    "dataset_df = pd.DataFrame([\n",
    "    {\n",
    "        'Dataset': name,\n",
    "        'Accuracy (%)': result['test_accuracy'],\n",
    "        'Training Time (s)': result['training_time'],\n",
    "        'Epochs': result['epochs_trained'],\n",
    "        'Inference Time (ms)': result['inference_time'],\n",
    "        'Train-Val Gap': result['train_val_gap']\n",
    "    }\n",
    "    for name, result in zip(dataset_names, dataset_results)\n",
    "])\n",
    "\n",
    "print(\"\\nDataset Comparison Results:\")\n",
    "print(dataset_df.to_string(index=False))\n",
    "\n",
    "# Plot confusion matrices\n",
    "print(\"\\nMNIST Confusion Matrix:\")\n",
    "mnist_cm, mnist_confused = plot_confusion_matrix(\n",
    "    mnist_model, mnist_test[0], mnist_y_test, \n",
    "    mnist_class_names, \"MNIST Confusion Matrix\"\n",
    ")\n",
    "\n",
    "print(\"\\nFashion MNIST Confusion Matrix:\")\n",
    "fashion_cm, fashion_confused = plot_confusion_matrix(\n",
    "    fashion_model, fashion_test[0], fashion_y_test, \n",
    "    fashion_class_names, \"Fashion MNIST Confusion Matrix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 11: COMPREHENSIVE ANALYSIS\n",
    "\n",
    "In this section, we'll analyze all experiment results together to draw overall conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPREHENSIVE ANALYSIS OF ALL EXPERIMENTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Consolidate results from all experiments\n",
    "all_results = depth_results + width_results + activation_results + dropout_results\n",
    "\n",
    "# Create comprehensive results table\n",
    "all_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': result['model_name'],\n",
    "        'Accuracy (%)': result['test_accuracy'],\n",
    "        'Training Time (s)': result['training_time'],\n",
    "        'Inference Time (ms)': result['inference_time'],\n",
    "        'Parameters': result['total_params'],\n",
    "        'Params/Second': result['params_per_second'],\n",
    "        'Accuracy/Million Params': result['accuracy_per_million_params'],\n",
    "        'Train-Val Gap': result['train_val_gap'],\n",
    "        'Epochs': result['epochs_trained']\n",
    "    }\n",
    "    for result in all_results\n",
    "])\n",
    "\n",
    "print(\"\\nAll Experiment Results:\")\n",
    "print(all_df.to_string(index=False))\n",
    "\n",
    "# Plot accuracy vs. parameters\n",
    "print(\"\\nModel Accuracy vs. Parameter Count:\")\n",
    "plot_metric_comparison(all_df, 'Parameters', 'Accuracy (%)', \n",
    "                      'Model Accuracy vs. Parameter Count')\n",
    "\n",
    "# Plot training time vs. parameter count\n",
    "print(\"\\nTraining Time vs. Parameter Count:\")\n",
    "plot_metric_comparison(all_df, 'Parameters', 'Training Time (s)', \n",
    "                      'Training Time vs. Parameter Count')\n",
    "\n",
    "# Plot inference time vs. parameter count\n",
    "print(\"\\nInference Time vs. Parameter Count:\")\n",
    "plot_metric_comparison(all_df, 'Parameters', 'Inference Time (ms)', \n",
    "                      'Inference Time vs. Parameter Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot efficiency metrics\n",
    "print(\"\\nEfficiency Metrics Comparison:\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Accuracy per million parameters\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(all_df['Model'], all_df['Accuracy/Million Params'])\n",
    "plt.title('Accuracy per Million Parameters')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Parameters trained per second\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.bar(all_df['Model'], all_df['Params/Second'])\n",
    "plt.title('Parameters Trained per Second')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Accuracy vs. Training Time\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(all_df['Training Time (s)'], all_df['Accuracy (%)'], s=100, alpha=0.7)\n",
    "for i, row in all_df.iterrows():\n",
    "    model_name = row['Model'].replace('FCNN_', '')\n",
    "    plt.annotate(model_name, \n",
    "                 (row['Training Time (s)'], row['Accuracy (%)']),\n",
    "                 xytext=(5, 0), textcoords='offset points')\n",
    "plt.title('Accuracy vs. Training Time')\n",
    "plt.xlabel('Training Time (seconds)')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Accuracy vs. Inference Time\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(all_df['Inference Time (ms)'], all_df['Accuracy (%)'], s=100, alpha=0.7)\n",
    "for i, row in all_df.iterrows():\n",
    "    model_name = row['Model'].replace('FCNN_', '')\n",
    "    plt.annotate(model_name, \n",
    "                 (row['Inference Time (ms)'], row['Accuracy (%)']),\n",
    "                 xytext=(5, 0), textcoords='offset points')\n",
    "plt.title('Accuracy vs. Inference Time')\n",
    "plt.xlabel('Inference Time (ms)')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 12: IDENTIFY OPTIMAL ARCHITECTURES\n",
    "\n",
    "In this section, we'll identify the best performing models according to different criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"IDENTIFYING OPTIMAL ARCHITECTURES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Find models with best metrics\n",
    "best_accuracy_model = all_df.loc[all_df['Accuracy (%)'].idxmax()]\n",
    "print(\"\\nModel with Best Accuracy:\")\n",
    "print(best_accuracy_model.to_string())\n",
    "\n",
    "# Find model with best accuracy per parameter\n",
    "best_efficiency_model = all_df.loc[all_df['Accuracy/Million Params'].idxmax()]\n",
    "print(\"\\nModel with Best Accuracy/Parameter Ratio:\")\n",
    "print(best_efficiency_model.to_string())\n",
    "\n",
    "# Find model with fastest inference\n",
    "fastest_inference_model = all_df.loc[all_df['Inference Time (ms)'].idxmin()]\n",
    "print(\"\\nModel with Fastest Inference:\")\n",
    "print(fastest_inference_model.to_string())\n",
    "\n",
    "# Find model with fastest training\n",
    "fastest_training_model = all_df.loc[all_df['Training Time (s)'].idxmin()]\n",
    "print(\"\\nModel with Fastest Training:\")\n",
    "print(fastest_training_model.to_string())\n",
    "\n",
    "# Calculate Pareto frontier (non-dominated models in terms of accuracy and inference time)\n",
    "print(\"\\nCalculating Pareto Frontier for Accuracy vs. Inference Time...\")\n",
    "\n",
    "# Convert to efficiency problem (higher accuracy is better, lower inference time is better)\n",
    "costs = np.column_stack([\n",
    "    -all_df['Accuracy (%)'].values,  # Negative because we want to maximize accuracy\n",
    "    all_df['Inference Time (ms)'].values  # We want to minimize inference time\n",
    "])\n",
    "\n",
    "# Find Pareto-efficient models\n",
    "pareto_efficient = is_pareto_efficient(costs)\n",
    "pareto_df = all_df[pareto_efficient].copy()\n",
    "\n",
    "print(\"\\nPareto-Efficient Models (Accuracy vs. Inference Time):\")\n",
    "print(pareto_df.to_string(index=False))\n",
    "\n",
    "# Plot Pareto frontier\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(all_df['Inference Time (ms)'], all_df['Accuracy (%)'], s=100, alpha=0.5, label='All Models')\n",
    "plt.scatter(pareto_df['Inference Time (ms)'], pareto_df['Accuracy (%)'], s=150, color='red', label='Pareto Frontier')\n",
    "\n",
    "# Annotate Pareto frontier points\n",
    "for i, row in pareto_df.iterrows():\n",
    "    model_name = row['Model'].replace('FCNN_', '')\n",
    "    plt.annotate(model_name, \n",
    "                 (row['Inference Time (ms)'], row['Accuracy (%)']),\n",
    "                 xytext=(10, 5), textcoords='offset points',\n",
    "                 color='red', fontweight='bold')\n",
    "\n",
    "plt.title('Pareto Frontier: Accuracy vs. Inference Time')\n",
    "plt.xlabel('Inference Time (ms)')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 13: RESULTS SUMMARY FOR WORKSHEET\n",
    "\n",
    "Here we summarize the results of all experiments for easier reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTS SUMMARY FOR WORKSHEET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nPart 1: Network Depth Experiment\")\n",
    "for i, result in enumerate(depth_results):\n",
    "    depth = i + 1\n",
    "    print(f\"\\nDepth {depth}:\")\n",
    "    print(f\"  Test Accuracy: {result['test_accuracy']:.2f}%\")\n",
    "    print(f\"  Training Time: {result['training_time']:.2f} seconds\")\n",
    "    print(f\"  Inference Time: {result['inference_time']:.4f} ms\")\n",
    "    print(f\"  Total Parameters: {result['total_params']:,}\")\n",
    "\n",
    "print(\"\\nPart 2: Network Width Experiment\")\n",
    "for i, width in enumerate([64, 128, 256, 512]):\n",
    "    result = width_results[i]\n",
    "    print(f\"\\nWidth {width}:\")\n",
    "    print(f\"  Test Accuracy: {result['test_accuracy']:.2f}%\")\n",
    "    print(f\"  Training Time: {result['training_time']:.2f} seconds\")\n",
    "    print(f\"  Inference Time: {result['inference_time']:.4f} ms\")\n",
    "    print(f\"  Total Parameters: {result['total_params']:,}\")\n",
    "\n",
    "print(\"\\nPart 3: Activation Functions\")\n",
    "for i, activation in enumerate(['relu', 'sigmoid', 'tanh', 'elu']):\n",
    "    result = activation_results[i]\n",
    "    print(f\"\\n{activation.upper()}:\")\n",
    "    print(f\"  Test Accuracy: {result['test_accuracy']:.2f}%\")\n",
    "    print(f\"  Training Time: {result['training_time']:.2f} seconds\")\n",
    "    print(f\"  Epochs to Converge: {result['epochs_trained']}\")\n",
    "    print(f\"  Inference Time: {result['inference_time']:.4f} ms\")\n",
    "\n",
    "print(\"\\nPart 4: Dropout Regularization\")\n",
    "for i, dropout_rate in enumerate([0.0, 0.2, 0.4, 0.6]):\n",
    "    result = dropout_results[i]\n",
    "    print(f\"\\nDropout Rate {dropout_rate}:\")\n",
    "    print(f\"  Test Accuracy: {result['test_accuracy']:.2f}%\")\n",
    "    print(f\"  Training Time: {result['training_time']:.2f} seconds\")\n",
    "    print(f\"  Epochs to Converge: {result['epochs_trained']}\")\n",
    "    print(f\"  Train-Val Gap: {result['train_val_gap']:.4f}\")\n",
    "\n",
    "print(\"\\nPart 5: Memory Profiling\")\n",
    "for _, row in memory_df.iterrows():\n",
    "    print(f\"\\n{row['Model']}:\")\n",
    "    print(f\"  Parameters: {row['Parameters']:,}\")\n",
    "    print(f\"  Baseline Memory: {row['Baseline Memory (MB)']:.2f} MB\")\n",
    "    print(f\"  Peak Memory: {row['Peak Memory (MB)']:.2f} MB\")\n",
    "    print(f\"  Memory Increase: {row['Memory Increase (MB)']:.2f} MB\")\n",
    "\n",
    "print(\"\\nPart 6: Dataset Comparison\")\n",
    "for i, dataset in enumerate(['MNIST', 'Fashion MNIST']):\n",
    "    result = dataset_results[i]\n",
    "    print(f\"\\n{dataset}:\")\n",
    "    print(f\"  Test Accuracy: {result['test_accuracy']:.2f}%\")\n",
    "    print(f\"  Training Time: {result['training_time']:.2f} seconds\")\n",
    "    print(f\"  Epochs to Converge: {result['epochs_trained']}\")\n",
    "\n",
    "print(\"\\nPart 7: Efficiency Metrics\")\n",
    "print(f\"\\nBest Accuracy Model ({best_accuracy_model['Model']})\")\n",
    "print(f\"  Accuracy/Million Params: {best_accuracy_model['Accuracy/Million Params']:.2f}\")\n",
    "print(f\"  Params/Second: {best_accuracy_model['Params/Second']:.2f}\")\n",
    "\n",
    "print(f\"\\nFastest Training Model ({fastest_training_model['Model']})\")\n",
    "print(f\"  Accuracy/Million Params: {fastest_training_model['Accuracy/Million Params']:.2f}\")\n",
    "print(f\"  Params/Second: {fastest_training_model['Params/Second']:.2f}\")\n",
    "\n",
    "print(f\"\\nFastest Inference Model ({fastest_inference_model['Model']})\")\n",
    "print(f\"  Accuracy/Million Params: {fastest_inference_model['Accuracy/Million Params']:.2f}\")\n",
    "print(f\"  Params/Second: {fastest_inference_model['Params/Second']:.2f}\")\n",
    "\n",
    "print(f\"\\nMost Parameter-Efficient Model ({best_efficiency_model['Model']})\")\n",
    "print(f\"  Accuracy/Million Params: {best_efficiency_model['Accuracy/Million Params']:.2f}\")\n",
    "print(f\"  Params/Second: {best_efficiency_model['Params/Second']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 14: SAVE RESULTS (UNCOMMENT TO USE)\n",
    "\n",
    "Here we provide code to save the experiment results and best model if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save results to Google Drive\n",
    "# results_path = \"/content/drive/My Drive/ML_Hardware_Course/Lab2/fcnn_results.csv\"\n",
    "# all_df.to_csv(results_path, index=False)\n",
    "# print(f\"Results saved to {results_path}\")\n",
    "\n",
    "# Save the best model\n",
    "# best_model_path = \"/content/drive/My Drive/ML_Hardware_Course/Lab2/best_fcnn_model.h5\"\n",
    "# model_idx = all_df[all_df['Model'] == best_accuracy_model['Model']].index[0]\n",
    "# model_group = model_idx // len(depth_results)\n",
    "# model_within_group = model_idx % len(depth_results)\n",
    "\n",
    "# if model_group == 0:\n",
    "#     depth_models[model_within_group].save(best_model_path)\n",
    "# elif model_group == 1:\n",
    "#     width_models[model_within_group].save(best_model_path)\n",
    "# elif model_group == 2:\n",
    "#     activation_models[model_within_group].save(best_model_path)\n",
    "# else:\n",
    "#     dropout_models[model_within_group].save(best_model_path)\n",
    "\n",
    "# print(f\"Best model saved to {best_model_path}\")\n",
    "\n",
    "print(\"\\nLab 2 completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this lab, we experimented with Fully Connected Neural Networks (FCNNs) to understand how different architectural choices impact model performance, training efficiency, and hardware utilization. We systematically varied network depth, width, activation functions, and regularization techniques to analyze their effects.\n",
    "\n",
    "## Key Findings:\n",
    "\n",
    "1. **Network Depth**: Deeper networks can capture more complex patterns but may require more training time and computational resources. Diminishing returns appear after a certain depth.\n",
    "\n",
    "2. **Network Width**: Wider networks with more neurons per layer generally provide higher accuracy but at the cost of more parameters and memory usage.\n",
    "\n",
    "3. **Activation Functions**: ReLU generally performs well with fast training convergence, while other activation functions like sigmoid and tanh may have different convergence characteristics.\n",
    "\n",
    "4. **Dropout Regularization**: Appropriate dropout rates help reduce overfitting, as indicated by smaller train-validation accuracy gaps.\n",
    "\n",
    "5. **Memory Requirements**: Model size correlates with memory usage, which is an important consideration for deployment on resource-constrained devices.\n",
    "\n",
    "6. **Dataset Complexity**: Fashion MNIST is a more challenging dataset than MNIST, resulting in lower accuracy and potentially requiring more complex models.\n",
    "\n",
    "These insights can guide the design of neural networks that balance performance needs with hardware constraints for different applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
