{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L6gaMhEfKTJ"
      },
      "source": [
        "# LAB 1: WORKING WITH PRE-TRAINED MODELS\n",
        "## Machine Learning Hardware Course\n",
        "\n",
        "---\n",
        "\n",
        "## OVERVIEW\n",
        "\n",
        "This lab introduces you to the practical application of pre-trained convolutional neural networks (CNNs) using the MNIST dataset. You will experiment with established architectures such as MobileNet, ResNet, and VGG, analyzing their performance, efficiency, and hardware requirements. Through hands-on implementation, you will gain experience with transfer learning, model adaptation, and quantitative evaluation of model characteristics.\n",
        "\n",
        "---\n",
        "\n",
        "## LEARNING OBJECTIVES\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "1. Configure a Google Colab environment for deep learning development\n",
        "2. Adapt pre-trained CNN architectures for the MNIST dataset\n",
        "3. Compare multiple model architectures based on performance metrics\n",
        "4. Evaluate the impact of model complexity on hardware requirements\n",
        "5. Implement transfer learning techniques for efficient model adaptation\n",
        "6. Quantitatively analyze model performance versus computational cost\n",
        "\n",
        "---\n",
        "\n",
        "## TIME ALLOCATION\n",
        "\n",
        "Total time: 2 hours (120 minutes)\n",
        "\n",
        "| Activity | Duration |\n",
        "|----------|----------|\n",
        "| Environment Setup | 15 minutes |\n",
        "| Dataset Preparation | 15 minutes |\n",
        "| Model Adaptation | 30 minutes |\n",
        "| Model Evaluation | 30 minutes |\n",
        "| Performance Analysis | 20 minutes |\n",
        "| Worksheet Completion | 20 minutes |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrBbx8wJfKTT"
      },
      "source": [
        "# PART 1: ENVIRONMENT SETUP\n",
        "\n",
        "In this section, we'll configure the Google Colab environment, import necessary libraries, and check for GPU availability.\n",
        "\n",
        "## Mounting Google Drive\n",
        "\n",
        "First, we'll mount Google Drive to save our work and create a directory for this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94zb93WgfKTU",
        "outputId": "49c1bc6f-24ea-4e4d-c525-0f3c96dbe18d"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a directory for this lab\n",
        "!mkdir -p \"/content/drive/My Drive/ML_Hardware_Course/Lab1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kqzq197fKTW"
      },
      "source": [
        "## Installing Required Libraries\n",
        "\n",
        "Next, we'll import the necessary libraries for this lab, including TensorFlow, Keras, NumPy, Matplotlib, and other data analysis tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXPQP_lAfKTX",
        "outputId": "744599a7-4b08-47e2-a79a-3ecafdbafa32"
      },
      "outputs": [],
      "source": [
        "# Import basic libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D  # For model modification\n",
        "from tensorflow.keras.applications import MobileNetV2, ResNet50, VGG16\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
        "print(\"GPU Details:\")\n",
        "try:\n",
        "    !nvidia-smi\n",
        "except:\n",
        "    print(\"nvidia-smi command not available (likely not running on GPU)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xtE8MKxfKTX"
      },
      "source": [
        "# PART 2: MNIST DATASET PREPARATION\n",
        "\n",
        "In this section, we'll load the MNIST dataset and prepare it for use with pre-trained models. The MNIST dataset contains grayscale images of handwritten digits (0-9), but pre-trained models expect RGB inputs with specific sizes. We need to adapt the data accordingly.\n",
        "\n",
        "## Loading the MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92Hw8punfKTY",
        "outputId": "c5ac5b08-b5f0-4b80-b6de-a1557470c511"
      },
      "outputs": [],
      "source": [
        "# Choose which dataset to use (uncomment one)\n",
        "# Option 1: MNIST (Handwritten Digits)\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "class_names = [str(i) for i in range(10)]  # 0-9 digits\n",
        "\n",
        "# Option 2: Fashion-MNIST (Clothing Items)\n",
        "# (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "# class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "#                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Print dataset shapes\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Training labels shape:\", y_train.shape)\n",
        "print(\"Test data shape:\", X_test.shape)\n",
        "print(\"Test labels shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3YO2SR3fKTZ"
      },
      "source": [
        "## Visualizing Sample Images\n",
        "\n",
        "Let's visualize some sample images from the MNIST dataset to better understand the data we're working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "SM-eYOh5fKTa",
        "outputId": "7b61f4ee-9298-457d-990a-7571e7de9205"
      },
      "outputs": [],
      "source": [
        "# Create a function to display multiple images\n",
        "def display_sample_images(X, y, num_samples=10):\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(1, num_samples, i+1)\n",
        "        plt.imshow(X[i], cmap='gray')\n",
        "        plt.title(f\"{class_names[y[i]]}\")\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display 10 sample images\n",
        "display_sample_images(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufHCsvNTfKTa"
      },
      "source": [
        "## Preprocessing the MNIST Dataset\n",
        "\n",
        "Now, we'll define a simple preprocessing function for the MNIST dataset. We'll normalize the pixel values to [0,1] and reshape the images to add a channel dimension, which is required for CNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBiYD6P1fKTb",
        "outputId": "df1ef644-1c2a-4067-dbe4-ba64eeb98859"
      },
      "outputs": [],
      "source": [
        "# Simple preprocessing function for MNIST\n",
        "def preprocess_mnist_simple(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Simple preprocessing for MNIST, normalizing and reshaping to add channel dimension\n",
        "    \"\"\"\n",
        "    # Normalize to [0,1]\n",
        "    X_train = X_train.astype('float32') / 255.0\n",
        "    X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "    # Add channel dimension (28x28 -> 28x28x1)\n",
        "    X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "\n",
        "    return X_train, X_test\n",
        "\n",
        "# Prepare the labels\n",
        "y_train_encoded = to_categorical(y_train, 10)\n",
        "y_test_encoded = to_categorical(y_test, 10)\n",
        "\n",
        "# Create a validation set (20% of training data)\n",
        "val_size = 12000  # 20% of 60,000\n",
        "X_val = X_train[-val_size:]\n",
        "y_val = y_train_encoded[-val_size:]\n",
        "X_train_final = X_train[:-val_size]\n",
        "y_train_final = y_train_encoded[:-val_size]\n",
        "\n",
        "print(\"Training set size:\", X_train_final.shape[0])\n",
        "print(\"Validation set size:\", X_val.shape[0])\n",
        "print(\"Test set size:\", X_test.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDCBzlC7fKTb"
      },
      "source": [
        "# PART 3: MOBILENETV2 MODEL PREPARATION\n",
        "\n",
        "In this section, we'll prepare the MobileNetV2 model for the MNIST dataset. MobileNetV2 is a lightweight architecture designed for mobile and edge devices, making it an excellent choice for efficiency-critical applications.\n",
        "\n",
        "We need to adapt the model for our MNIST dataset, which requires addressing two key challenges:\n",
        "1. MNIST images are grayscale (1 channel), while pre-trained models expect RGB images (3 channels)\n",
        "2. MNIST images are 28x28 pixels, while pre-trained models often expect larger input sizes\n",
        "\n",
        "Our approach will use a padding layer to increase the image size and a convolutional layer to convert from grayscale to RGB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "iUE643NjfKTc",
        "outputId": "06b1aa5c-2002-4ae6-e205-ff0c1804c35f"
      },
      "outputs": [],
      "source": [
        "def create_mobilenet_model():\n",
        "    \"\"\"\n",
        "    Create MobileNetV2 model with properly padded input for MNIST\n",
        "    \"\"\"\n",
        "    # Preprocess data\n",
        "    X_train_mobilenet, X_test_mobilenet = preprocess_mnist_simple(X_train_final, X_test)\n",
        "    X_val_mobilenet, _ = preprocess_mnist_simple(X_val, np.zeros((1, 28, 28)))\n",
        "\n",
        "    # Create model architecture\n",
        "    inputs = Input(shape=(28, 28, 1))\n",
        "\n",
        "    # Pad the input from 28x28 to 32x32 using zero padding\n",
        "    x = tf.keras.layers.ZeroPadding2D(padding=2)(inputs)  # Add 2 pixels on each side: 28x28 -> 32x32\n",
        "    # Convert single-channel grayscale to 3-channel RGB format required by MobileNetV2\n",
        "    x = Conv2D(16, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = Conv2D(3, kernel_size=1, padding='same', activation='relu')(x)  # Output 3 channels\n",
        "\n",
        "    # Create sub-model with MobileNetV2\n",
        "    base_model = MobileNetV2(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=(32, 32, 3),\n",
        "        pooling='avg'\n",
        "    )\n",
        "\n",
        "    # Sets the MobileNetV2 layers as trainable, allowing fine-tuning on MNIST data\n",
        "    base_model.trainable = True\n",
        "\n",
        "    # # Freeze the base model layers\n",
        "    # base_model.trainable = False\n",
        "\n",
        "    # Continue with the model architecture\n",
        "    x = base_model(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "    # Combines the input and output layers into a single Keras model.\n",
        "    mobilenet_model = Model(inputs, outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    mobilenet_model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Display model summary\n",
        "    mobilenet_model.summary()\n",
        "\n",
        "    return mobilenet_model, X_train_mobilenet, X_val_mobilenet, X_test_mobilenet\n",
        "\n",
        "# Create MobileNetV2 model\n",
        "print(\"\\n--- Creating MobileNetV2 Model ---\")\n",
        "mobilenet_model, X_train_mobilenet, X_val_mobilenet, X_test_mobilenet = create_mobilenet_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HntF8NEUhV9K"
      },
      "source": [
        "**Train MobileNetV2  model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f8Peez6hkH7",
        "outputId": "77aaaf21-924f-4d64-873b-e61ef9fc7a22"
      },
      "outputs": [],
      "source": [
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train MobileNetV2 model\n",
        "print(\"\\n--- Training MobileNetV2 Model ---\")\n",
        "start_time = time.time()\n",
        "mobilenet_history = mobilenet_model.fit(\n",
        "    X_train_mobilenet,\n",
        "    y_train_final,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val_mobilenet, y_val),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "mobilenet_training_time = time.time() - start_time\n",
        "print(f\"MobileNetV2 - Training completed in {mobilenet_training_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAEi2U84fKTc"
      },
      "source": [
        "# PART 4: RESNET50 MODEL PREPARATION\n",
        "\n",
        "Now, we'll prepare the ResNet50 model for the MNIST dataset. ResNet50 is a deeper architecture known for its residual connections that help address the vanishing gradient problem in deep networks.\n",
        "\n",
        "Similar to the MobileNetV2 approach, we'll need to adapt the ResNet50 model to work with our MNIST dataset by addressing the channel and size discrepancies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "RyTvfBgJfKTd",
        "outputId": "40630448-6db2-4ecc-b132-d18270e2b532"
      },
      "outputs": [],
      "source": [
        "def create_resnet_model():\n",
        "    \"\"\"\n",
        "    Create ResNet50 model with properly padded input for MNIST\n",
        "    \"\"\"\n",
        "    # Preprocess data\n",
        "    X_train_resnet, X_test_resnet = preprocess_mnist_simple(X_train_final, X_test)\n",
        "    X_val_resnet, _ = preprocess_mnist_simple(X_val, np.zeros((1, 28, 28)))\n",
        "\n",
        "    # Create model architecture\n",
        "    inputs = Input(shape=(28, 28, 1))\n",
        "\n",
        "    # Pad the input from 28x28 to 32x32\n",
        "    x = tf.keras.layers.ZeroPadding2D(padding=2)(inputs)  # Add 2 pixels on each side\n",
        "\n",
        "    # Convert single-channel to 3-channel input\n",
        "    x = Conv2D(16, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = Conv2D(3, kernel_size=1, padding='same', activation='relu')(x)  # Output 3 channels\n",
        "\n",
        "    # Load ResNet50 with proper input shape\n",
        "    base_model = ResNet50(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=(32, 32, 3),\n",
        "        pooling='avg'\n",
        "    )\n",
        "\n",
        "    # Sets the ResNet50 layers as trainable, allowing fine-tuning on MNIST data\n",
        "    base_model.trainable = True\n",
        "\n",
        "    # Continue with model architecture\n",
        "    x = base_model(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "    resnet_model = Model(inputs, outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    resnet_model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Display model summary\n",
        "    resnet_model.summary()\n",
        "\n",
        "    return resnet_model, X_train_resnet, X_val_resnet, X_test_resnet\n",
        "\n",
        "# Create ResNet50 model\n",
        "print(\"\\n--- Creating ResNet50 Model ---\")\n",
        "resnet_model, X_train_resnet, X_val_resnet, X_test_resnet = create_resnet_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHbrL2LFfKTd",
        "outputId": "3bb8be0f-43b9-4b4d-adf2-d441cde52d59"
      },
      "outputs": [],
      "source": [
        "# Train ResNet50 model\n",
        "print(\"\\n--- Training ResNet50 Model ---\")\n",
        "start_time = time.time()\n",
        "resnet_history = resnet_model.fit(\n",
        "    X_train_resnet,\n",
        "    y_train_final,\n",
        "    epochs=10,\n",
        "    batch_size=32,  # Smaller batch size due to larger model\n",
        "    validation_data=(X_val_resnet, y_val),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "resnet_training_time = time.time() - start_time\n",
        "print(f\"ResNet50 - Training completed in {resnet_training_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0yW-8VQkP_r"
      },
      "source": [
        "# PART 4: VGG16 MODEL PREPARATION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If accuracy is low, please change the optimizer to sgd like below.    \n",
        "\n",
        "```\n",
        "vgg_model.compile(\n",
        "        optimizer='sgd',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "2yiAJqDmkgV6",
        "outputId": "50ed16b2-dab2-431e-8090-380c6c80e84d"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/anaconda3/envs/pytorch/bin/python' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -p /opt/anaconda3/envs/pytorch ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def create_vgg_model():\n",
        "    \"\"\"\n",
        "    Create VGG16 model with properly padded input for MNIST\n",
        "    \"\"\"\n",
        "    # Preprocess data\n",
        "    X_train_vgg, X_test_vgg = preprocess_mnist_simple(X_train_final, X_test)\n",
        "    X_val_vgg, _ = preprocess_mnist_simple(X_val, np.zeros((1, 28, 28)))\n",
        "\n",
        "    # Create model architecture\n",
        "    inputs = Input(shape=(28, 28, 1))\n",
        "\n",
        "    # Pad the input from 28x28 to 32x32\n",
        "    x = tf.keras.layers.ZeroPadding2D(padding=2)(inputs)  # Add 2 pixels on each side\n",
        "\n",
        "    # Convert single-channel to 3-channel\n",
        "    x = Conv2D(16, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = Conv2D(3, kernel_size=1, padding='same', activation='relu')(x)  # Output 3 channels\n",
        "\n",
        "    # Load VGG16 with proper input shape\n",
        "    base_model = VGG16(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=(32, 32, 3),\n",
        "        pooling='avg'\n",
        "    )\n",
        "\n",
        "    # Sets the VGG16 layers as trainable, allowing fine-tuning on MNIST data\n",
        "    base_model.trainable = True\n",
        "\n",
        "    # Continue with model architecture\n",
        "    x = base_model(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "    vgg_model = Model(inputs, outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    vgg_model.compile(\n",
        "        optimizer='adam',   # sgd\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Display model summary\n",
        "    vgg_model.summary()\n",
        "\n",
        "    return vgg_model, X_train_vgg, X_val_vgg, X_test_vgg\n",
        "\n",
        "\n",
        "# Create VGG16 model\n",
        "vgg_model, X_train_vgg, X_val_vgg, X_test_vgg = create_vgg_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFWjvvYtfKTe"
      },
      "source": [
        "## Training VGG16 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYJIVo3HfKTe",
        "outputId": "d5b7bd75-7b84-46dc-9880-69f8627009b0"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/anaconda3/envs/pytorch/bin/python' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -p /opt/anaconda3/envs/pytorch ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Train VGG16 model\n",
        "print(\"\\n--- Training VGG16 Model ---\")\n",
        "start_time = time.time()\n",
        "vgg_history = vgg_model.fit(\n",
        "    X_train_vgg,\n",
        "    y_train_final,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val_vgg, y_val),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "vgg_training_time = time.time() - start_time\n",
        "print(f\"VGG16 - Training completed in {vgg_training_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-fsIQQRfKTe"
      },
      "source": [
        "# PART 7: MODEL EVALUATION\n",
        "\n",
        "Now that we have trained all three models, let's evaluate their performance on the test set and visualize the training history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1jvKX6afKTf",
        "outputId": "c42a75ff-8112-436e-a664-e148c18371b1"
      },
      "outputs": [],
      "source": [
        "# Evaluate models on test set\n",
        "print(\"\\n--- Model Evaluation on Test Set ---\")\n",
        "mobilenet_loss, mobilenet_accuracy = mobilenet_model.evaluate(X_test_mobilenet, y_test_encoded)\n",
        "print(f\"MobileNetV2 - Test accuracy: {mobilenet_accuracy*100:.2f}%\")\n",
        "\n",
        "resnet_loss, resnet_accuracy = resnet_model.evaluate(X_test_resnet, y_test_encoded)\n",
        "print(f\"ResNet50 - Test accuracy: {resnet_accuracy*100:.2f}%\")\n",
        "\n",
        "vgg_loss, vgg_accuracy = vgg_model.evaluate(X_test_vgg, y_test_encoded)\n",
        "print(f\"VGG16 - Test accuracy: {vgg_accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkovgbUqfKTf"
      },
      "source": [
        "## Visualizing Training History\n",
        "\n",
        "Let's visualize the training history of all three models to compare their learning curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "UYPM6g1NfKTf",
        "outputId": "c4658a66-b03f-422a-eb21-50765d5d3322"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "def plot_training_history(histories, titles):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for history, title in zip(histories, titles):\n",
        "        plt.plot(history.history['accuracy'], label=f'{title} - Training')\n",
        "        plt.plot(history.history['val_accuracy'], label=f'{title} - Validation')\n",
        "\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for history, title in zip(histories, titles):\n",
        "        plt.plot(history.history['loss'], label=f'{title} - Training')\n",
        "        plt.plot(history.history['val_loss'], label=f'{title} - Validation')\n",
        "\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot training history for all models\n",
        "plot_training_history(\n",
        "    [mobilenet_history, resnet_history, vgg_history],\n",
        "    ['MobileNetV2', 'ResNet50', 'VGG16']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkwHbdo6fKTg"
      },
      "source": [
        "# PART 8: CONFUSION MATRICES AND CLASSIFICATION REPORTS\n",
        "\n",
        "In this section, we'll analyze the performance of each model in more detail by generating confusion matrices and classification reports. This will help us understand which digits are most frequently misclassified by each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6jPtu2ABfKTg",
        "outputId": "648113b9-388f-4390-d711-137c766ce6ee"
      },
      "outputs": [],
      "source": [
        "# Function to generate predictions and confusion matrix\n",
        "def analyze_model_performance(model, X_test, y_test, model_name):\n",
        "    # Generate predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "    plt.title(f'{model_name} - Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "    # Find the most confused pairs\n",
        "    cm_normalized = cm.copy()\n",
        "    np.fill_diagonal(cm_normalized, 0)  # Ignore correct predictions\n",
        "    max_confusion = np.unravel_index(np.argmax(cm_normalized), cm_normalized.shape)\n",
        "    print(f\"Most confused pair: True digit {max_confusion[0]} predicted as {max_confusion[1]} ({cm_normalized[max_confusion]} times)\")\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(y_true_classes, y_pred_classes, output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    print(f\"{model_name} Classification Report:\")\n",
        "    print(report_df.round(3))\n",
        "\n",
        "    return y_pred_classes, report, max_confusion\n",
        "\n",
        "# Analyze each model's performance\n",
        "print(\"\\n--- MobileNetV2 Performance Analysis ---\")\n",
        "mobilenet_pred, mobilenet_report, mobilenet_confused_pair = analyze_model_performance(\n",
        "    mobilenet_model, X_test_mobilenet, y_test_encoded, 'MobileNetV2'\n",
        ")\n",
        "\n",
        "print(\"\\n--- ResNet50 Performance Analysis ---\")\n",
        "resnet_pred, resnet_report, resnet_confused_pair = analyze_model_performance(\n",
        "    resnet_model, X_test_resnet, y_test_encoded, 'ResNet50'\n",
        ")\n",
        "\n",
        "print(\"\\n--- VGG16 Performance Analysis ---\")\n",
        "vgg_pred, vgg_report, vgg_confused_pair = analyze_model_performance(\n",
        "    vgg_model, X_test_vgg, y_test_encoded, 'VGG16'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4ZIbexIfKTg"
      },
      "source": [
        "# PART 9: MODEL METRICS COMPARISON\n",
        "\n",
        "Now, let's compare the models based on various metrics such as parameter count, training time, and inference time. This will help us understand the trade-offs between model complexity, performance, and efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DB0VLBSfKTh",
        "outputId": "e177d752-3bc5-4530-bf39-e41cefeb83dc"
      },
      "outputs": [],
      "source": [
        "def count_model_parameters(model):\n",
        "    trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
        "    non_trainable_params = np.sum([np.prod(v.shape) for v in model.non_trainable_weights])\n",
        "    total_params = trainable_params + non_trainable_params\n",
        "    return trainable_params, non_trainable_params, total_params\n",
        "\n",
        "# Get parameter counts for each model\n",
        "mobilenet_trainable, mobilenet_non_trainable, mobilenet_total = count_model_parameters(mobilenet_model)\n",
        "resnet_trainable, resnet_non_trainable, resnet_total = count_model_parameters(resnet_model)\n",
        "vgg_trainable, vgg_non_trainable, vgg_total = count_model_parameters(vgg_model)\n",
        "\n",
        "# Function to measure inference time\n",
        "def measure_inference_time(model, X_test, batch_size=1, num_runs=50):\n",
        "    # Warm-up\n",
        "    for _ in range(10):\n",
        "        _ = model.predict(X_test[:batch_size])\n",
        "\n",
        "    # Measure time for inference\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        _ = model.predict(X_test[:batch_size])\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    # Calculate average inference time per batch\n",
        "    avg_time = total_time / num_runs\n",
        "    return avg_time * 1000  # Convert to milliseconds\n",
        "\n",
        "# Measure inference time for each model (single image)\n",
        "mobilenet_inference_time = measure_inference_time(mobilenet_model, X_test_mobilenet)\n",
        "resnet_inference_time = measure_inference_time(resnet_model, X_test_resnet)\n",
        "vgg_inference_time = measure_inference_time(vgg_model, X_test_vgg)\n",
        "\n",
        "print(\"\\n--- Single Image Inference Time ---\")\n",
        "print(f\"MobileNetV2 - Inference time (1 image): {mobilenet_inference_time:.2f} ms\")\n",
        "print(f\"ResNet50 - Inference time (1 image): {resnet_inference_time:.2f} ms\")\n",
        "print(f\"VGG16 - Inference time (1 image): {vgg_inference_time:.2f} ms\")\n",
        "\n",
        "# Measure inference time for batch of 32 images\n",
        "mobilenet_batch_time = measure_inference_time(mobilenet_model, X_test_mobilenet, batch_size=32, num_runs=20)\n",
        "resnet_batch_time = measure_inference_time(resnet_model, X_test_resnet, batch_size=32, num_runs=20)\n",
        "vgg_batch_time = measure_inference_time(vgg_model, X_test_vgg, batch_size=32, num_runs=20)\n",
        "\n",
        "print(\"\\n--- Batch Inference Time (32 images) ---\")\n",
        "print(f\"MobileNetV2 - Inference time (32 images): {mobilenet_batch_time:.2f} ms\")\n",
        "print(f\"ResNet50 - Inference time (32 images): {resnet_batch_time:.2f} ms\")\n",
        "print(f\"VGG16 - Inference time (32 images): {vgg_batch_time:.2f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f9b35UzfKTh"
      },
      "source": [
        "## Creating Comparison Tables\n",
        "\n",
        "Let's create a comprehensive table that compares all the models based on various metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXEEm6WJfKTh",
        "outputId": "c720fbc7-14d0-4d88-d7ab-c84363a8edcb"
      },
      "outputs": [],
      "source": [
        "# Create comparison table\n",
        "model_metrics = {\n",
        "    'Model': ['MobileNetV2', 'ResNet50', 'VGG16'],\n",
        "    'Test Accuracy (%)': [\n",
        "        mobilenet_accuracy * 100,\n",
        "        resnet_accuracy * 100,\n",
        "        vgg_accuracy * 100\n",
        "    ],\n",
        "    'Trainable Parameters': [\n",
        "        mobilenet_trainable,\n",
        "        resnet_trainable,\n",
        "        vgg_trainable\n",
        "    ],\n",
        "    'Total Parameters': [\n",
        "        mobilenet_total,\n",
        "        resnet_total,\n",
        "        vgg_total\n",
        "    ],\n",
        "    'Training Time (s)': [\n",
        "        mobilenet_training_time,\n",
        "        resnet_training_time,\n",
        "        vgg_training_time\n",
        "    ],\n",
        "    'Inference Time (ms)': [\n",
        "        mobilenet_inference_time,\n",
        "        resnet_inference_time,\n",
        "        vgg_inference_time\n",
        "    ],\n",
        "    'Batch Inference Time (ms)': [\n",
        "        mobilenet_batch_time,\n",
        "        resnet_batch_time,\n",
        "        vgg_batch_time\n",
        "    ],\n",
        "    'Parameters/Second': [\n",
        "        mobilenet_total / mobilenet_training_time,\n",
        "        resnet_total / resnet_training_time,\n",
        "        vgg_total / vgg_training_time\n",
        "    ],\n",
        "    'Accuracy/Million Params': [\n",
        "        (mobilenet_accuracy * 100) / (mobilenet_total / 1e6),\n",
        "        (resnet_accuracy * 100) / (resnet_total / 1e6),\n",
        "        (vgg_accuracy * 100) / (vgg_total / 1e6)\n",
        "    ],\n",
        "    'Most Confused Pair': [\n",
        "        f\"{mobilenet_confused_pair[0]}-{mobilenet_confused_pair[1]}\",\n",
        "        f\"{resnet_confused_pair[0]}-{resnet_confused_pair[1]}\",\n",
        "        f\"{vgg_confused_pair[0]}-{vgg_confused_pair[1]}\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create and display DataFrame\n",
        "metrics_df = pd.DataFrame(model_metrics).set_index('Model')\n",
        "print(\"\\n--- Model Comparison Metrics ---\")\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "print(metrics_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckcPtMtQfKTi"
      },
      "source": [
        "# PART 10: VISUALIZATION OF MODEL COMPARISONS\n",
        "\n",
        "Let's create visual comparisons of the models to better understand their performance characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0KTVfjXhfKTi",
        "outputId": "7a1771b0-2c65-4f53-ba84-4d44b871bcee"
      },
      "outputs": [],
      "source": [
        "# Visualize the metrics\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Accuracy comparison\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.bar(model_metrics['Model'], model_metrics['Test Accuracy (%)'])\n",
        "plt.title('Test Accuracy (%)')\n",
        "plt.ylim(90, 100)  # Adjust as needed\n",
        "plt.grid(axis='y')\n",
        "\n",
        "# Training time comparison\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.bar(model_metrics['Model'], model_metrics['Training Time (s)'])\n",
        "plt.title('Training Time (seconds)')\n",
        "plt.grid(axis='y')\n",
        "\n",
        "# Parameter count comparison (log scale)\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.bar(model_metrics['Model'], [np.log10(p) for p in model_metrics['Total Parameters']])\n",
        "plt.title('Log10(Total Parameters)')\n",
        "plt.grid(axis='y')\n",
        "\n",
        "# Efficiency comparison\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.bar(model_metrics['Model'], model_metrics['Accuracy/Million Params'])\n",
        "plt.title('Accuracy/Million Parameters')\n",
        "plt.grid(axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Inference time comparison\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(model_metrics['Model'], model_metrics['Inference Time (ms)'])\n",
        "plt.title('Single Image Inference Time (ms)')\n",
        "plt.grid(axis='y')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(model_metrics['Model'], model_metrics['Batch Inference Time (ms)'])\n",
        "plt.title('Batch Inference Time (32 images, ms)')\n",
        "plt.grid(axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY7u74sWfKTi"
      },
      "source": [
        "# PART 11: WORKSHEET VALUES SUMMARY\n",
        "\n",
        "In this section, we'll extract the key metrics and findings that you'll need to complete the graded worksheet for this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKwyUp9ZfKTi",
        "outputId": "7cac33fe-ee19-43a9-a32c-4cd3ecab631e"
      },
      "outputs": [],
      "source": [
        "print(\"\\n===== WORKSHEET VALUES =====\")\n",
        "\n",
        "print(\"\\n1.1 Basic Performance Metrics:\")\n",
        "for model_name, trainable, total, accuracy, train_time, infer_time in zip(\n",
        "    model_metrics['Model'],\n",
        "    model_metrics['Trainable Parameters'],\n",
        "    model_metrics['Total Parameters'],\n",
        "    model_metrics['Test Accuracy (%)'],\n",
        "    model_metrics['Training Time (s)'],\n",
        "    model_metrics['Inference Time (ms)']\n",
        "):\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  Trainable Parameters: {trainable}\")\n",
        "    print(f\"  Total Parameters: {total}\")\n",
        "    print(f\"  Test Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"  Training Time: {train_time:.2f} seconds\")\n",
        "    print(f\"  Inference Time: {infer_time:.2f} ms\")\n",
        "\n",
        "print(\"\\n1.2 Efficiency Metrics:\")\n",
        "for model_name, params_per_sec, acc_per_mil, batch_time in zip(\n",
        "    model_metrics['Model'],\n",
        "    model_metrics['Parameters/Second'],\n",
        "    model_metrics['Accuracy/Million Params'],\n",
        "    model_metrics['Batch Inference Time (ms)']\n",
        "):\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  Parameters/Second: {params_per_sec:.2f}\")\n",
        "    print(f\"  Accuracy/Million Params: {acc_per_mil:.2f}\")\n",
        "    print(f\"  Batch Inference Time: {batch_time:.2f} ms\")\n",
        "\n",
        "print(\"\\n2.1 Most Confused Digit Pairs:\")\n",
        "for model_name, confused_pair in zip(\n",
        "    model_metrics['Model'],\n",
        "    model_metrics['Most Confused Pair']\n",
        "):\n",
        "    print(f\"  {model_name}: {confused_pair}\")\n",
        "\n",
        "# Get the best performing model\n",
        "best_model_idx = np.argmax(model_metrics['Test Accuracy (%)'])\n",
        "best_model_name = model_metrics['Model'][best_model_idx]\n",
        "best_model_report = [mobilenet_report, resnet_report, vgg_report][best_model_idx]\n",
        "\n",
        "print(f\"\\n2.2 Per-Class Precision for Best Model ({best_model_name}):\")\n",
        "for digit in range(10):\n",
        "    print(f\"  Digit {digit}: {best_model_report[str(digit)]['precision']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wQkQiJgfKTj"
      },
      "source": [
        "## Model Comparison Table for Worksheet\n",
        "\n",
        "Below is a formatted table of the key metrics that you need to include in your worksheet submission:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8hfFmz-fKTj",
        "outputId": "be17d911-d737-4825-cd67-a8aa1c95dadf"
      },
      "outputs": [],
      "source": [
        "# Create a results summary DataFrame for the worksheet\n",
        "results_df = pd.DataFrame({\n",
        "    \"Model\": model_metrics['Model'],\n",
        "    \"Test Accuracy (%)\": model_metrics['Test Accuracy (%)'],\n",
        "    \"Total Parameters\": model_metrics['Total Parameters'],\n",
        "    \"Training Time (s)\": model_metrics['Training Time (s)'],\n",
        "    \"Inference Time (ms)\": model_metrics['Inference Time (ms)'],\n",
        "    \"Accuracy/Million Params\": model_metrics['Accuracy/Million Params']\n",
        "})\n",
        "\n",
        "print(\"\\n--- Results Summary for Worksheet ---\")\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYe6htOdfKTk"
      },
      "source": [
        "# PART 12: SAVE MODELS AND NOTEBOOK\n",
        "\n",
        "Now that we've completed all the analyses, let's save our models and the notebook for future reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-W7nuFcfKTk",
        "outputId": "22d39725-4c49-4c2c-d594-94f57d2f0ade"
      },
      "outputs": [],
      "source": [
        "# Save models to Google Drive\n",
        "mobilenet_model.save(\"/content/drive/My Drive/ML_Hardware_Course/Lab1/mobilenet_mnist.h5\")\n",
        "resnet_model.save(\"/content/drive/My Drive/ML_Hardware_Course/Lab1/resnet_mnist.h5\")\n",
        "vgg_model.save(\"/content/drive/My Drive/ML_Hardware_Course/Lab1/vgg_mnist.h5\")\n",
        "print(\"Models saved successfully!\")\n",
        "\n",
        "# Create a results summary file\n",
        "results_summary = {\n",
        "    \"basic_metrics\": {\n",
        "        \"mobilenet\": {\n",
        "            \"trainable_params\": int(mobilenet_trainable),\n",
        "            \"total_params\": int(mobilenet_total),\n",
        "            \"test_accuracy\": float(mobilenet_accuracy * 100),\n",
        "            \"training_time\": float(mobilenet_training_time),\n",
        "            \"inference_time\": float(mobilenet_inference_time),\n",
        "        },\n",
        "        \"resnet\": {\n",
        "            \"trainable_params\": int(resnet_trainable),\n",
        "            \"total_params\": int(resnet_total),\n",
        "            \"test_accuracy\": float(resnet_accuracy * 100),\n",
        "            \"training_time\": float(resnet_training_time),\n",
        "            \"inference_time\": float(resnet_inference_time),\n",
        "        },\n",
        "        \"vgg\": {\n",
        "            \"trainable_params\": int(vgg_trainable),\n",
        "            \"total_params\": int(vgg_total),\n",
        "            \"test_accuracy\": float(vgg_accuracy * 100),\n",
        "            \"training_time\": float(vgg_training_time),\n",
        "            \"inference_time\": float(vgg_inference_time),\n",
        "        }\n",
        "    },\n",
        "    \"efficiency_metrics\": {\n",
        "        \"mobilenet\": {\n",
        "            \"params_per_second\": float(mobilenet_total / mobilenet_training_time),\n",
        "            \"accuracy_per_million_params\": float((mobilenet_accuracy * 100) / (mobilenet_total / 1e6)),\n",
        "            \"batch_inference_time\": float(mobilenet_batch_time),\n",
        "        },\n",
        "        \"resnet\": {\n",
        "            \"params_per_second\": float(resnet_total / resnet_training_time),\n",
        "            \"accuracy_per_million_params\": float((resnet_accuracy * 100) / (resnet_total / 1e6)),\n",
        "            \"batch_inference_time\": float(resnet_batch_time),\n",
        "        },\n",
        "        \"vgg\": {\n",
        "            \"params_per_second\": float(vgg_total / vgg_training_time),\n",
        "            \"accuracy_per_million_params\": float((vgg_accuracy * 100) / (vgg_total / 1e6)),\n",
        "            \"batch_inference_time\": float(vgg_batch_time),\n",
        "        }\n",
        "    },\n",
        "    \"confusion_pairs\": {\n",
        "        \"mobilenet\": {\n",
        "            \"pair\": f\"{mobilenet_confused_pair[0]}-{mobilenet_confused_pair[1]}\",\n",
        "            \"count\": int(confusion_matrix(np.argmax(y_test_encoded, axis=1), mobilenet_pred)[mobilenet_confused_pair])\n",
        "        },\n",
        "        \"resnet\": {\n",
        "            \"pair\": f\"{resnet_confused_pair[0]}-{resnet_confused_pair[1]}\",\n",
        "            \"count\": int(confusion_matrix(np.argmax(y_test_encoded, axis=1), resnet_pred)[resnet_confused_pair])\n",
        "        },\n",
        "        \"vgg\": {\n",
        "            \"pair\": f\"{vgg_confused_pair[0]}-{vgg_confused_pair[1]}\",\n",
        "            \"count\": int(confusion_matrix(np.argmax(y_test_encoded, axis=1), vgg_pred)[vgg_confused_pair])\n",
        "        }\n",
        "    },\n",
        "    \"best_model\": {\n",
        "        \"name\": best_model_name,\n",
        "        \"precision_by_digit\": {str(digit): best_model_report[str(digit)]['precision'] for digit in range(10)}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save results as JSON\n",
        "import json\n",
        "with open(\"/content/drive/My Drive/ML_Hardware_Course/Lab1/results_summary.json\", \"w\") as f:\n",
        "    json.dump(results_summary, f, indent=4)\n",
        "print(\"Results summary saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXWrRzy9fKTk"
      },
      "source": [
        "# PART 13: ANALYSIS QUESTIONS DISCUSSION\n",
        "\n",
        "Based on the experimental results, let's address the analysis questions posed in the lab worksheet. These discussions can help you formulate your answers for the worksheet submission.\n",
        "\n",
        "## 1. Which model provides the best balance between accuracy and computational efficiency? Why?\n",
        "\n",
        "To answer this question, consider metrics like:\n",
        "- Test accuracy\n",
        "- Inference time (especially important for deployment)\n",
        "- Number of parameters (affects memory usage)\n",
        "- Accuracy per million parameters (efficiency metric)\n",
        "\n",
        "Looking at our results, MobileNetV2 likely provides the best balance because it was specifically designed for mobile and edge devices where computational resources are limited, while still maintaining good accuracy.\n",
        "\n",
        "## 2. How does model size affect training time versus inference time? Explain the differences observed.\n",
        "\n",
        "Compare the relationship between:\n",
        "- Total parameters vs. training time\n",
        "- Total parameters vs. inference time\n",
        "\n",
        "Generally, larger models (like VGG16) take longer to train and have higher inference times compared to smaller models (like MobileNetV2). However, the relationship isn't always linear due to model architecture differences.\n",
        "\n",
        "## 3. Why might you choose MobileNetV2 over ResNet50 or VGG16 for a mobile application?\n",
        "\n",
        "Consider factors like:\n",
        "- Memory footprint (parameter count)\n",
        "- Inference speed\n",
        "- Power consumption implications\n",
        "- Acceptable accuracy trade-offs for mobile scenarios\n",
        "\n",
        "MobileNetV2 was specifically designed with mobile devices in mind, using techniques like depthwise separable convolutions to reduce computational complexity while maintaining reasonable accuracy.\n",
        "\n",
        "## 4. What hardware factors significantly impact the performance of these pre-trained models?\n",
        "\n",
        "Consider:\n",
        "- GPU vs. CPU differences\n",
        "- Memory bandwidth\n",
        "- Cache size effects\n",
        "- Parallel processing capabilities\n",
        "- Model quantization benefits\n",
        "\n",
        "The performance of deep learning models is highly dependent on hardware acceleration (like GPUs), memory bandwidth, and the ability to parallelize operations, especially for larger models like ResNet50 and VGG16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3isGmNZOfKTk"
      },
      "source": [
        "# CONCLUSION\n",
        "\n",
        "In this lab, we explored three pre-trained CNN architectures (MobileNetV2, ResNet50, and VGG16) and adapted them for the MNIST dataset. We compared their performance in terms of accuracy, training time, inference time, and model size.\n",
        "\n",
        "Key takeaways from this lab:\n",
        "\n",
        "1. **Transfer Learning Effectiveness**: We successfully applied transfer learning to adapt pre-trained ImageNet models to a different domain (handwritten digit recognition).\n",
        "\n",
        "2. **Performance-Efficiency Trade-offs**: We observed the trade-offs between model size, accuracy, and computational efficiency across different architectures.\n",
        "\n",
        "3. **Adaptation Challenges**: We addressed the challenges of adapting pre-trained models to work with different input sizes and channel dimensions.\n",
        "\n",
        "4. **Quantitative Evaluation**: We performed detailed quantitative evaluation of model performance and efficiency metrics.\n",
        "\n",
        "This knowledge will be valuable when selecting appropriate model architectures for different deployment scenarios, especially when hardware constraints are a consideration."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
