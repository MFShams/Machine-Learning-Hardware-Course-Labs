{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azhwTXyF8ivC"
      },
      "source": [
        "# Lab 6: Transformers\n",
        "## Machine Learning Hardware Course\n",
        "\n",
        "This notebook is Part 1 of the transformer lab. It covers:\n",
        "1. Environment setup\n",
        "2. Understand basic operations in Transformer.\n",
        "\n",
        "Reference.\n",
        "1. https://uvadlc-notebooks.readthedocs.io.    \n",
        "2. https://github.com/phlippe/uvadlc_notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WcJQHSUGnNZ"
      },
      "source": [
        "## PART 1: Basic Operations in Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfs6YGBrDCI8"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import json\n",
        "from functools import partial\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "\n",
        "## tqdm for loading bars\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision import transforms\n",
        "\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial6\"\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJtfM_L_DUcb"
      },
      "source": [
        "Two pre-trained models are downloaded below. Make sure to have adjusted your `CHECKPOINT_PATH` before running this code if not already done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFM6Bts8DChg"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "# Github URL where saved models are stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial6/\"\n",
        "# Files to download\n",
        "pretrained_files = [\"ReverseTask.ckpt\", \"SetAnomalyTask.ckpt\"]\n",
        "\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for file_name in pretrained_files:\n",
        "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
        "    if \"/\" in file_name:\n",
        "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyQWwceSDZ_T"
      },
      "source": [
        "## The Transformer architecture\n",
        "\n",
        "In the first part of this notebook, we will implement the Transformer architecture by hand. As the architecture is so popular, there already exists a Pytorch module `nn.Transformer` ([documentation](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)) and a [tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) on how to use it for next token prediction. However, we will implement it here ourselves, to get through to the smallest details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsE8Mh8bDcaA"
      },
      "source": [
        "### What is Attention?\n",
        "\n",
        "The attention mechanism describes a recent new group of layers in neural networks that has attracted a lot of interest in the past few years, especially in sequence tasks. There are a lot of different possible definitions of \"attention\" in the literature, but the one we will use here is the following: _the attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on an input query and elements' keys_. So what does this exactly mean? The goal is to take an average over the features of multiple elements. However, instead of weighting each element equally, we want to weight them depending on their actual values. In other words, we want to dynamically decide on which inputs we want to \"attend\" more than others. In particular, an attention mechanism has usually four parts we need to specify:\n",
        "\n",
        "* **Query**: The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\n",
        "* **Keys**: For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is \"offering\", or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\n",
        "* **Values**: For each input element, we also have a value vector. This feature vector is the one we want to average over.\n",
        "* **Score function**: To rate which elements we want to pay attention to, we need to specify a score function $f_{attn}$. The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP.\n",
        "\n",
        "\n",
        "The weights of the average are calculated by a softmax over all score function outputs. Hence, we assign those value vectors a higher weight whose corresponding key is most similar to the query. If we try to describe it with pseudo-math, we can write:\n",
        "\n",
        "$$\n",
        "\\alpha_i = \\frac{\\exp\\left(f_{attn}\\left(\\text{key}_i, \\text{query}\\right)\\right)}{\\sum_j \\exp\\left(f_{attn}\\left(\\text{key}_j, \\text{query}\\right)\\right)}, \\hspace{5mm} \\text{out} = \\sum_i \\alpha_i \\cdot \\text{value}_i\n",
        "$$\n",
        "\n",
        "For every word, we have one key and one value vector. The query is compared to all keys with a score function (in this case the dot product) to determine the weights. The softmax is not visualized for simplicity. Finally, the value vectors of all words are averaged using the attention weights.\n",
        "\n",
        "Most attention mechanisms differ in terms of what queries they use, how the key and value vectors are defined, and what score function is used. The attention applied inside the Transformer architecture is called **self-attention**. In self-attention, each sequence element provides a key, value, and query. For each element, we perform an attention layer where based on its query, we check the similarity of the all sequence elements' keys, and returned a different, averaged value vector for each element. We will now go into a bit more detail by first looking at the specific implementation of the attention mechanism which is in the Transformer case the scaled dot product attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkuifjDmDvMI"
      },
      "source": [
        "### Scaled Dot Product Attention\n",
        "\n",
        "The core concept behind self-attention is the scaled dot product attention. Our goal is to have an attention mechanism with which any element in a sequence can attend to any other while still being efficient to compute. The dot product attention takes as input a set of queries $Q\\in\\mathbb{R}^{T\\times d_k}$, keys $K\\in\\mathbb{R}^{T\\times d_k}$ and values $V\\in\\mathbb{R}^{T\\times d_v}$ where $T$ is the sequence length, and $d_k$ and $d_v$ are the hidden dimensionality for queries/keys and values respectively. For simplicity, we neglect the batch dimension for now. The attention value from element $i$ to $j$ is based on its similarity of the query $Q_i$ and key $K_j$, using the dot product as the similarity metric. In math, we calculate the dot product attention as follows:\n",
        "\n",
        "$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "The matrix multiplication $QK^T$ performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape $T\\times T$. Each row represents the attention logits for a specific element $i$ to all other elements in the sequence. On these, we apply a softmax and multiply with the value vector to obtain a weighted mean (the weights being determined by the attention). Another perspective on this attention mechanism offers the computation graph which is visualized below (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n",
        "\n",
        "One aspect we haven't discussed yet is the scaling factor of $1/\\sqrt{d_k}$. This scaling factor is crucial to maintain an appropriate variance of attention values after initialization. Remember that we intialize our layers with the intention of having equal variance throughout the model, and hence, $Q$ and $K$ might also have a variance close to $1$. However, performing a dot product over two vectors with a variance $\\sigma^2$ results in a scalar having $d_k$-times higher variance:\n",
        "\n",
        "$$q_i \\sim \\mathcal{N}(0,\\sigma^2), k_i \\sim \\mathcal{N}(0,\\sigma^2) \\to \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i\\cdot k_i\\right) = \\sigma^4\\cdot d_k$$\n",
        "\n",
        "\n",
        "If we do not scale down the variance back to $\\sim\\sigma^2$, the softmax over the logits will already saturate to $1$ for one random element and $0$ for all others. The gradients through the softmax will be close to zero so that we can't learn the parameters appropriately. Note that the extra factor of $\\sigma^2$, i.e., having $\\sigma^4$ instead of $\\sigma^2$, is usually not an issue, since we keep the original variance $\\sigma^2$ close to $1$ anyways.\n",
        "\n",
        "The block `Mask (opt.)` in the diagram above represents the optional masking of specific entries in the attention matrix. This is for instance used if we stack multiple sequences with different lengths into a batch. To still benefit from parallelization in PyTorch, we pad the sentences to the same length and mask out the padding tokens during the calculation of the attention values. This is usually done by setting the respective attention logits to a very low value.\n",
        "\n",
        "After we have discussed the details of the scaled dot product attention block, we can write a function below which computes the output features given the triple of queries, keys, and values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEuRXvWNDCj6"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attn_logits = attn_logits / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "    attention = F.softmax(attn_logits, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTMKxHMiD1Sk"
      },
      "source": [
        "Note that our code above supports any additional dimensionality in front of the sequence length so that we can also use it for batches. However, for a better understanding, let's generate a few random queries, keys, and value vectors, and calculate the attention outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twv-NL5pDCmg"
      },
      "outputs": [],
      "source": [
        "seq_len, d_k = 3, 2\n",
        "pl.seed_everything(42)\n",
        "q = torch.randn(seq_len, d_k)\n",
        "k = torch.randn(seq_len, d_k)\n",
        "v = torch.randn(seq_len, d_k)\n",
        "values, attention = scaled_dot_product(q, k, v)\n",
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)\n",
        "print(\"Values\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch_u3yIID83C"
      },
      "source": [
        "Before continuing, make sure you can follow the calculation of the specific values here, and also check it by hand. It is important to fully understand how the scaled dot product attention is calculated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxVmNBNqD7OT"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "The scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it. This is why we extend the attention mechanisms to multiple heads, i.e. multiple different query-key-value triplets on the same features. Specifically, given a query, key, and value matrix, we transform those into $h$ sub-queries, sub-keys, and sub-values, which we pass through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix. Mathematically, we can express this operation as:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n",
        "    \\text{where } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "We refer to this as Multi-Head Attention layer with the learnable parameters $W_{1...h}^{Q}\\in\\mathbb{R}^{D\\times d_k}$, $W_{1...h}^{K}\\in\\mathbb{R}^{D\\times d_k}$, $W_{1...h}^{V}\\in\\mathbb{R}^{D\\times d_v}$, and $W^{O}\\in\\mathbb{R}^{h\\cdot d_v\\times d_{out}}$ ($D$ being the input dimensionality).\n",
        "\n",
        "\n",
        "How are we applying a Multi-Head Attention layer in a neural network, where we don't have an arbitrary query, key, and value vector as input? Looking at the computation graph above, a simple but effective implementation is to set the current feature map in a NN, $X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$, as $Q$, $K$ and $V$ ($B$ being the batch size, $T$ the sequence length, $d_{\\text{model}}$ the hidden dimensionality of $X$). The consecutive weight matrices $W^{Q}$, $W^{K}$, and $W^{V}$ can transform $X$ to the corresponding feature vectors that represent the queries, keys, and values of the input. Using this approach, we can implement the Multi-Head Attention module below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1k4MVPfDCrb"
      },
      "outputs": [],
      "source": [
        "# Helper function to support different mask shapes.\n",
        "# Output shape supports (batch_size, number of heads, seq length, seq length)\n",
        "# If 2D: broadcasted over batch size and number of heads\n",
        "# If 3D: broadcasted over number of heads\n",
        "# If 4D: leave as is\n",
        "def expand_mask(mask):\n",
        "    assert mask.ndim >= 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n",
        "    if mask.ndim == 3:\n",
        "        mask = mask.unsqueeze(1)\n",
        "    while mask.ndim < 4:\n",
        "        mask = mask.unsqueeze(0)\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2hrYcuRDCuA"
      },
      "outputs": [],
      "source": [
        "# http://jalammar.github.io/illustrated-transformer/\n",
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Stack all weight matrices 1...h together for efficiency\n",
        "        # Note that in many implementations you see \"bias=False\" which is optional\n",
        "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, input_dim)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
        "        self.qkv_proj.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "        if mask is not None:\n",
        "            mask = expand_mask(mask)\n",
        "        qkv = self.qkv_proj(x)\n",
        "\n",
        "        # Separate Q, K, V from linear output\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # Determine value outputs\n",
        "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
        "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
        "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
        "        o = self.o_proj(values)\n",
        "\n",
        "        if return_attention:\n",
        "            return o, attention\n",
        "        else:\n",
        "            return o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SJNR9dFEBmi"
      },
      "source": [
        "One crucial characteristic of the multi-head attention is that it is permutation-equivariant with respect to its inputs. This means that if we switch two input elements in the sequence, e.g. $X_1\\leftrightarrow X_2$ (neglecting the batch dimension for now), the output is exactly the same besides the elements 1 and 2 switched. Hence, the multi-head attention is actually looking at the input not as a sequence, but as a set of elements. This property makes the multi-head attention block and the Transformer architecture so powerful and widely applicable! But what if the order of the input is actually important for solving the task, like language modeling? The answer is to encode the position in the input features, which we will take a closer look at later (topic _Positional encodings_ below).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTmiZV-GEFV5"
      },
      "source": [
        "### Transformer Encoder\n",
        "\n",
        "Next, we will look at how to apply the multi-head attention block inside the Transformer architecture.\n",
        "\n",
        "The encoder consists of $N$ identical blocks that are applied in sequence. Taking as input $x$, it is first passed through a Multi-Head Attention block as we have implemented above. The output is added to the original input using a residual connection, and we apply a consecutive Layer Normalization on the sum. Overall, it calculates $\\text{LayerNorm}(x+\\text{Multihead}(x,x,x))$ ($x$ being $Q$, $K$ and $V$ input to the attention layer). The residual connection is crucial in the Transformer architecture for two reasons:\n",
        "\n",
        "1. Similar to ResNets, Transformers are designed to be very deep. Some models contain more than 24 blocks in the encoder. Hence, the residual connections are crucial for enabling a smooth gradient flow through the model.\n",
        "2. Without the residual connection, the information about the original sequence is lost. Remember that the Multi-Head Attention layer ignores the position of elements in a sequence, and can only learn it based on the input features. Removing the residual connections would mean that this information is lost after the first attention layer (after initialization), and with a randomly initialized query and key vector, the output vectors for position $i$ has no relation to its original input. All outputs of the attention are likely to represent similar/same information, and there is no chance for the model to distinguish which information came from which input element. An alternative option to residual connection would be to fix at least one head to focus on its original input, but this is very inefficient and does not have the benefit of the improved gradient flow.\n",
        "\n",
        "The Layer Normalization also plays an important role in the Transformer architecture as it enables faster training and provides small regularization. Additionally, it ensures that the features are in a similar magnitude among the elements in the sequence. We are not using Batch Normalization because it depends on the batch size which is often small with Transformers (they require a lot of GPU memory), and BatchNorm has shown to perform particularly bad in language as the features of words tend to have a much higher variance (there are many, very rare words which need to be considered for a good distribution estimate).\n",
        "\n",
        "Additionally to the Multi-Head Attention, a small fully connected feed-forward network is added to the model, which is applied to each position separately and identically. Specifically, the model uses a Linear$\\to$ReLU$\\to$Linear MLP. The full transformation including the residual connection can be expressed as:  \n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\text{FFN}(x) & = \\max(0, xW_1+b_1)W_2 + b_2\\\\\n",
        "    x & = \\text{LayerNorm}(x + \\text{FFN}(x))\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "This MLP adds extra complexity to the model and allows transformations on each sequence element separately. You can imagine as this allows the model to \"post-process\" the new information added by the previous Multi-Head Attention, and prepare it for the next attention block. Usually, the inner dimensionality of the MLP is 2-8$\\times$ larger than $d_{\\text{model}}$, i.e. the dimensionality of the original input $x$. The general advantage of a wider layer instead of a narrow, multi-layer MLP is the faster, parallelizable execution.\n",
        "\n",
        "Finally, after looking at all parts of the encoder architecture, we can start implementing it below. We first start by implementing a single encoder block. Additionally to the layers described above, we will add dropout layers in the MLP and on the output of the MLP and Multi-Head Attention for regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmlpN5zMDCwl"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            input_dim - Dimensionality of the input\n",
        "            num_heads - Number of heads to use in the attention block\n",
        "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
        "            dropout - Dropout probability to use in the dropout layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Attention layer\n",
        "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
        "\n",
        "        # Two-layer MLP\n",
        "        self.linear_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, input_dim)\n",
        "        )\n",
        "\n",
        "        # Layers to apply in between the main layers\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Attention part\n",
        "        attn_out = self.self_attn(x, mask=mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # MLP part\n",
        "        linear_out = self.linear_net(x)\n",
        "        x = x + self.dropout(linear_out)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vHOTr7JEKvG"
      },
      "source": [
        "Based on this block, we can implement a module for the full Transformer encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAwOLjEiEOP2"
      },
      "source": [
        "### Positional encoding\n",
        "\n",
        "We have discussed before that the Multi-Head Attention block is permutation-equivariant, and cannot distinguish whether an input comes before another one in the sequence or not. In tasks like language understanding, however, the position is important for interpreting the input words. The position information can therefore be added via the input features. We could learn a embedding for every possible position, but this would not generalize to a dynamical input sequence length. Hence, the better option is to use feature patterns that the network can identify from the features and potentially generalize to larger sequences. The specific pattern chosen by Vaswani et al. are sine and cosine functions of different frequencies, as follows:\n",
        "\n",
        "$$\n",
        "PE_{(pos,i)} = \\begin{cases}\n",
        "    \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) & \\text{if}\\hspace{3mm} i \\text{ mod } 2=0\\\\\n",
        "    \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) & \\text{otherwise}\\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$PE_{(pos,i)}$ represents the position encoding at position $pos$ in the sequence, and hidden dimensionality $i$. These values, concatenated for all hidden dimensions, are added to the original input features (in the Transformer visualization above, see \"Positional encoding\"), and constitute the position information. We distinguish between even ($i \\text{ mod } 2=0$) and uneven ($i \\text{ mod } 2=1$) hidden dimensionalities where we apply a sine/cosine respectively. The intuition behind this encoding is that you can represent $PE_{(pos+k,:)}$ as a linear function of $PE_{(pos,:)}$, which might allow the model to easily attend to relative positions. The wavelengths in different dimensions range from $2\\pi$ to $10000\\cdot 2\\pi$.\n",
        "\n",
        "The positional encoding is implemented below. The code is taken from the [PyTorch tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model) about Transformers on NLP and adjusted for our purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyIhMoECDC5L"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            d_model - Hidden dimensionality of the input.\n",
        "            max_len - Maximum length of a sequence to expect.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
        "        # Used for tensors that need to be on the same device as the module.\n",
        "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
        "        self.register_buffer('pe', pe, persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESF7CFpTEUg8"
      },
      "source": [
        "To understand the positional encoding, we can visualize it below. We will generate an image of the positional encoding over hidden dimensionality and position in a sequence. Each pixel, therefore, represents the change of the input feature we perform to encode the specific position. Let's do it below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fELefsWwDC7m"
      },
      "outputs": [],
      "source": [
        "encod_block = PositionalEncoding(d_model=48, max_len=96)\n",
        "pe = encod_block.pe.squeeze().T.cpu().numpy()\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\n",
        "pos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\n",
        "fig.colorbar(pos, ax=ax)\n",
        "ax.set_xlabel(\"Position in sequence\")\n",
        "ax.set_ylabel(\"Hidden dimension\")\n",
        "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
        "ax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\n",
        "ax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epY4wMhFEXsD"
      },
      "source": [
        "You can clearly see the sine and cosine waves with different wavelengths that encode the position in the hidden dimensions. Specifically, we can look at the sine/cosine wave for each hidden dimension separately, to get a better intuition of the pattern. Below we visualize the positional encoding for the hidden dimensions $1$, $2$, $3$ and $4$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLWE-ds7ES2s"
      },
      "outputs": [],
      "source": [
        "sns.set_theme()\n",
        "fig, ax = plt.subplots(2, 2, figsize=(12,4))\n",
        "ax = [a for a_list in ax for a in a_list]\n",
        "for i in range(len(ax)):\n",
        "    ax[i].plot(np.arange(1,17), pe[i,:16], color=f'C{i}', marker=\"o\", markersize=6, markeredgecolor=\"black\")\n",
        "    ax[i].set_title(f\"Encoding in hidden dimension {i+1}\")\n",
        "    ax[i].set_xlabel(\"Position in sequence\", fontsize=10)\n",
        "    ax[i].set_ylabel(\"Positional encoding\", fontsize=10)\n",
        "    ax[i].set_xticks(np.arange(1,17))\n",
        "    ax[i].tick_params(axis='both', which='major', labelsize=10)\n",
        "    ax[i].tick_params(axis='both', which='minor', labelsize=8)\n",
        "    ax[i].set_ylim(-1.2, 1.2)\n",
        "fig.subplots_adjust(hspace=0.8)\n",
        "sns.reset_orig()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KbX2I-pEb51"
      },
      "source": [
        "As we can see, the patterns between the hidden dimension $1$ and $2$ only differ in the starting angle. The wavelength is $2\\pi$, hence the repetition after position $6$. The hidden dimensions $2$ and $3$ have about twice the wavelength."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9u0EuY0DSPY"
      },
      "source": [
        "## PART 2: Vision Transformer\n",
        "\n",
        "This notebook is Part 2 of the transformer lab.\n",
        "\n",
        "In this tutorial, we will take a closer look at a recent new trend: Transformers for Computer Vision. Since Alexey Dosovitskiy et al. successfully applied a Transformer on a variety of image recognition benchmarks, there have been an incredible amount of follow-up works showing that CNNs might not be optimal architecture for Computer Vision anymore. But how do Vision Transformers work exactly, and what benefits and drawbacks do they offer in contrast to CNNs? We will answer these questions by implementing a Vision Transformer ourselves and train it on the popular, small dataset CIFAR10.  We will use PyTorch Lightning. Let's start with importing our standard set of libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO09T9_Y-88r"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import json\n",
        "from functools import partial\n",
        "from PIL import Image\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "\n",
        "## tqdm for loading bars\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Import tensorboard\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial15\"\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POnqtry1ESMk"
      },
      "source": [
        "We provide a pre-trained Vision Transformer which we download in the next cell. However, Vision Transformers can be relatively quickly trained on CIFAR10 with an overall training time of less than an hour on an NVIDIA TitanRTX. Feel free to experiment with training your own Transformer once you went through the whole notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLfp4Ew6-8_M"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "# Github URL where saved models are stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/\"\n",
        "# Files to download\n",
        "pretrained_files = [\"tutorial15/ViT.ckpt\", \"tutorial15/tensorboards/ViT/events.out.tfevents.ViT\",\n",
        "                    \"tutorial5/tensorboards/ResNet/events.out.tfevents.resnet\"]\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for file_name in pretrained_files:\n",
        "    file_path = os.path.join(CHECKPOINT_PATH, file_name.split(\"/\",1)[1])\n",
        "    if \"/\" in file_name.split(\"/\",1)[1]:\n",
        "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEgQPQnuEYTd"
      },
      "source": [
        "We load the CIFAR10 dataset below. We use the same setup of the datasets and data augmentations as for the CNNs. The constants in the transforms.Normalize correspond to the values that scale and shift the data to a zero mean and standard deviation of one.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPb9dO0_-9Bk"
      },
      "outputs": [],
      "source": [
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
        "                                     ])\n",
        "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
        "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
        "                                     ])\n",
        "# Loading the training dataset. We need to split it into a training and validation part\n",
        "# We need to do a little trick because the validation set should not use the augmentation.\n",
        "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
        "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
        "pl.seed_everything(42)\n",
        "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
        "pl.seed_everything(42)\n",
        "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n",
        "\n",
        "# Loading the test set\n",
        "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
        "\n",
        "# We define a set of data loaders that we can use for various purposes later.\n",
        "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
        "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
        "test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
        "\n",
        "# Visualize some examples\n",
        "NUM_IMAGES = 4\n",
        "CIFAR_images = torch.stack([val_set[idx][0] for idx in range(NUM_IMAGES)], dim=0)\n",
        "img_grid = torchvision.utils.make_grid(CIFAR_images, nrow=4, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Image examples of the CIFAR10 dataset\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIgHebcYEjC0"
      },
      "source": [
        "## Transformers for image classification\n",
        "\n",
        "Transformers have been originally proposed to process sets since it is a permutation-equivariant architecture, i.e., producing the same output permuted if the input is permuted. To apply Transformers to sequences, we have simply added a positional encoding to the input feature vectors, and the model learned by itself what to do with it. So, why not do the same thing on images? This is exactly what Alexey Dosovitskiy et al. proposed in their paper “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”. Specifically, the Vision Transformer is a model for image classification that views images as sequences of smaller patches. As a preprocessing step, we split an image of, for example, $48\\times48$ pixels into 9 $16\\times16$ patches. Each of those patches is considered to be a “word”/“token” and projected to a feature space. With adding positional encodings and a token for classification on top, we can apply a Transformer as usual to this sequence and start training it for our task. A nice GIF visualization of the architecture is shown below (figure credit - Phil Wang):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk1yXdA4GG5l"
      },
      "source": [
        "We will walk step by step through the Vision Transformer, and implement all parts by ourselves. First, let's implement the image preprocessing: an image of size $N*N$ has to be split into $(N/M)^2$ patches of size $M*M$. These represent the input words to the Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzKZn1xq-9D5"
      },
      "outputs": [],
      "source": [
        "def img_to_patch(x, patch_size, flatten_channels=True):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
        "        patch_size - Number of pixels per dimension of the patches (integer)\n",
        "        flatten_channels - If True, the patches will be returned in a flattened format\n",
        "                           as a feature vector instead of a image grid.\n",
        "    \"\"\"\n",
        "    B, C, H, W = x.shape\n",
        "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
        "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
        "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
        "    if flatten_channels:\n",
        "        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGVhoRjAFAKJ"
      },
      "source": [
        "Let's take a look at how that works for our CIFAR examples above. For our images of size $32×32$, we choose a patch size of 4. Hence, we obtain sequences of 64 patches of size $4×4$. We visualize them below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGY4BHs0-9GO"
      },
      "outputs": [],
      "source": [
        "img_patches = img_to_patch(CIFAR_images, patch_size=4, flatten_channels=False)\n",
        "\n",
        "fig, ax = plt.subplots(CIFAR_images.shape[0], 1, figsize=(14,3))\n",
        "fig.suptitle(\"Images as input sequences of patches\")\n",
        "for i in range(CIFAR_images.shape[0]):\n",
        "    img_grid = torchvision.utils.make_grid(img_patches[i], nrow=64, normalize=True, pad_value=0.9)\n",
        "    img_grid = img_grid.permute(1, 2, 0)\n",
        "    ax[i].imshow(img_grid)\n",
        "    ax[i].axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx5IcoBFFiqp"
      },
      "source": [
        "Compared to the original images, it is much harder to recognize the objects from those patch lists now. Still, this is the input we provide to the Transformer for classifying the images. The model has to learn itself how it has to combine the patches to recognize the objects. The inductive bias in CNNs that an image is a grid of pixels, is lost in this input format.\n",
        "\n",
        "After we have looked at the preprocessing, we can now start building the Transformer model. Since we have discussed the fundamentals of Multi-Head Attention, we will use the PyTorch module nn.MultiheadAttention here. Further, we use the Pre-Layer Normalization version of the Transformer blocks proposed by Ruibin Xiong et al. in 2020. The idea is to apply Layer Normalization not in between residual blocks, but instead as a first layer in the residual blocks. This reorganization of the layers supports better gradient flow and removes the necessity of a warm-up stage. A visualization of the difference between the standard Post-LN and the Pre-LN version is shown below.![pre_layer_norm.svg](data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:xlink="http://www.w3.org/1999/xlink"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   width="221.01935pt"
   height="205.05721pt"
   viewBox="0 0 221.01935 205.0572"
   version="1.2"
   id="svg1123"
   sodipodi:docname="pre_layer_norm.svg"
   inkscape:version="1.0.2 (e86c8708, 2021-01-15)">
  <metadata
     id="metadata1127">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
        <dc:title></dc:title>
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <sodipodi:namedview
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1"
     objecttolerance="10"
     gridtolerance="10"
     guidetolerance="10"
     inkscape:pageopacity="0"
     inkscape:pageshadow="2"
     inkscape:window-width="1198"
     inkscape:window-height="761"
     id="namedview1125"
     showgrid="false"
     inkscape:zoom="1.8638287"
     inkscape:cx="154.65019"
     inkscape:cy="143.26985"
     inkscape:window-x="0"
     inkscape:window-y="25"
     inkscape:window-maximized="0"
     inkscape:current-layer="surface9372"
     fit-margin-top="0"
     fit-margin-left="0"
     fit-margin-right="0"
     fit-margin-bottom="0" />
  <defs
     id="defs968">
    <g
       id="g962">
      <symbol
         overflow="visible"
         id="glyph0-0">
        <path
           style="stroke:none"
           d=""
           id="path851" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph0-1">
        <path
           style="stroke:none"
           d="M 4.640625,-2.078125 4.984375,-4.15625 H 4.78125 C 4.5625,-3.4375 4.375,-3.296875 3.65625,-3.296875 h -1.3125 l 0.34375,-2 c 0.03125,-0.25 0.09375,-0.296875 0.328125,-0.296875 H 4.25 c 1,0 1.1875,0.125 1.203125,0.9375 h 0.21875 l 0.1875,-1.28125 h -4.75 L 1.0625,-5.765625 c 0.65625,0.046875 0.765625,0.171875 0.65625,0.8125 l -0.65625,3.875 c -0.125,0.75 -0.234375,0.859375 -0.921875,0.90625 L 0.109375,0 H 2.625 L 2.640625,-0.171875 C 1.96875,-0.203125 1.859375,-0.328125 1.96875,-0.984375 L 2.296875,-2.9375 H 3.59375 c 0.734375,0 0.859375,0.140625 0.84375,0.859375 z m 0,0"
           id="path854" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph0-2">
        <path
           style="stroke:none"
           d="m 2.265625,-4.125 -1.46875,0.5 -0.03125,0.125 h 0.0625 c 0.125,-0.03125 0.234375,-0.03125 0.3125,-0.03125 0.21875,0 0.28125,0.140625 0.21875,0.53125 L 1,-0.921875 c -0.109375,0.65625 -0.203125,0.75 -0.828125,0.78125 L 0.140625,0 h 2.125 l 0.03125,-0.140625 c -0.59375,-0.03125 -0.640625,-0.125 -0.53125,-0.78125 L 2.28125,-4.09375 Z m -0.09375,-2 c -0.25,0 -0.484375,0.203125 -0.515625,0.453125 -0.046875,0.25 0.109375,0.46875 0.359375,0.46875 0.265625,0 0.5,-0.203125 0.546875,-0.46875 C 2.59375,-5.921875 2.4375,-6.125 2.171875,-6.125 Z m 0,0"
           id="path857" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph0-3">
        <path
           style="stroke:none"
           d="m 4.796875,-3.484375 0.0625,-0.34375 H 4.15625 c -0.171875,0 -0.296875,-0.03125 -0.46875,-0.09375 L 3.5,-3.984375 C 3.28125,-4.078125 3.046875,-4.125 2.8125,-4.125 c -0.84375,0 -1.609375,0.640625 -1.75,1.46875 -0.09375,0.5625 0.09375,0.890625 0.625,1.1875 -0.1875,0.171875 -0.375,0.328125 -0.4375,0.359375 -0.328125,0.265625 -0.484375,0.453125 -0.515625,0.625 -0.03125,0.1875 0.0625,0.296875 0.390625,0.46875 -0.71875,0.46875 -1,0.765625 -1.046875,1.09375 -0.09375,0.484375 0.546875,0.875 1.40625,0.875 0.671875,0 1.40625,-0.234375 1.9375,-0.609375 C 3.796875,1.0625 4,0.78125 4.0625,0.4375 4.15625,-0.109375 3.796875,-0.5 3.140625,-0.515625 l -1.15625,-0.0625 C 1.515625,-0.59375 1.3125,-0.671875 1.328125,-0.8125 c 0.03125,-0.1875 0.375,-0.5 0.640625,-0.5625 0.078125,0 0.125,0.015625 0.15625,0.015625 0.171875,0.015625 0.28125,0.015625 0.34375,0.015625 0.328125,0 0.703125,-0.125 1.03125,-0.375 0.34375,-0.25 0.515625,-0.5625 0.59375,-1 C 4.140625,-2.984375 4.125,-3.1875 4.046875,-3.484375 Z m -3.484375,3.5 c 0.28125,0.0625 1,0.125 1.4375,0.125 0.8125,0 1.09375,0.109375 1.03125,0.4375 C 3.703125,1.09375 2.953125,1.4375 1.953125,1.4375 1.15625,1.4375 0.6875,1.1875 0.75,0.796875 0.78125,0.578125 0.859375,0.46875 1.3125,0.015625 Z M 1.859375,-3.03125 c 0.09375,-0.53125 0.40625,-0.84375 0.8125,-0.84375 0.28125,0 0.484375,0.15625 0.578125,0.421875 0.109375,0.3125 0.15625,0.734375 0.09375,1.078125 -0.078125,0.5 -0.390625,0.8125 -0.8125,0.8125 C 2,-1.5625 1.71875,-2.140625 1.859375,-3 Z m 0,0"
           id="path860" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph0-4">
        <path
           style="stroke:none"
           d="M 4.375,-0.453125 H 4.328125 c -0.421875,0 -0.5,-0.09375 -0.421875,-0.5 l 0.5,-3.078125 H 3 l -0.03125,0.140625 c 0.5625,0.03125 0.640625,0.125 0.578125,0.578125 L 3.1875,-1.203125 c -0.046875,0.25 -0.109375,0.375 -0.25,0.46875 C 2.65625,-0.53125 2.375,-0.4375 2.09375,-0.4375 1.75,-0.4375 1.515625,-0.734375 1.578125,-1.109375 L 2.0625,-4.03125 H 0.75 l -0.015625,0.125 C 1.15625,-3.890625 1.265625,-3.75 1.1875,-3.34375 l -0.375,2.265625 C 0.703125,-0.375 1.046875,0.09375 1.703125,0.09375 c 0.328125,0 0.703125,-0.140625 1,-0.390625 l 0.4375,-0.390625 -0.125,0.75 0.03125,0.015625 C 3.53125,-0.09375 3.875,-0.203125 4.34375,-0.328125 Z m 0,0"
           id="path863" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph0-5">
        <path
           style="stroke:none"
           d="M 0.640625,-3.5 C 0.78125,-3.53125 0.859375,-3.53125 0.96875,-3.53125 1.1875,-3.53125 1.25,-3.390625 1.1875,-3 l -0.375,2.25 C 0.734375,-0.3125 0.65625,-0.234375 0.078125,-0.140625 L 0.046875,0 h 2.15625 L 2.21875,-0.140625 C 1.609375,-0.15625 1.484375,-0.296875 1.5625,-0.8125 l 0.34375,-2.015625 c 0.046875,-0.28125 0.5,-0.734375 0.75,-0.734375 0.046875,0 0.125,0.046875 0.21875,0.140625 0.109375,0.125 0.203125,0.171875 0.3125,0.171875 0.21875,0 0.375,-0.15625 0.421875,-0.40625 C 3.65625,-3.953125 3.5,-4.125 3.203125,-4.125 c -0.375,0 -0.671875,0.203125 -1.21875,0.84375 l 0.125,-0.828125 L 2.078125,-4.125 c -0.515625,0.203125 -0.84375,0.3125 -1.40625,0.484375 z m 0,0"
           id="path866" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph0-6">
        <path
           style="stroke:none"
           d="m 3.90625,-1.46875 c -0.546875,0.671875 -0.96875,0.9375 -1.546875,0.9375 -0.515625,0 -0.859375,-0.265625 -1.03125,-0.765625 -0.09375,-0.34375 -0.125,-0.640625 -0.046875,-1.1875 H 4.046875 C 4.078125,-3.0625 4.03125,-3.328125 3.84375,-3.609375 3.625,-3.9375 3.25,-4.125 2.796875,-4.125 2.34375,-4.125 1.890625,-3.96875 1.5,-3.65625 c -0.484375,0.359375 -0.84375,1 -0.953125,1.734375 -0.203125,1.234375 0.3125,2.015625 1.34375,2.015625 0.84375,0 1.609375,-0.53125 2.140625,-1.5 z M 1.359375,-2.765625 c 0.203125,-0.703125 0.5625,-1.03125 1.109375,-1.03125 0.546875,0 0.71875,0.25 0.71875,1.03125 z m 0,0"
           id="path869" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph0-7">
        <path
           style="stroke:none"
           d="M 3.625,-6.0625 1.875,-5.25 1.84375,-5.125 C 1.96875,-5.171875 2.078125,-5.203125 2.109375,-5.21875 2.28125,-5.28125 2.4375,-5.3125 2.53125,-5.3125 c 0.1875,0 0.25,0.125 0.1875,0.421875 l -0.671875,4.0625 C 2,-0.53125 1.890625,-0.328125 1.734375,-0.25 1.59375,-0.171875 1.46875,-0.140625 1.078125,-0.140625 L 1.0625,0 H 3.53125 L 3.5625,-0.140625 c -0.703125,0 -0.84375,-0.09375 -0.765625,-0.515625 L 3.6875,-6.046875 Z m 0,0"
           id="path872" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph0-8">
        <path
           style="stroke:none"
           d="m 1.28125,-0.890625 c -0.28125,0 -0.546875,0.234375 -0.59375,0.5 C 0.640625,-0.125 0.828125,0.09375 1.09375,0.09375 c 0.28125,0 0.546875,-0.21875 0.59375,-0.484375 0.046875,-0.265625 -0.140625,-0.5 -0.40625,-0.5 z m 0,0"
           id="path875" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-0">
        <path
           style="stroke:none"
           d=""
           id="path878" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-1">
        <path
           style="stroke:none"
           d="M 2.640625,-6.0625 C 1.96875,-5.625 1.703125,-5.40625 1.375,-4.984375 c -0.640625,0.78125 -0.953125,1.671875 -0.953125,2.71875 0,1.140625 0.34375,2.03125 1.125,2.9375 0.359375,0.4375 0.59375,0.625 1.0625,0.921875 L 2.71875,1.4375 C 2,0.875 1.75,0.5625 1.515625,-0.109375 c -0.21875,-0.59375 -0.3125,-1.28125 -0.3125,-2.171875 0,-0.953125 0.109375,-1.6875 0.359375,-2.234375 0.25,-0.5625 0.515625,-0.890625 1.15625,-1.40625 z m 0,0"
           id="path881" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-2">
        <path
           style="stroke:none"
           d="m 3.953125,-0.59375 c -0.15625,0.125 -0.265625,0.171875 -0.390625,0.171875 -0.203125,0 -0.28125,-0.125 -0.28125,-0.515625 v -1.75 C 3.28125,-3.15625 3.25,-3.421875 3.109375,-3.625 2.90625,-3.953125 2.53125,-4.125 2,-4.125 c -0.84375,0 -1.5,0.4375 -1.5,1 0,0.203125 0.171875,0.390625 0.390625,0.390625 0.203125,0 0.390625,-0.1875 0.390625,-0.375 0,-0.03125 0,-0.078125 -0.015625,-0.140625 C 1.25,-3.34375 1.25,-3.40625 1.25,-3.46875 c 0,-0.25 0.28125,-0.4375 0.640625,-0.4375 0.4375,0 0.671875,0.25 0.671875,0.734375 V -2.625 c -1.375,0.5625 -1.53125,0.640625 -1.90625,0.96875 -0.203125,0.1875 -0.328125,0.484375 -0.328125,0.78125 0,0.5625 0.390625,0.96875 0.9375,0.96875 0.390625,0 0.765625,-0.1875 1.3125,-0.65625 0.046875,0.46875 0.203125,0.65625 0.5625,0.65625 0.3125,0 0.5,-0.109375 0.8125,-0.453125 z M 2.5625,-1.109375 c 0,0.28125 -0.046875,0.359375 -0.234375,0.46875 C 2.125,-0.515625 1.875,-0.4375 1.6875,-0.4375 1.375,-0.4375 1.125,-0.734375 1.125,-1.125 v -0.03125 c 0,-0.53125 0.359375,-0.859375 1.4375,-1.25 z m 0,0"
           id="path884" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-3">
        <path
           style="stroke:none"
           d="M 0.34375,1.59375 C 1,1.15625 1.265625,0.921875 1.59375,0.515625 2.234375,-0.265625 2.546875,-1.171875 2.546875,-2.21875 2.546875,-3.359375 2.21875,-4.234375 1.4375,-5.140625 1.0625,-5.578125 0.828125,-5.78125 0.359375,-6.0625 l -0.09375,0.140625 c 0.703125,0.5625 0.953125,0.875 1.203125,1.546875 0.21875,0.609375 0.3125,1.296875 0.3125,2.1875 0,0.9375 -0.125,1.671875 -0.359375,2.21875 -0.25,0.5625 -0.515625,0.890625 -1.15625,1.40625 z m 0,0"
           id="path887" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-4">
        <path
           style="stroke:none"
           d="m 1.8125,-2.609375 c 0.21875,0.015625 0.375,0.03125 0.609375,0.03125 0.703125,0 1.1875,-0.09375 1.5625,-0.3125 0.53125,-0.28125 0.859375,-0.828125 0.859375,-1.421875 0,-0.375 -0.125,-0.71875 -0.359375,-0.984375 C 4.125,-5.6875 3.34375,-5.9375 2.5,-5.9375 H 0.140625 v 0.171875 c 0.65625,0.078125 0.75,0.15625 0.75,0.8125 v 3.875 c 0,0.75 -0.0625,0.84375 -0.75,0.90625 V 0 h 2.5 V -0.171875 C 1.9375,-0.203125 1.8125,-0.328125 1.8125,-0.984375 Z m 0,-2.6875 c 0,-0.25 0.0625,-0.3125 0.296875,-0.3125 1.203125,0 1.765625,0.421875 1.765625,1.34375 0,0.875 -0.53125,1.328125 -1.5625,1.328125 -0.171875,0 -0.296875,-0.015625 -0.5,-0.03125 z m 0,0"
           id="path890" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-5">
        <path
           style="stroke:none"
           d="m 2.234375,-4.125 c -1.15625,0 -1.96875,0.859375 -1.96875,2.09375 0,1.21875 0.828125,2.125 1.953125,2.125 1.125,0 1.984375,-0.953125 1.984375,-2.1875 0,-1.1875 -0.828125,-2.03125 -1.96875,-2.03125 z M 2.125,-3.875 c 0.75,0 1.265625,0.859375 1.265625,2.09375 0,1.015625 -0.390625,1.625 -1.0625,1.625 -0.359375,0 -0.6875,-0.21875 -0.875,-0.578125 C 1.203125,-1.203125 1.0625,-1.828125 1.0625,-2.46875 1.0625,-3.3125 1.484375,-3.875 2.125,-3.875 Z m 0,0"
           id="path893" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-6">
        <path
           style="stroke:none"
           d="M 2.8125,-2.8125 2.78125,-4.03125 H 2.6875 l -0.03125,0.015625 c -0.078125,0.0625 -0.078125,0.0625 -0.125,0.0625 -0.046875,0 -0.140625,-0.015625 -0.234375,-0.0625 -0.203125,-0.0625 -0.390625,-0.09375 -0.625,-0.09375 -0.703125,0 -1.21875,0.453125 -1.21875,1.09375 0,0.5 0.28125,0.859375 1.046875,1.296875 l 0.515625,0.296875 c 0.3125,0.171875 0.46875,0.390625 0.46875,0.671875 0,0.390625 -0.28125,0.640625 -0.734375,0.640625 -0.3125,0 -0.59375,-0.109375 -0.765625,-0.3125 -0.1875,-0.21875 -0.265625,-0.4375 -0.375,-0.9375 H 0.46875 V 0.03125 h 0.109375 c 0.0625,-0.078125 0.09375,-0.109375 0.203125,-0.109375 0.078125,0 0.203125,0.03125 0.421875,0.078125 0.25,0.046875 0.484375,0.09375 0.640625,0.09375 0.6875,0 1.265625,-0.53125 1.265625,-1.15625 0,-0.4375 -0.21875,-0.734375 -0.75,-1.0625 l -0.96875,-0.578125 c -0.25,-0.140625 -0.375,-0.359375 -0.375,-0.609375 0,-0.359375 0.265625,-0.609375 0.6875,-0.609375 0.5,0 0.765625,0.3125 0.984375,1.109375 z m 0,0"
           id="path896" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-7">
        <path
           style="stroke:none"
           d="M 2.28125,-4.03125 H 1.375 v -1.046875 c 0,-0.09375 0,-0.109375 -0.0625,-0.109375 -0.0625,0.078125 -0.109375,0.15625 -0.171875,0.25 -0.34375,0.484375 -0.734375,0.921875 -0.875,0.953125 -0.09375,0.0625 -0.15625,0.125 -0.15625,0.171875 0,0.03125 0.015625,0.046875 0.046875,0.0625 H 0.625 v 2.703125 c 0,0.75 0.265625,1.140625 0.796875,1.140625 0.4375,0 0.78125,-0.21875 1.078125,-0.6875 L 2.375,-0.6875 C 2.1875,-0.46875 2.03125,-0.375 1.84375,-0.375 1.515625,-0.375 1.375,-0.625 1.375,-1.1875 V -3.75 h 0.90625 z m 0,0"
           id="path899" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-8">
        <path
           style="stroke:none"
           d="m 0.34375,-2.3125 v 0.578125 H 2.546875 V -2.3125 Z m 0,0"
           id="path902" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-9">
        <path
           style="stroke:none"
           d="M 5.125,-1.5625 C 4.96875,-1.21875 4.84375,-1.015625 4.703125,-0.859375 4.390625,-0.515625 3.90625,-0.34375 3.15625,-0.34375 H 2.5625 c -0.65625,0 -0.765625,-0.0625 -0.765625,-0.375 v -4.234375 c 0,-0.640625 0.125,-0.765625 0.828125,-0.8125 V -5.9375 H 0.109375 v 0.171875 c 0.65625,0.046875 0.78125,0.171875 0.78125,0.8125 v 3.96875 c 0,0.640625 -0.125,0.765625 -0.78125,0.8125 V 0 h 4.8125 L 5.34375,-1.5625 Z m 0,0"
           id="path905" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-10">
        <path
           style="stroke:none"
           d="M 5.46875,0.09375 V -4.625 c 0,-0.53125 0.09375,-0.859375 0.265625,-0.984375 0.125,-0.078125 0.25,-0.125 0.578125,-0.15625 V -5.9375 H 4.21875 v 0.171875 c 0.34375,0.03125 0.46875,0.0625 0.59375,0.140625 0.1875,0.125 0.265625,0.453125 0.265625,1 v 3.03125 l -3.4375,-4.34375 h -1.53125 v 0.171875 c 0.375,0 0.5,0.078125 0.859375,0.484375 v 3.96875 c 0,0.921875 -0.125,1.09375 -0.859375,1.140625 V 0 h 2.09375 V -0.171875 C 1.515625,-0.203125 1.375,-0.40625 1.375,-1.3125 V -4.828125 L 5.3125,0.09375 Z m 0,0"
           id="path908" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-11">
        <path
           style="stroke:none"
           d="m 2.265625,-5.5625 v 4.484375 c 0,0.765625 -0.09375,0.859375 -0.828125,0.90625 V 0 H 4.046875 V -0.171875 C 3.3125,-0.203125 3.1875,-0.328125 3.1875,-0.984375 V -5.5625 h 0.484375 c 1.015625,0 1.203125,0.15625 1.40625,1.15625 h 0.21875 L 5.25,-5.9375 H 0.203125 L 0.15625,-4.40625 H 0.359375 C 0.578125,-5.390625 0.78125,-5.5625 1.78125,-5.5625 Z m 0,0"
           id="path911" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-12">
        <path
           style="stroke:none"
           d="m 0.0625,-3.5 c 0.125,-0.03125 0.203125,-0.03125 0.3125,-0.03125 0.21875,0 0.296875,0.140625 0.296875,0.53125 v 2.25 c 0,0.4375 -0.0625,0.515625 -0.625,0.609375 V 0 H 2.1875 V -0.140625 C 1.578125,-0.15625 1.4375,-0.296875 1.4375,-0.8125 v -2.015625 c 0,-0.28125 0.375,-0.734375 0.625,-0.734375 0.046875,0 0.125,0.046875 0.21875,0.140625 C 2.4375,-3.296875 2.53125,-3.25 2.640625,-3.25 2.859375,-3.25 3,-3.40625 3,-3.65625 3,-3.953125 2.8125,-4.125 2.5,-4.125 c -0.375,0 -0.625,0.203125 -1.0625,0.84375 V -4.109375 L 1.390625,-4.125 C 0.90625,-3.921875 0.59375,-3.8125 0.0625,-3.640625 Z m 0,0"
           id="path914" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-13">
        <path
           style="stroke:none"
           d="m 0.140625,-3.5625 c 0.0625,-0.03125 0.140625,-0.046875 0.25,-0.046875 0.25,0 0.328125,0.140625 0.328125,0.578125 v 2.21875 c 0,0.515625 -0.109375,0.640625 -0.5625,0.671875 V 0 H 2.0625 V -0.140625 C 1.59375,-0.171875 1.46875,-0.28125 1.46875,-0.59375 V -3.125 c 0.421875,-0.40625 0.625,-0.5 0.921875,-0.5 0.4375,0 0.640625,0.265625 0.640625,0.859375 v 1.875 c 0,0.5625 -0.109375,0.71875 -0.5625,0.75 V 0 h 1.859375 v -0.140625 c -0.4375,-0.03125 -0.53125,-0.140625 -0.53125,-0.578125 v -2.0625 c 0,-0.84375 -0.40625,-1.34375 -1.0625,-1.34375 -0.40625,0 -0.6875,0.15625 -1.296875,0.71875 V -4.109375 L 1.375,-4.125 c -0.4375,0.15625 -0.734375,0.265625 -1.234375,0.40625 z m 0,0"
           id="path917" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-14">
        <path
           style="stroke:none"
           d="M 2.765625,-4.03125 H 1.65625 v -1.046875 c 0,-0.515625 0.171875,-0.796875 0.515625,-0.796875 0.1875,0 0.3125,0.09375 0.46875,0.34375 0.140625,0.234375 0.25,0.328125 0.40625,0.328125 0.203125,0 0.375,-0.15625 0.375,-0.359375 0,-0.328125 -0.390625,-0.5625 -0.921875,-0.5625 -0.5625,0 -1.03125,0.234375 -1.265625,0.65625 C 1,-5.078125 0.921875,-4.75 0.921875,-4.03125 H 0.1875 V -3.75 h 0.734375 v 2.8125 c 0,0.65625 -0.09375,0.765625 -0.75,0.796875 V 0 H 2.5 V -0.140625 C 1.765625,-0.15625 1.671875,-0.265625 1.671875,-0.9375 V -3.75 h 1.09375 z m 0,0"
           id="path920" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-15">
        <path
           style="stroke:none"
           d="m 0.171875,-3.5625 c 0.109375,-0.03125 0.1875,-0.046875 0.28125,-0.046875 0.234375,0 0.3125,0.140625 0.3125,0.578125 v 2.265625 c 0,0.484375 -0.125,0.625 -0.625,0.625 V 0 H 2.125 V -0.140625 C 1.65625,-0.15625 1.515625,-0.25 1.515625,-0.59375 V -3.125 c 0,-0.015625 0.078125,-0.109375 0.140625,-0.171875 C 1.875,-3.5 2.265625,-3.65625 2.578125,-3.65625 c 0.390625,0 0.578125,0.3125 0.578125,0.9375 v 1.953125 c 0,0.5 -0.09375,0.59375 -0.59375,0.625 V 0 h 2 v -0.140625 c -0.515625,0 -0.640625,-0.15625 -0.640625,-0.71875 v -2.25 C 4.1875,-3.5 4.484375,-3.65625 4.890625,-3.65625 c 0.5,0 0.671875,0.234375 0.671875,0.984375 v 1.890625 c 0,0.515625 -0.078125,0.578125 -0.59375,0.640625 V 0 H 6.921875 V -0.140625 L 6.6875,-0.15625 C 6.421875,-0.171875 6.3125,-0.328125 6.3125,-0.6875 V -2.53125 C 6.3125,-3.59375 5.96875,-4.125 5.265625,-4.125 4.75,-4.125 4.296875,-3.890625 3.8125,-3.375 3.65625,-3.890625 3.359375,-4.125 2.875,-4.125 2.46875,-4.125 2.21875,-4 1.484375,-3.4375 V -4.109375 L 1.421875,-4.125 c -0.453125,0.171875 -0.765625,0.265625 -1.25,0.40625 z m 0,0"
           id="path923" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-16">
        <path
           style="stroke:none"
           d="m 3.640625,-1.46875 c -0.421875,0.671875 -0.8125,0.9375 -1.375,0.9375 -0.515625,0 -0.890625,-0.265625 -1.15625,-0.765625 -0.15625,-0.34375 -0.21875,-0.640625 -0.25,-1.1875 H 3.625 C 3.546875,-3.0625 3.453125,-3.328125 3.234375,-3.609375 2.96875,-3.9375 2.5625,-4.125 2.09375,-4.125 c -0.453125,0 -0.875,0.15625 -1.203125,0.46875 -0.421875,0.359375 -0.671875,1 -0.671875,1.734375 0,1.234375 0.640625,2.015625 1.671875,2.015625 0.859375,0 1.53125,-0.53125 1.90625,-1.5 z m -2.75,-1.296875 c 0.09375,-0.703125 0.390625,-1.03125 0.9375,-1.03125 0.546875,0 0.765625,0.25 0.875,1.03125 z m 0,0"
           id="path926" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-17">
        <path
           style="stroke:none"
           d="M 0.171875,-5.59375 H 0.21875 c 0.109375,0 0.203125,-0.015625 0.28125,-0.015625 0.28125,0 0.375,0.125 0.375,0.546875 v 4.28125 c 0,0.484375 -0.125,0.609375 -0.6875,0.640625 V 0 H 2.296875 V -0.140625 C 1.734375,-0.171875 1.625,-0.265625 1.625,-0.75 V -6.109375 L 1.59375,-6.125 C 1.125,-5.96875 0.78125,-5.890625 0.171875,-5.734375 Z m 0,0"
           id="path929" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-18">
        <path
           style="stroke:none"
           d="M 4.25,-4.03125 H 3.03125 v 0.125 c 0.296875,0 0.4375,0.078125 0.4375,0.234375 0,0.03125 -0.015625,0.078125 -0.03125,0.140625 L 2.5625,-1.046875 1.53125,-3.3125 c -0.046875,-0.125 -0.078125,-0.25 -0.078125,-0.34375 0,-0.171875 0.125,-0.234375 0.515625,-0.25 v -0.125 H 0.125 v 0.125 c 0.234375,0.03125 0.390625,0.125 0.453125,0.28125 L 1.59375,-1.421875 1.625,-1.34375 1.765625,-1.078125 c 0.25,0.453125 0.390625,0.765625 0.390625,0.90625 0,0.140625 -0.203125,0.703125 -0.359375,0.96875 -0.125,0.234375 -0.328125,0.40625 -0.453125,0.40625 -0.046875,0 -0.125,-0.015625 -0.21875,-0.0625 -0.171875,-0.0625 -0.328125,-0.09375 -0.46875,-0.09375 -0.203125,0 -0.390625,0.171875 -0.390625,0.390625 0,0.296875 0.28125,0.515625 0.65625,0.515625 0.609375,0 1.03125,-0.5 1.515625,-1.796875 L 3.8125,-3.5 C 3.9375,-3.78125 4.03125,-3.875 4.25,-3.90625 Z m 0,0"
           id="path932" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-19">
        <path
           style="stroke:none"
           d="m 0.953125,1.265625 c 0.625,-0.3125 1,-0.84375 1,-1.375 0,-0.453125 -0.3125,-0.8125 -0.703125,-0.8125 -0.328125,0 -0.53125,0.203125 -0.53125,0.515625 0,0.296875 0.1875,0.453125 0.53125,0.453125 0.046875,0 0.109375,0 0.15625,-0.015625 0.0625,-0.015625 0.0625,-0.015625 0.078125,-0.015625 0.078125,0 0.125,0.0625 0.125,0.125 0,0.296875 -0.25,0.625 -0.734375,0.953125 z m 0.265625,-5.375 c -0.265625,0 -0.5,0.21875 -0.5,0.5 0,0.265625 0.234375,0.484375 0.484375,0.484375 0.28125,0 0.515625,-0.21875 0.515625,-0.484375 0,-0.28125 -0.234375,-0.5 -0.5,-0.5 z m 0,0"
           id="path935" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph1-20">
        <path
           style="stroke:none"
           d="M 1.375,-6.109375 1.328125,-6.125 c -0.375,0.140625 -0.625,0.203125 -1.046875,0.328125 l -0.25,0.0625 v 0.140625 c 0.046875,0 0.078125,0 0.140625,0 0.359375,0 0.4375,0.078125 0.4375,0.453125 v 4.65625 c 0,0.28125 0.765625,0.578125 1.484375,0.578125 1.171875,0 2.09375,-0.984375 2.09375,-2.265625 C 4.1875,-3.28125 3.5,-4.125 2.609375,-4.125 2.0625,-4.125 1.546875,-3.796875 1.375,-3.359375 Z m 0,3.21875 c 0,-0.34375 0.40625,-0.671875 0.875,-0.671875 0.703125,0 1.140625,0.703125 1.140625,1.796875 0,1 -0.421875,1.5625 -1.15625,1.5625 C 1.765625,-0.203125 1.375,-0.40625 1.375,-0.625 Z m 0,0"
           id="path938" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph2-0">
        <path
           style="stroke:none"
           d=""
           id="path941" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph2-1">
        <path
           style="stroke:none"
           d="M 0.171875,-5.59375 H 0.21875 c 0.109375,0 0.21875,-0.015625 0.28125,-0.015625 0.296875,0 0.375,0.125 0.375,0.546875 v 4.28125 c 0,0.484375 -0.125,0.609375 -0.6875,0.640625 V 0 h 2.125 V -0.140625 C 1.734375,-0.171875 1.625,-0.265625 1.625,-0.75 V -6.109375 L 1.59375,-6.125 C 1.125,-5.96875 0.796875,-5.890625 0.171875,-5.734375 Z m 0,0"
           id="path944" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph2-2">
        <path
           style="stroke:none"
           d="m 3.96875,-0.59375 c -0.15625,0.125 -0.265625,0.171875 -0.40625,0.171875 -0.203125,0 -0.265625,-0.125 -0.265625,-0.515625 v -1.75 C 3.296875,-3.15625 3.25,-3.421875 3.125,-3.625 c -0.203125,-0.328125 -0.59375,-0.5 -1.109375,-0.5 -0.84375,0 -1.515625,0.4375 -1.515625,1 0,0.203125 0.1875,0.390625 0.390625,0.390625 0.21875,0 0.40625,-0.1875 0.40625,-0.375 C 1.296875,-3.140625 1.28125,-3.1875 1.28125,-3.25 1.25,-3.34375 1.25,-3.40625 1.25,-3.46875 c 0,-0.25 0.28125,-0.4375 0.640625,-0.4375 0.4375,0 0.6875,0.25 0.6875,0.734375 V -2.625 C 1.1875,-2.0625 1.046875,-1.984375 0.65625,-1.65625 0.453125,-1.46875 0.328125,-1.171875 0.328125,-0.875 c 0,0.5625 0.390625,0.96875 0.953125,0.96875 0.390625,0 0.75,-0.1875 1.296875,-0.65625 0.046875,0.46875 0.21875,0.65625 0.578125,0.65625 0.3125,0 0.5,-0.109375 0.8125,-0.453125 z M 2.578125,-1.109375 c 0,0.28125 -0.046875,0.359375 -0.234375,0.46875 C 2.125,-0.515625 1.875,-0.4375 1.6875,-0.4375 1.375,-0.4375 1.125,-0.734375 1.125,-1.125 v -0.03125 c 0,-0.53125 0.359375,-0.859375 1.453125,-1.25 z m 0,0"
           id="path947" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph2-3">
        <path
           style="stroke:none"
           d="m 4.265625,-4.03125 h -1.21875 v 0.125 c 0.296875,0 0.4375,0.078125 0.4375,0.234375 0,0.03125 -0.015625,0.078125 -0.046875,0.140625 L 2.578125,-1.046875 1.546875,-3.3125 c -0.0625,-0.125 -0.09375,-0.25 -0.09375,-0.34375 0,-0.171875 0.140625,-0.234375 0.515625,-0.25 v -0.125 H 0.125 v 0.125 c 0.234375,0.03125 0.390625,0.125 0.453125,0.28125 l 1.03125,2.203125 0.015625,0.078125 0.140625,0.265625 c 0.25,0.453125 0.390625,0.765625 0.390625,0.90625 0,0.140625 -0.203125,0.703125 -0.359375,0.96875 -0.125,0.234375 -0.3125,0.40625 -0.4375,0.40625 -0.0625,0 -0.140625,-0.015625 -0.234375,-0.0625 -0.171875,-0.0625 -0.3125,-0.09375 -0.46875,-0.09375 -0.203125,0 -0.390625,0.171875 -0.390625,0.390625 0,0.296875 0.296875,0.515625 0.671875,0.515625 0.59375,0 1.03125,-0.5 1.515625,-1.796875 L 3.828125,-3.5 c 0.125,-0.28125 0.21875,-0.375 0.4375,-0.40625 z m 0,0"
           id="path950" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph2-4">
        <path
           style="stroke:none"
           d="m 3.65625,-1.46875 c -0.421875,0.671875 -0.8125,0.9375 -1.390625,0.9375 -0.5,0 -0.890625,-0.265625 -1.15625,-0.765625 C 0.953125,-1.640625 0.890625,-1.9375 0.875,-2.484375 h 2.75 C 3.5625,-3.0625 3.46875,-3.328125 3.25,-3.609375 2.984375,-3.9375 2.5625,-4.125 2.09375,-4.125 c -0.4375,0 -0.859375,0.15625 -1.203125,0.46875 -0.421875,0.359375 -0.671875,1 -0.671875,1.734375 0,1.234375 0.65625,2.015625 1.6875,2.015625 0.84375,0 1.515625,-0.53125 1.890625,-1.5 z M 0.890625,-2.765625 c 0.09375,-0.703125 0.40625,-1.03125 0.953125,-1.03125 0.546875,0 0.75,0.25 0.875,1.03125 z m 0,0"
           id="path953" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph2-5">
        <path
           style="stroke:none"
           d="m 0.0625,-3.5 c 0.125,-0.03125 0.203125,-0.03125 0.3125,-0.03125 0.21875,0 0.3125,0.140625 0.3125,0.53125 v 2.25 c 0,0.4375 -0.0625,0.515625 -0.640625,0.609375 V 0 h 2.15625 V -0.140625 C 1.59375,-0.15625 1.4375,-0.296875 1.4375,-0.8125 v -2.015625 c 0,-0.28125 0.390625,-0.734375 0.625,-0.734375 0.046875,0 0.140625,0.046875 0.234375,0.140625 C 2.4375,-3.296875 2.53125,-3.25 2.65625,-3.25 2.875,-3.25 3,-3.40625 3,-3.65625 3,-3.953125 2.8125,-4.125 2.515625,-4.125 2.140625,-4.125 1.875,-3.921875 1.4375,-3.28125 V -4.109375 L 1.390625,-4.125 C 0.921875,-3.921875 0.59375,-3.8125 0.0625,-3.640625 Z m 0,0"
           id="path956" />
      </symbol>
      <symbol
         overflow="visible"
         id="glyph2-6">
        <path
           style="stroke:none"
           d="m 1.125,-0.890625 c -0.265625,0 -0.5,0.234375 -0.5,0.5 0,0.265625 0.234375,0.484375 0.484375,0.484375 0.28125,0 0.515625,-0.21875 0.515625,-0.484375 0,-0.265625 -0.234375,-0.5 -0.5,-0.5 z m 0,0"
           id="path959" />
      </symbol>
    </g>
    <clipPath
       id="clip1">
      <path
         d="M 42.703125,4.011719 H 197.83984 V 191.17578 H 42.703125 Z m 0,0"
         id="path964" />
    </clipPath>
    <image
       id="image9376"
       width="593"
       height="715"
       xlink:href="data:image/png;base64,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" />
  <style>
      svg { background-color: rgb(252, 252, 252); }
    </style>
  </defs>
  <g
     id="surface9372"
     transform="translate(-4.556736,-4.011719)">
    <g
       clip-path="url(#clip1)"
       clip-rule="nonzero"
       id="g972">
      <use
         xlink:href="#image9376"
         transform="matrix(0.262583,0,0,0.26259,42.43166,3.6933)"
         id="use970"
         x="0"
         y="0"
         width="100%"
         height="100%" />
    </g>
    <g
       style="fill:#211e1e;fill-opacity:1"
       id="g1000"
       transform="translate(-34.5)">
      <use
         xlink:href="#glyph1-1"
         x="38.634861"
         y="207.1158"
         id="use994"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-2"
         x="41.611713"
         y="207.1158"
         id="use996"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-3"
         x="45.580853"
         y="207.1158"
         id="use998"
         width="100%"
         height="100%" />
    </g>
    <g
       style="fill:#211e1e;fill-opacity:1"
       id="g1016"
       transform="translate(-34.5)">
      <use
         xlink:href="#glyph1-4"
         x="50.810459"
         y="207.1158"
         id="use1002"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-5"
         x="55.780823"
         y="207.1158"
         id="use1004"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-6"
         x="60.250572"
         y="207.1158"
         id="use1006"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-7"
         x="63.728039"
         y="207.1158"
         id="use1008"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-8"
         x="66.213219"
         y="207.1158"
         id="use1010"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-9"
         x="69.190071"
         y="207.1158"
         id="use1012"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-10"
         x="74.652107"
         y="207.1158"
         id="use1014"
         width="100%"
         height="100%" />
    </g>
    <g
       style="fill:#211e1e;fill-opacity:1"
       id="g1020"
       transform="translate(-34.5)">
      <use
         xlink:href="#glyph1-11"
         x="83.351135"
         y="207.1158"
         id="use1018"
         width="100%"
         height="100%" />
    </g>
    <g
       style="fill:#211e1e;fill-opacity:1"
       id="g1042"
       transform="translate(-34.5)">
      <use
         xlink:href="#glyph1-12"
         x="88.500282"
         y="207.1158"
         id="use1022"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-2"
         x="91.477135"
         y="207.1158"
         id="use1024"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-13"
         x="95.446274"
         y="207.1158"
         id="use1026"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-6"
         x="99.916023"
         y="207.1158"
         id="use1028"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-14"
         x="103.39349"
         y="207.1158"
         id="use1030"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-5"
         x="106.37035"
         y="207.1158"
         id="use1032"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-12"
         x="110.8401"
         y="207.1158"
         id="use1034"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-15"
         x="113.81695"
         y="207.1158"
         id="use1036"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-16"
         x="120.77188"
         y="207.1158"
         id="use1038"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-12"
         x="124.74102"
         y="207.1158"
         id="use1040"
         width="100%"
         height="100%" />
    </g>
    <g
       style="fill:#211e1e;fill-opacity:1"
       id="g1056"
       transform="translate(-34.5)">
      <use
         xlink:href="#glyph1-17"
         x="129.97063"
         y="207.1158"
         id="use1044"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-2"
         x="132.45581"
         y="207.1158"
         id="use1046"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-18"
         x="136.42494"
         y="207.1158"
         id="use1048"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-16"
         x="140.8947"
         y="207.1158"
         id="use1050"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-12"
         x="144.86383"
         y="207.1158"
         id="use1052"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-19"
         x="147.84068"
         y="207.1158"
         id="use1054"
         width="100%"
         height="100%" />
    </g>
    <g
       style="fill:#211e1e;fill-opacity:1"
       id="g1064"
       transform="translate(-34.5)">
      <use
         xlink:href="#glyph1-1"
         x="152.57863"
         y="207.1158"
         id="use1058"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-20"
         x="155.55548"
         y="207.1158"
         id="use1060"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-3"
         x="160.02522"
         y="207.1158"
         id="use1062"
         width="100%"
         height="100%" />
    </g>
    <g
       style="fill:#211e1e;fill-opacity:1"
       id="g1078"
       transform="translate(-34.5)">
      <use
         xlink:href="#glyph1-4"
         x="165.2459"
         y="207.1158"
         id="use1066"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-12"
         x="170.21625"
         y="207.1158"
         id="use1068"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-16"
         x="173.19312"
         y="207.1158"
         id="use1070"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-8"
         x="177.16225"
         y="207.1158"
         id="use1072"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-9"
         x="180.1391"
         y="207.1158"
         id="use1074"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-10"
         x="185.60114"
         y="207.1158"
         id="use1076"
         width="100%"
         height="100%" />
    </g>
    <g
       style="fill:#211e1e;fill-opacity:1"
       id="g1082"
       transform="translate(-34.5)">
      <use
         xlink:href="#glyph1-11"
         x="194.30821"
         y="207.1158"
         id="use1080"
         width="100%"
         height="100%" />
    </g>
    <g
       style="fill:#211e1e;fill-opacity:1"
       id="g1104"
       transform="translate(-34.5)">
      <use
         xlink:href="#glyph1-12"
         x="199.45735"
         y="207.1158"
         id="use1084"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-2"
         x="202.43422"
         y="207.1158"
         id="use1086"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-13"
         x="206.40335"
         y="207.1158"
         id="use1088"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-6"
         x="210.87309"
         y="207.1158"
         id="use1090"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-14"
         x="214.35057"
         y="207.1158"
         id="use1092"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-5"
         x="217.32742"
         y="207.1158"
         id="use1094"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-12"
         x="221.79716"
         y="207.1158"
         id="use1096"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-15"
         x="224.77402"
         y="207.1158"
         id="use1098"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-16"
         x="231.72896"
         y="207.1158"
         id="use1100"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph1-12"
         x="235.69809"
         y="207.1158"
         id="use1102"
         width="100%"
         height="100%" />
    </g>
    <g
       style="fill:#211e1e;fill-opacity:1"
       id="g1116"
       transform="translate(202.00353,-10.974321)">
      <use
         xlink:href="#glyph2-1"
         x="4.5177598"
         y="218.07491"
         id="use1106"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph2-2"
         x="7.0104189"
         y="218.07491"
         id="use1108"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph2-3"
         x="10.991501"
         y="218.07491"
         id="use1110"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph2-4"
         x="15.474701"
         y="218.07491"
         id="use1112"
         width="100%"
         height="100%" />
      <use
         xlink:href="#glyph2-5"
         x="19.455782"
         y="218.07491"
         id="use1114"
         width="100%"
         height="100%" />
    </g>
    <g
       style="fill:#211e1e;fill-opacity:1"
       id="g1120"
       transform="translate(202.00353,-10.974321)">
      <use
         xlink:href="#glyph2-6"
         x="21.947544"
         y="218.07491"
         id="use1118"
         width="100%"
         height="100%" />
    </g>
  </g>
</svg>
)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_39rums2IGcK"
      },
      "source": [
        "The implementation of the Pre-LN attention block looks as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8k3kev--9Il"
      },
      "outputs": [],
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            embed_dim - Dimensionality of input and attention feature vectors\n",
        "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
        "                         (usually 2-4x larger than embed_dim)\n",
        "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
        "            dropout - Amount of dropout to apply in the feed-forward network\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads,\n",
        "                                          dropout=dropout)\n",
        "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        inp_x = self.layer_norm_1(x)\n",
        "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
        "        x = x + self.linear(self.layer_norm_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_knLsBIKAd"
      },
      "source": [
        "Now we have all modules ready to build our own Vision Transformer. Besides the Transformer encoder, we need the following modules:\n",
        "\n",
        "*   A linear projection layer that maps the input patches to a feature vector of larger size. It is implemented by a simple linear layer that takes each\n",
        " patch independently as input.\n",
        "\n",
        "*   A classification token that is added to the input sequence. We will use the output feature vector of the classification token (CLS token in short) for determining the classification prediction.\n",
        "\n",
        "*   Learnable positional encodings that are added to the tokens before being processed by the Transformer. Those are needed to learn position-dependent information, and convert the set to a sequence. Since we usually work with a fixed resolution, we can learn the positional encodings instead of having the pattern of sine and cosine functions.\n",
        "\n",
        "*   An MLP head that takes the output feature vector of the CLS token, and maps it to a classification prediction. This is usually implemented by a small feed-forward network or even a single linear layer.\n",
        "\n",
        "With those components in mind, let's implement the full Vision Transformer below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlpXwkIf-9Ks"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
        "            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
        "                         within the Transformer\n",
        "            num_channels - Number of channels of the input (3 for RGB)\n",
        "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
        "            num_layers - Number of layers to use in the Transformer\n",
        "            num_classes - Number of classes to predict\n",
        "            patch_size - Number of pixels that the patches have per dimension\n",
        "            num_patches - Maximum number of patches an image can have\n",
        "            dropout - Amount of dropout to apply in the feed-forward network and\n",
        "                      on the input encoding\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Layers/Networks\n",
        "        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Parameters/Embeddings\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Preprocess input\n",
        "        x = img_to_patch(x, self.patch_size)\n",
        "        B, T, _ = x.shape\n",
        "        x = self.input_layer(x)\n",
        "\n",
        "        # Add CLS token and positional encoding\n",
        "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
        "        x = torch.cat([cls_token, x], dim=1)\n",
        "        x = x + self.pos_embedding[:,:T+1]\n",
        "\n",
        "        # Apply Transforrmer\n",
        "        x = self.dropout(x)\n",
        "        x = x.transpose(0, 1)\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Perform classification prediction\n",
        "        cls = x[0]\n",
        "        out = self.mlp_head(cls)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsLxrlIqIWXd"
      },
      "source": [
        "Finally, we can put everything into a PyTorch Lightning Module as usual. We use torch.optim.AdamW as the optimizer, which is Adam with a corrected weight decay implementation. Since we use the Pre-LN Transformer version, we do not need to use a learning rate warmup stage anymore. Instead, we use the same learning rate scheduler as the CNNs on image classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ0no3W5-9My"
      },
      "outputs": [],
      "source": [
        "class ViT(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model_kwargs, lr):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = VisionTransformer(**model_kwargs)\n",
        "        self.example_input_array = next(iter(train_loader))[0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
        "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150], gamma=0.1)\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "    def _calculate_loss(self, batch, mode=\"train\"):\n",
        "        imgs, labels = batch\n",
        "        preds = self.model(imgs)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        self.log(f'{mode}_loss', loss)\n",
        "        self.log(f'{mode}_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self._calculate_loss(batch, mode=\"train\")\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode=\"val\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWaDwwZldn8T"
      },
      "source": [
        "## ViT Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usEUssd9ZD1D"
      },
      "source": [
        "### Inference latency with different layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEn_E-AlVzCK"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def test_inference_time(model, input_tensor, num_runs=100, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    \"\"\"\n",
        "    Test the inference runtime of the VisionTransformer model.\n",
        "\n",
        "    Parameters:\n",
        "        model: VisionTransformer model instance\n",
        "        input_tensor: Input tensor, shape (batch_size, num_channels, height, width)\n",
        "        num_runs: Number of inference runs to compute average time\n",
        "        device: Device to run on ('cuda' or 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        avg_time: Average inference time per run (seconds)\n",
        "    \"\"\"\n",
        "    # Move model and input to the specified device\n",
        "    model = model.to(device)\n",
        "    input_tensor = input_tensor.to(device)\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Warm-up runs to eliminate initial run overhead\n",
        "    with torch.no_grad():\n",
        "        for _ in range(100):\n",
        "            _ = model(input_tensor)\n",
        "\n",
        "    # Record inference time\n",
        "    total_time = 0.0\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_runs):\n",
        "            start_time = time.time()\n",
        "            _ = model(input_tensor)\n",
        "            torch.cuda.synchronize() if device == 'cuda' else None  # Ensure GPU execution is complete\n",
        "            end_time = time.time()\n",
        "            total_time += (end_time - start_time)\n",
        "\n",
        "    # Calculate average inference time\n",
        "    avg_time = total_time / num_runs\n",
        "    print(f\"Average inference time over {num_runs} runs: {avg_time:.6f} seconds\")\n",
        "\n",
        "    return avg_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgXLj4RTV4qP"
      },
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "model_kwargs = {\n",
        "    'embed_dim': 256,\n",
        "    'hidden_dim': 512,\n",
        "    'num_channels': 3,\n",
        "    'num_heads': 8,\n",
        "    'num_layers': 3,\n",
        "    'num_classes': 10,\n",
        "    'patch_size': 4,\n",
        "    'num_patches': 64,\n",
        "    'dropout': 0.2\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "model = VisionTransformer(**model_kwargs)\n",
        "\n",
        "# Create example input tensor (batch_size=32, 3 channels, 32x32 images)\n",
        "batch_size = 32\n",
        "input_tensor = torch.randn(batch_size, 3, 32, 32)\n",
        "\n",
        "# Test inference time\n",
        "avg_time = test_inference_time(model, input_tensor, num_runs=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf28PdOnZMx4"
      },
      "source": [
        "### Inference latency V.S. layer numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2SaR63xYm3H"
      },
      "outputs": [],
      "source": [
        "def test_latency_vs_layers(model_kwargs, input_tensor, layer_range, num_runs=100, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    \"\"\"\n",
        "    Test the inference latency of VisionTransformer with different numbers of layers and plot the results.\n",
        "\n",
        "    Parameters:\n",
        "        model_kwargs: Dictionary of initialization parameters for VisionTransformer\n",
        "        input_tensor: Input tensor, shape (batch_size, num_channels, height, width)\n",
        "        layer_range: Range of layer counts to test (list or range)\n",
        "        num_runs: Number of inference runs per test\n",
        "        device: Device to run on ('cuda' or 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        latencies: List of average inference times for each layer count\n",
        "    \"\"\"\n",
        "    latencies = []\n",
        "\n",
        "    for num_layers in layer_range:\n",
        "        print(f\"\\nTesting with {num_layers} layers...\")\n",
        "        model_kwargs['num_layers'] = num_layers\n",
        "        model = VisionTransformer(**model_kwargs)\n",
        "        avg_time = test_inference_time(model, input_tensor, num_runs, device)\n",
        "        latencies.append(avg_time)\n",
        "\n",
        "    # Plot the chart\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(layer_range, latencies, marker='o', linestyle='-', color='b', label='Inference Latency')\n",
        "    plt.title('Inference Latency vs. Number of Transformer Layers')\n",
        "    plt.xlabel('Number of Layers')\n",
        "    plt.ylabel('Average Inference Time (seconds)')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return latencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtSE3056YrRy"
      },
      "outputs": [],
      "source": [
        "# test different layer number\n",
        "layer_range = range(1, 13)\n",
        "latencies = test_latency_vs_layers(model_kwargs, input_tensor, layer_range, num_runs=1000)\n",
        "\n",
        "for layers, latency in zip(layer_range, latencies):\n",
        "    print(f\"Layers: {layers}, Latency: {latency:.6f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnDswUMzcTTO"
      },
      "source": [
        "### Inference latency V.S. head number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbP74RPyZk9w"
      },
      "outputs": [],
      "source": [
        "def test_latency_vs_heads(model_kwargs, input_tensor, head_range, num_runs=100, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    \"\"\"\n",
        "    Test the inference latency of VisionTransformer with different numbers of attention heads and plot the results.\n",
        "\n",
        "    Parameters:\n",
        "        model_kwargs: Dictionary of initialization parameters for VisionTransformer\n",
        "        input_tensor: Input tensor, shape (batch_size, num_channels, height, width)\n",
        "        head_range: Range of attention head counts to test (list or range)\n",
        "        num_runs: Number of inference runs per test\n",
        "        device: Device to run on ('cuda' or 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        latencies: List of average inference times for each head count\n",
        "        valid_heads: List of valid head counts tested\n",
        "    \"\"\"\n",
        "    embed_dim = model_kwargs['embed_dim']\n",
        "    # Filter valid num_heads to ensure embed_dim is divisible by num_heads\n",
        "    valid_heads = [h for h in head_range if embed_dim % h == 0]\n",
        "    if not valid_heads:\n",
        "        raise ValueError(f\"No valid num_heads in {head_range} can divide embed_dim={embed_dim}\")\n",
        "\n",
        "    latencies = []\n",
        "\n",
        "    for num_heads in valid_heads:\n",
        "        print(f\"\\nTesting with {num_heads} heads...\")\n",
        "        model_kwargs['num_heads'] = num_heads\n",
        "        model = VisionTransformer(**model_kwargs)\n",
        "        avg_time = test_inference_time(model, input_tensor, num_runs, device)\n",
        "        latencies.append(avg_time)\n",
        "\n",
        "    # Plot the chart\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(valid_heads, latencies, marker='o', linestyle='-', color='b', label='Inference Latency')\n",
        "    plt.title('Inference Latency vs. Number of Attention Heads')\n",
        "    plt.xlabel('Number of Attention Heads')\n",
        "    plt.ylabel('Average Inference Time (seconds)')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return latencies, valid_heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSXILExvZtNz"
      },
      "outputs": [],
      "source": [
        "# inference latency with different head range\n",
        "head_range = [2, 4, 8, 16, 32, ]\n",
        "latencies, valid_heads = test_latency_vs_heads(model_kwargs, input_tensor, head_range, num_runs=1000)\n",
        "\n",
        "for heads, latency in zip(valid_heads, latencies):\n",
        "    print(f\"Heads: {heads}, Latency: {latency:.6f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHUgnLR_Ij_X"
      },
      "source": [
        "### ViT Training\n",
        "Commonly, Vision Transformers are applied to large-scale image classification benchmarks such as ImageNet to leverage their full potential. However, here we take a step back and ask: can Vision Transformer also succeed on classical, small benchmarks such as CIFAR10? To find this out, we train a Vision Transformer from scratch on the CIFAR10 dataset. Let’s first create a training function for our PyTorch Lightning module which also loads the pre-trained model if you have downloaded it above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NW2ZwwVy-9PD"
      },
      "outputs": [],
      "source": [
        "def train_model(**kwargs):\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ViT\"),\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=180,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
        "                                    LearningRateMonitor(\"epoch\")])\n",
        "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ViT.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
        "        model = ViT.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
        "    else:\n",
        "        pl.seed_everything(42) # To be reproducable\n",
        "        model = ViT(**kwargs)\n",
        "        trainer.fit(model, train_loader, val_loader)\n",
        "        model = ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
        "\n",
        "    # Test best model on validation and test set\n",
        "    val_result = trainer.test(model, val_loader, verbose=False)\n",
        "    test_result = trainer.test(model, test_loader, verbose=False)\n",
        "    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
        "\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WdWJGFfIotd"
      },
      "source": [
        "Now, we can already start training our model. As seen in our implementation, we have a couple of hyperparameters that we have to set. When creating this notebook, we have performed a small grid search over hyperparameters and listed the best hyperparameters in the cell below. Nevertheless, it is worth discussing the influence that each hyperparameter has, and what intuition we have for choosing its value.\n",
        "\n",
        "First, let's consider the patch size. The smaller we make the patches, the longer the input sequences to the Transformer become. While in general, this allows the Transformer to model more complex functions, it requires a longer computation time due to its quadratic memory usage in the attention layer. Furthermore, small patches can make the task more difficult since the Transformer has to learn which patches are close-by, and which are far away. We experimented with patch sizes of 2, 4, and 8 which gives us the input sequence lengths of 256, 64, and 16 respectively. We found 4 to result in the best performance and hence pick it below.\n",
        "\n",
        "Next, the embedding and hidden dimensionality have a similar impact on a Transformer as to an MLP. The larger the sizes, the more complex the model becomes, and the longer it takes to train. In Transformers, however, we have one more aspect to consider: the query-key sizes in the Multi-Head Attention layers. Each key has the feature dimensionality of embed_dim/num_heads. Considering that we have an input sequence length of 64, a minimum reasonable size for the key vectors is 16 or 32. Lower dimensionalities can restrain the possible attention maps too much. We observed that more than 8 heads are not necessary for the Transformer, and therefore pick an embedding dimensionality of 256. The hidden dimensionality in the feed-forward networks is usually 2-4x larger than the embedding dimensionality, and thus we pick 512.\n",
        "\n",
        "Finally, the learning rate for Transformers is usually relatively small, and in papers, a common value to use is 3e-5. However, since we work with a smaller dataset and have a potentially easier task, we found that we are able to increase the learning rate to 3e-4 without any problems. To reduce overfitting, we use a dropout value of 0.2. Remember that we also use small image augmentations as regularization during training.\n",
        "\n",
        "Feel free to explore the hyperparameters yourself by changing the values below. In general, the Vision Transformer did not show to be too sensitive to the hyperparameter choices on the CIFAR10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8xGv0Kz-9Rc"
      },
      "outputs": [],
      "source": [
        "model, results = train_model(model_kwargs={\n",
        "                                'embed_dim': 256,\n",
        "                                'hidden_dim': 512,\n",
        "                                'num_heads': 8,\n",
        "                                'num_layers': 6,\n",
        "                                'patch_size': 4,\n",
        "                                'num_channels': 3,\n",
        "                                'num_patches': 64,\n",
        "                                'num_classes': 10,\n",
        "                                'dropout': 0.2\n",
        "                            },\n",
        "                            lr=3e-4)\n",
        "print(\"ViT results\", results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL4vPjG2ZF-F"
      },
      "source": [
        "### Change the parameters and retrain the ViT, below is an example of patch size/number of patches\n",
        "###Take away: Transformer training takes a lot of time!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhyqGReFXTxC"
      },
      "outputs": [],
      "source": [
        "# always train\n",
        "def train_model(**kwargs):\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ViT\"),\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=180,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
        "                                    LearningRateMonitor(\"epoch\")])\n",
        "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ViT.ckpt\")\n",
        "    # if os.path.isfile(pretrained_filename):\n",
        "    #     print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
        "    #     model = ViT.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
        "    # else:\n",
        "    pl.seed_everything(42) # To be reproducable\n",
        "    model = ViT(**kwargs)\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "    model = ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
        "\n",
        "    # Test best model on validation and test set\n",
        "    val_result = trainer.test(model, val_loader, verbose=False)\n",
        "    test_result = trainer.test(model, test_loader, verbose=False)\n",
        "    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
        "\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oqy4gHi-Toem"
      },
      "outputs": [],
      "source": [
        "embed_dim_list = [128,256]\n",
        "hidden_dim = [256,512]\n",
        "num_heads_list = [4,8]\n",
        "num_layers=[4,8]\n",
        "patch_num=[[2,256],[8,16]]\n",
        "\n",
        "for i in range(len(patch_num)):\n",
        "  model, results = train_model(model_kwargs={\n",
        "                                  'embed_dim': 256,\n",
        "                                  'hidden_dim': 512,\n",
        "                                  'num_heads': 8,\n",
        "                                  'num_layers': 6,\n",
        "                                  'patch_size': patch_num[i][0],\n",
        "                                  'num_channels': 3,\n",
        "                                  'num_patches': patch_num[i][1],\n",
        "                                  'num_classes': 10,\n",
        "                                  'dropout': 0.2\n",
        "                              },\n",
        "                              lr=3e-4)\n",
        "  print(f\"patch size: {patch_num[i][0]}, patch number: {patch_num[i][1]},  ViT results\", results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wizMVW_Yb8h4"
      },
      "source": [
        "The Vision Transformer achieves a validation and test performance of about 75%. In comparison, almost all CNN architectures that we have tested obtained a classification performance of around 90%. This is a considerable gap and shows that although Vision Transformers perform strongly on ImageNet with potential pretraining, they cannot come close to simple CNNs on CIFAR10 when being trained from scratch. The differences between a CNN and Transformer can be well observed in the training curves.\n",
        "\n",
        "All those observed phenomenons can be explained with a concept that we have visited before: inductive biases. Convolutional Neural Networks have been designed with the assumption that images are translation invariant. Hence, we apply convolutions with shared filters across the image. Furthermore, a CNN architecture integrates the concept of distance in an image: two pixels that are close to each other are more related than two distant pixels. Local patterns are combined into larger patterns until we perform our classification prediction. All those aspects are inductive biases of a CNN. In contrast, a Vision Transformer does not know which two pixels are close to each other, and which are far apart. It has to learn this information solely from the sparse learning signal of the classification task. This is a huge disadvantage when we have a small dataset since such information is crucial for generalizing to an unseen test dataset. With large enough datasets and/or good pre-training, a Transformer can learn this information without the need for inductive biases, and instead is more flexible than a CNN. Especially long-distance relations between local patterns can be difficult to process in CNNs, while in Transformers, all patches have the distance of one. This is why Vision Transformers are so strong on large-scale datasets such as ImageNet but underperform a lot when being applied to a small dataset such as CIFAR10."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}