{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 3: CONVOLUTIONAL NEURAL NETWORKS (CNN)\n",
    "## Machine Learning Hardware Course\n",
    "\n",
    "---\n",
    "\n",
    "## OVERVIEW\n",
    "\n",
    "This lab focuses on Convolutional Neural Networks (CNNs), a specialized type of neural network designed for processing structured grid-like data, particularly images. Building on previous labs, you will implement increasingly complex CNN architectures, visualize learned features, experiment with different hyperparameters, and analyze the computational requirements and hardware efficiency of CNN models. By working through this lab, you will gain a deeper understanding of how CNN architecture choices affect model performance, memory usage, and computational demands.\n",
    "\n",
    "---\n",
    "\n",
    "## LEARNING OBJECTIVES\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Implement and train CNNs with various architectures\n",
    "2. Understand the impact of convolutional layers, pooling operations, and filter sizes on model performance\n",
    "3. Measure and analyze computational requirements of different CNN architectures\n",
    "4. Compare CNNs with Fully Connected Neural Networks (FCNNs) in terms of efficiency and performance\n",
    "5. Apply transfer learning techniques with pre-trained CNN models\n",
    "6. Make informed architectural decisions for CNN models based on performance-efficiency trade-offs\n",
    "7. Visualize and interpret feature maps and filters learned by CNN layers\n",
    "\n",
    "---\n",
    "\n",
    "## TIME ALLOCATION\n",
    "\n",
    "Total time: 2 hours (120 minutes)\n",
    "\n",
    "| Activity | Duration |\n",
    "|----------|----------|\n",
    "| Environment Setup | 10 minute |\n",
    "| CNN Fundamentals | 10 minutes |\n",
    "| Basic CNN Implementation | 20 minutes |\n",
    "| Architectural Exploration | 30 minutes |\n",
    "| Transfer Learning | 20 minutes |\n",
    "| Performance Analysis | 20 minutes |\n",
    "| Feature Visualization | 10 minutes |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: ENVIRONMENT SETUP\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment for the CNN experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, Flatten, Input, Conv2D, MaxPooling2D, AveragePooling2D,\n",
    "    BatchNormalization, GlobalAveragePooling2D\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import VGG16, MobileNetV2\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Check TensorFlow version and GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: CNN FUNDAMENTALS\n",
    "\n",
    "Let's start by understanding the key components of CNN architectures. This section will visualize and explain the fundamental concepts of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Components of CNNs\n",
    "\n",
    "1. **Convolutional Layers**: The core building block\n",
    "   - Apply filters (kernels) to detect spatial patterns\n",
    "   - Parameters: number of filters, filter size, stride, padding\n",
    "\n",
    "2. **Pooling Layers**: Reduce spatial dimensions\n",
    "   - Max Pooling: Take maximum value in a region\n",
    "   - Average Pooling: Take average of values in a region\n",
    "   - Parameters: pool size, stride\n",
    "\n",
    "3. **Activation Functions**: Introduce non-linearity\n",
    "   - ReLU: Most common for CNNs\n",
    "   - Others: LeakyReLU, ELU, etc.\n",
    "\n",
    "4. **Fully Connected Layers**: Usually at the end of the network\n",
    "   - Connect to all activations in the previous layer\n",
    "   - Used for final classification/regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: DATASET PREPARATION\n",
    "\n",
    "We'll use both the MNIST and CIFAR-10 datasets for our CNN experiments. Let's load and prepare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_mnist():\n",
    "    \"\"\"\n",
    "    Load and prepare MNIST dataset for CNN training.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Training, validation, and test data\n",
    "    \"\"\"\n",
    "    # Load MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Reshape for CNN (add channel dimension): (samples, height, width, channels)\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    y_train_encoded = to_categorical(y_train, 10)\n",
    "    y_test_encoded = to_categorical(y_test, 10)\n",
    "    \n",
    "    # Create validation set\n",
    "    val_size = 6000\n",
    "    x_val = x_train[-val_size:]\n",
    "    y_val = y_train_encoded[-val_size:]\n",
    "    x_train_final = x_train[:-val_size]\n",
    "    y_train_final = y_train_encoded[:-val_size]\n",
    "    \n",
    "    print(f\"MNIST shapes:\")\n",
    "    print(f\"  Training set: {x_train_final.shape}\")\n",
    "    print(f\"  Validation set: {x_val.shape}\")\n",
    "    print(f\"  Test set: {x_test.shape}\")\n",
    "    \n",
    "    return (x_train_final, y_train_final), (x_val, y_val), (x_test, y_test_encoded), y_test\n",
    "\n",
    "def load_and_prepare_cifar10():\n",
    "    \"\"\"\n",
    "    Load and prepare CIFAR-10 dataset for CNN training.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Training, validation, and test data\n",
    "    \"\"\"\n",
    "    # Load CIFAR-10 dataset\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.squeeze()\n",
    "    y_train_encoded = to_categorical(y_train, 10)\n",
    "    y_test_encoded = to_categorical(y_test, 10)\n",
    "    \n",
    "    # Create validation set\n",
    "    val_size = 5000\n",
    "    x_val = x_train[-val_size:]\n",
    "    y_val = y_train_encoded[-val_size:]\n",
    "    x_train_final = x_train[:-val_size]\n",
    "    y_train_final = y_train_encoded[:-val_size:]\n",
    "    \n",
    "    print(f\"CIFAR-10 shapes:\")\n",
    "    print(f\"  Training set: {x_train_final.shape}\")\n",
    "    print(f\"  Validation set: {x_val.shape}\")\n",
    "    print(f\"  Test set: {x_test.shape}\")\n",
    "    \n",
    "    return (x_train_final, y_train_final), (x_val, y_val), (x_test, y_test_encoded), y_test\n",
    "\n",
    "# Load datasets\n",
    "print(\"\\nLoading MNIST dataset...\")\n",
    "mnist_data = load_and_prepare_mnist()\n",
    "print(\"\\nLoading CIFAR-10 dataset...\")\n",
    "cifar10_data = load_and_prepare_cifar10()\n",
    "\n",
    "# Unpack the datasets\n",
    "(mnist_train, mnist_y_train), (mnist_val, mnist_y_val), (mnist_test, mnist_y_test), mnist_y_test_raw = mnist_data\n",
    "(cifar_train, cifar_y_train), (cifar_val, cifar_y_val), (cifar_test, cifar_y_test), cifar_y_test_raw = cifar10_data\n",
    "\n",
    "# Define class names\n",
    "mnist_class_names = [str(i) for i in range(10)]\n",
    "cifar10_class_names = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', \n",
    "                      'Dog', 'Frog', 'Horse', 'Ship', 'Truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images\n",
    "def display_sample_images(dataset, labels, class_names, title, num_samples=5):\n",
    "    \"\"\"Display sample images from a dataset.\"\"\"\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    indices = np.random.choice(range(len(dataset)), num_samples, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.subplot(1, num_samples, i+1)\n",
    "        img = dataset[idx]\n",
    "        if img.shape[-1] == 1:  # Grayscale\n",
    "            plt.imshow(img.squeeze(), cmap='gray')\n",
    "        else:  # RGB\n",
    "            plt.imshow(img)\n",
    "        \n",
    "        if labels.ndim > 1:  # One-hot encoded\n",
    "            label_idx = np.argmax(labels[idx])\n",
    "        else:\n",
    "            label_idx = labels[idx]\n",
    "        \n",
    "        plt.title(f\"{class_names[label_idx]}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display samples\n",
    "print(\"\\nDisplaying dataset samples:\")\n",
    "display_sample_images(mnist_train, mnist_y_train, mnist_class_names, \"MNIST Samples\")\n",
    "display_sample_images(cifar_train, cifar_y_train, cifar10_class_names, \"CIFAR-10 Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 3: CONVOLUTIONAL NEURAL NETWORKS (CNN) - PART B\n",
    "## Basic CNN Implementation\n",
    "\n",
    "In this section, we'll implement a basic CNN architecture for both the MNIST and CIFAR-10 datasets, train the models, and evaluate their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4: BASIC CNN IMPLEMENTATION\n",
    "\n",
    "Here we'll create functions to build, train, and evaluate a basic CNN model, then apply it to both MNIST and CIFAR-10 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_cnn(input_shape, num_classes=10):\n",
    "    \"\"\"\n",
    "    Create a basic CNN model for image classification.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input shape (height, width, channels)\n",
    "        num_classes: Number of output classes\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First convolutional block\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, train_data, val_data, test_data, model_name, batch_size=64, epochs=15, patience=3):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model, measuring performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Compiled Keras model\n",
    "        train_data: Tuple of (x_train, y_train)\n",
    "        val_data: Tuple of (x_val, y_val)\n",
    "        test_data: Tuple of (x_test, y_test)\n",
    "        model_name: Name for the model\n",
    "        batch_size: Batch size for training\n",
    "        epochs: Maximum number of epochs\n",
    "        patience: Early stopping patience\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results including metrics\n",
    "    \"\"\"\n",
    "    x_train, y_train = train_data\n",
    "    x_val, y_val = val_data\n",
    "    x_test, y_test = test_data\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=patience,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_val, y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    _ = model.predict(x_test[:1000], verbose=0)\n",
    "    inference_time = (time.time() - start_time) / 1000  # per sample\n",
    "    \n",
    "    # Count parameters\n",
    "    trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    non_trainable_params = np.sum([np.prod(v.shape) for v in model.non_trainable_weights])\n",
    "    total_params = trainable_params + non_trainable_params\n",
    "    \n",
    "    # Calculate efficiency metrics\n",
    "    params_per_second = total_params / training_time\n",
    "    accuracy_per_million_params = test_accuracy * 100 / (total_params / 1e6)\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'history': history,\n",
    "        'training_time': training_time,\n",
    "        'test_accuracy': test_accuracy * 100,  # convert to percentage\n",
    "        'test_loss': test_loss,\n",
    "        'inference_time': inference_time * 1000,  # convert to milliseconds\n",
    "        'total_params': total_params,\n",
    "        'trainable_params': trainable_params,\n",
    "        'params_per_second': params_per_second,\n",
    "        'accuracy_per_million_params': accuracy_per_million_params,\n",
    "        'epochs_trained': len(history.history['accuracy']),\n",
    "        'batch_size': batch_size\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n--- {model_name} Results ---\")\n",
    "    print(f\"Test Accuracy: {results['test_accuracy']:.2f}%\")\n",
    "    print(f\"Training Time: {results['training_time']:.2f} seconds\")\n",
    "    print(f\"Inference Time: {results['inference_time']:.4f} ms\")\n",
    "    print(f\"Total Parameters: {results['total_params']:,}\")\n",
    "    print(f\"Epochs Trained: {results['epochs_trained']}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "    plt.title(f'{model_name} - Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.title(f'{model_name} - Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models on MNIST and CIFAR-10\n",
    "\n",
    "Now we'll create and train our basic CNN models on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a basic CNN on MNIST\n",
    "print(\"\\nTraining basic CNN model on MNIST...\")\n",
    "mnist_input_shape = mnist_train.shape[1:]  # (28, 28, 1)\n",
    "basic_cnn_model = create_basic_cnn(mnist_input_shape)\n",
    "basic_cnn_results, basic_cnn_model = train_and_evaluate_model(\n",
    "    model=basic_cnn_model,\n",
    "    train_data=(mnist_train, mnist_y_train),\n",
    "    val_data=(mnist_val, mnist_y_val),\n",
    "    test_data=(mnist_test, mnist_y_test),\n",
    "    model_name=\"Basic CNN (MNIST)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a basic CNN on CIFAR-10\n",
    "print(\"\\nTraining basic CNN model on CIFAR-10...\")\n",
    "cifar_input_shape = cifar_train.shape[1:]  # (32, 32, 3)\n",
    "cifar_cnn_model = create_basic_cnn(cifar_input_shape)\n",
    "cifar_cnn_results, cifar_cnn_model = train_and_evaluate_model(\n",
    "    model=cifar_cnn_model,\n",
    "    train_data=(cifar_train, cifar_y_train),\n",
    "    val_data=(cifar_val, cifar_y_val),\n",
    "    test_data=(cifar_test, cifar_y_test),\n",
    "    model_name=\"Basic CNN (CIFAR-10)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Model Predictions with Confusion Matrices\n",
    "\n",
    "Let's visualize how our models perform by creating confusion matrices. This will help us identify which classes are most challenging for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrices\n",
    "def plot_confusion_matrix(model, x_test, y_test_true, class_names, title):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix for model predictions.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        x_test: Test inputs\n",
    "        y_test_true: True labels (not one-hot encoded)\n",
    "        class_names: List of class names\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # Generate predictions\n",
    "    y_pred = model.predict(x_test, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test_true, y_pred_classes)\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find most confused pairs\n",
    "    np.fill_diagonal(cm, 0)  # Ignore correct classifications\n",
    "    max_confusion = np.unravel_index(np.argmax(cm), cm.shape)\n",
    "    print(f\"Most confused pair: {class_names[max_confusion[0]]} mistaken for {class_names[max_confusion[1]]} ({cm[max_confusion]} times)\")\n",
    "\n",
    "# Generate confusion matrices\n",
    "print(\"\\nGenerating confusion matrices...\")\n",
    "plot_confusion_matrix(\n",
    "    basic_cnn_model, mnist_test, mnist_y_test_raw, \n",
    "    mnist_class_names, \"MNIST - Basic CNN Confusion Matrix\"\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    cifar_cnn_model, cifar_test, cifar_y_test_raw, \n",
    "    cifar10_class_names, \"CIFAR-10 - Basic CNN Confusion Matrix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 3: CONVOLUTIONAL NEURAL NETWORKS (CNN) - PART C\n",
    "## Architectural Exploration\n",
    "\n",
    "In this section, we'll explore different CNN architectures and hyperparameters to better understand their impact on model performance and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 5: ARCHITECTURAL EXPLORATION\n",
    "\n",
    "Now we'll experiment with different CNN architectures to understand how variations in depth, width, and filter sizes affect performance, training time, and parameter efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_with_config(input_shape, config, num_classes=10):\n",
    "    \"\"\"\n",
    "    Create a CNN model with the specified configuration.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input shape (height, width, channels)\n",
    "        config: Dictionary with model configuration\n",
    "        num_classes: Number of output classes\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add convolutional blocks\n",
    "    for i, block in enumerate(config['conv_blocks']):\n",
    "        # First layer in block (with input shape if it's the first layer)\n",
    "        if i == 0:\n",
    "            model.add(Conv2D(\n",
    "                filters=block['filters'],\n",
    "                kernel_size=block['kernel_size'],\n",
    "                activation=config['activation'],\n",
    "                padding=block.get('padding', 'same'),\n",
    "                input_shape=input_shape\n",
    "            ))\n",
    "        else:\n",
    "            model.add(Conv2D(\n",
    "                filters=block['filters'],\n",
    "                kernel_size=block['kernel_size'],\n",
    "                activation=config['activation'],\n",
    "                padding=block.get('padding', 'same')\n",
    "            ))\n",
    "        \n",
    "        # Add batch normalization if specified\n",
    "        if config.get('batch_norm', False):\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "        # Add pooling layer if specified\n",
    "        if 'pool_size' in block:\n",
    "            if block.get('pool_type', 'max') == 'max':\n",
    "                model.add(MaxPooling2D(pool_size=block['pool_size']))\n",
    "            else:\n",
    "                model.add(AveragePooling2D(pool_size=block['pool_size']))\n",
    "    \n",
    "    # Flatten and add dense layers\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for units in config['dense_units']:\n",
    "        model.add(Dense(units, activation=config['activation']))\n",
    "        if config.get('dropout_rate', 0) > 0:\n",
    "            model.add(Dropout(config['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=config.get('optimizer', 'adam'),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Different CNN Architectures\n",
    "\n",
    "We'll experiment with four different CNN architectures, each with different characteristics:\n",
    "\n",
    "1. **ShallowCNN**: A network with just two convolutional layers\n",
    "2. **DeepCNN**: A deeper network with three convolutional layers\n",
    "3. **WideCNN**: A network with more filters per layer\n",
    "4. **TinyCNN**: A small network with few filters and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different CNN configurations to explore\n",
    "cnn_configurations = [\n",
    "    {\n",
    "        \"name\": \"ShallowCNN\",\n",
    "        \"config\": {\n",
    "            \"conv_blocks\": [\n",
    "                {\"filters\": 16, \"kernel_size\": (3, 3), \"pool_size\": (2, 2)},\n",
    "                {\"filters\": 32, \"kernel_size\": (3, 3), \"pool_size\": (2, 2)}\n",
    "            ],\n",
    "            \"activation\": \"relu\",\n",
    "            \"dense_units\": [64],\n",
    "            \"dropout_rate\": 0.3,\n",
    "            \"optimizer\": \"adam\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DeepCNN\",\n",
    "        \"config\": {\n",
    "            \"conv_blocks\": [\n",
    "                {\"filters\": 32, \"kernel_size\": (3, 3), \"pool_size\": (2, 2)},\n",
    "                {\"filters\": 64, \"kernel_size\": (3, 3), \"pool_size\": (2, 2)},\n",
    "                {\"filters\": 128, \"kernel_size\": (3, 3), \"pool_size\": (2, 2)}\n",
    "            ],\n",
    "            \"activation\": \"relu\",\n",
    "            \"dense_units\": [128],\n",
    "            \"dropout_rate\": 0.3,\n",
    "            \"optimizer\": \"adam\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"WideCNN\",\n",
    "        \"config\": {\n",
    "            \"conv_blocks\": [\n",
    "                {\"filters\": 64, \"kernel_size\": (3, 3), \"pool_size\": (2, 2)},\n",
    "                {\"filters\": 128, \"kernel_size\": (3, 3), \"pool_size\": (2, 2)}\n",
    "            ],\n",
    "            \"activation\": \"relu\",\n",
    "            \"dense_units\": [256],\n",
    "            \"dropout_rate\": 0.3,\n",
    "            \"optimizer\": \"adam\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"TinyCNN\",\n",
    "        \"config\": {\n",
    "            \"conv_blocks\": [\n",
    "                {\"filters\": 8, \"kernel_size\": (3, 3), \"pool_size\": (2, 2)},\n",
    "                {\"filters\": 16, \"kernel_size\": (3, 3), \"pool_size\": (2, 2)}\n",
    "            ],\n",
    "            \"activation\": \"relu\",\n",
    "            \"dense_units\": [32],\n",
    "            \"dropout_rate\": 0.3,\n",
    "            \"optimizer\": \"adam\"\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all configurations on MNIST\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENTING WITH DIFFERENT CNN ARCHITECTURES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "mnist_architecture_results = []\n",
    "mnist_architecture_models = []\n",
    "\n",
    "for config_info in cnn_configurations:\n",
    "    name = config_info[\"name\"]\n",
    "    config = config_info[\"config\"]\n",
    "    \n",
    "    print(f\"\\nTraining {name} on MNIST...\")\n",
    "    model = create_cnn_with_config(mnist_input_shape, config)\n",
    "    \n",
    "    results, trained_model = train_and_evaluate_model(\n",
    "        model=model,\n",
    "        train_data=(mnist_train, mnist_y_train),\n",
    "        val_data=(mnist_val, mnist_y_val),\n",
    "        test_data=(mnist_test, mnist_y_test),\n",
    "        model_name=f\"{name} (MNIST)\"\n",
    "    )\n",
    "    \n",
    "    mnist_architecture_results.append(results)\n",
    "    mnist_architecture_models.append(trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Different Filter Sizes\n",
    "\n",
    "Now, let's experiment with different filter sizes to understand their impact on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different filter sizes on MNIST\n",
    "filter_size_configs = [\n",
    "    {\n",
    "        \"name\": \"SmallFilters\",\n",
    "        \"config\": {\n",
    "            \"conv_blocks\": [\n",
    "                {\"filters\": 32, \"kernel_size\": (2, 2), \"pool_size\": (2, 2)},\n",
    "                {\"filters\": 64, \"kernel_size\": (2, 2), \"pool_size\": (2, 2)}\n",
    "            ],\n",
    "            \"activation\": \"relu\",\n",
    "            \"dense_units\": [128],\n",
    "            \"dropout_rate\": 0.3,\n",
    "            \"optimizer\": \"adam\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"LargeFilters\",\n",
    "        \"config\": {\n",
    "            \"conv_blocks\": [\n",
    "                {\"filters\": 32, \"kernel_size\": (5, 5), \"pool_size\": (2, 2)},\n",
    "                {\"filters\": 64, \"kernel_size\": (5, 5), \"pool_size\": (2, 2)}\n",
    "            ],\n",
    "            \"activation\": \"relu\",\n",
    "            \"dense_units\": [128],\n",
    "            \"dropout_rate\": 0.3,\n",
    "            \"optimizer\": \"adam\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MixedFilters\",\n",
    "        \"config\": {\n",
    "            \"conv_blocks\": [\n",
    "                {\"filters\": 32, \"kernel_size\": (3, 3), \"pool_size\": (2, 2)},\n",
    "                {\"filters\": 64, \"kernel_size\": (5, 5), \"pool_size\": (2, 2)}\n",
    "            ],\n",
    "            \"activation\": \"relu\",\n",
    "            \"dense_units\": [128],\n",
    "            \"dropout_rate\": 0.3,\n",
    "            \"optimizer\": \"adam\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENTING WITH DIFFERENT FILTER SIZES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train filter size configurations on MNIST\n",
    "for config_info in filter_size_configs:\n",
    "    name = config_info[\"name\"]\n",
    "    config = config_info[\"config\"]\n",
    "    \n",
    "    print(f\"\\nTraining {name} on MNIST...\")\n",
    "    model = create_cnn_with_config(mnist_input_shape, config)\n",
    "    \n",
    "    results, trained_model = train_and_evaluate_model(\n",
    "        model=model,\n",
    "        train_data=(mnist_train, mnist_y_train),\n",
    "        val_data=(mnist_val, mnist_y_val),\n",
    "        test_data=(mnist_test, mnist_y_test),\n",
    "        model_name=f\"{name} (MNIST)\"\n",
    "    )\n",
    "    \n",
    "    mnist_architecture_results.append(results)\n",
    "    mnist_architecture_models.append(trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Results of Different Architectures\n",
    "\n",
    "Let's create a comparison table and visualize the results of our architectural exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results of different architectures\n",
    "architecture_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': result['model_name'],\n",
    "        'Accuracy (%)': result['test_accuracy'],\n",
    "        'Training Time (s)': result['training_time'],\n",
    "        'Inference Time (ms)': result['inference_time'],\n",
    "        'Parameters': result['total_params'],\n",
    "        'Params/Second': result['params_per_second'],\n",
    "        'Accuracy/Million Params': result['accuracy_per_million_params']\n",
    "    }\n",
    "    for result in mnist_architecture_results\n",
    "])\n",
    "\n",
    "print(\"\\nMNIST Architecture Comparison:\")\n",
    "print(architecture_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Accuracy vs Parameters\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(architecture_df['Parameters'], architecture_df['Accuracy (%)'], s=100)\n",
    "for i, row in architecture_df.iterrows():\n",
    "    plt.annotate(row['Model'].replace(' (MNIST)', ''), \n",
    "                 (row['Parameters'], row['Accuracy (%)']),\n",
    "                 xytext=(5, 5), textcoords='offset points')\n",
    "plt.title('Accuracy vs Parameters')\n",
    "plt.xlabel('Number of Parameters')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Inference Time vs Parameters\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(architecture_df['Parameters'], architecture_df['Inference Time (ms)'], s=100)\n",
    "for i, row in architecture_df.iterrows():\n",
    "    plt.annotate(row['Model'].replace(' (MNIST)', ''), \n",
    "                 (row['Parameters'], row['Inference Time (ms)']),\n",
    "                 xytext=(5, 5), textcoords='offset points')\n",
    "plt.title('Inference Time vs Parameters')\n",
    "plt.xlabel('Number of Parameters')\n",
    "plt.ylabel('Inference Time (ms)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Accuracy vs Training Time\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(architecture_df['Training Time (s)'], architecture_df['Accuracy (%)'], s=100)\n",
    "for i, row in architecture_df.iterrows():\n",
    "    plt.annotate(row['Model'].replace(' (MNIST)', ''), \n",
    "                 (row['Training Time (s)'], row['Accuracy (%)']),\n",
    "                 xytext=(5, 5), textcoords='offset points')\n",
    "plt.title('Accuracy vs Training Time')\n",
    "plt.xlabel('Training Time (s)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Accuracy/Million Parameters \n",
    "plt.subplot(2, 2, 4)\n",
    "plt.bar(architecture_df['Model'].str.replace(' (MNIST)', ''), architecture_df['Accuracy/Million Params'])\n",
    "plt.title('Accuracy per Million Parameters')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Accuracy/Million Params')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Architectural Experiments\n",
    "\n",
    "From our architectural experiments, we can observe several key patterns:\n",
    "\n",
    "1. **Depth vs Width**: The DeepCNN and WideCNN models typically achieve higher accuracy than the ShallowCNN, but require more training time and have more parameters.\n",
    "\n",
    "2. **Parameter Efficiency**: The TinyCNN model, while having the lowest raw accuracy, achieves the highest accuracy per million parameters, making it a good choice for resource-constrained environments.\n",
    "\n",
    "3. **Filter Size Impact**: Larger filter sizes (5×5) capture more context but require more computations and parameters compared to smaller filters (2×2). The mixed filter approach (3×3 followed by 5×5) often provides a good balance between accuracy and efficiency.\n",
    "\n",
    "4. **Training Time**: The training time is roughly proportional to the number of parameters in the model, but the relationship is not perfectly linear due to GPU parallelization.\n",
    "\n",
    "5. **Inference Time**: Similarly, inference time increases with model complexity, which is an important consideration for real-time applications.\n",
    "\n",
    "These observations demonstrate the importance of considering trade-offs between model accuracy, size, and computational requirements when designing CNN architectures for practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 3: CONVOLUTIONAL NEURAL NETWORKS (CNN) - PART D\n",
    "## Transfer Learning and Performance Analysis\n",
    "\n",
    "In this final section, we'll explore transfer learning techniques using pre-trained CNN models, and perform a comprehensive analysis of the performance and efficiency of all our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 6: TRANSFER LEARNING\n",
    "\n",
    "Transfer learning allows us to leverage the feature extraction capabilities of models pre-trained on large datasets. We'll use MobileNetV2, a model pre-trained on ImageNet, and adapt it for our CIFAR-10 classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRANSFER LEARNING EXPERIMENTS\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transfer_learning_model(base_model, input_shape, num_classes=10, freeze_base=True):\n",
    "    \"\"\"\n",
    "    Create a transfer learning model using a pre-trained base model.\n",
    "    \n",
    "    Args:\n",
    "        base_model: Pre-trained model to use as base\n",
    "        input_shape: Input shape for the model\n",
    "        num_classes: Number of output classes\n",
    "        freeze_base: Whether to freeze the base model weights\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Freeze base model if requested\n",
    "    base_model.trainable = not freeze_base\n",
    "    \n",
    "    # Create input layer with required input shape\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Resize input if needed (e.g., for MNIST)\n",
    "    # Most pre-trained models expect RGB images of specific sizes\n",
    "    resized_inputs = inputs\n",
    "    if input_shape != base_model.input_shape[1:]:\n",
    "        # Add preprocessing to match expected input\n",
    "        if input_shape[-1] == 1:  # Grayscale to RGB\n",
    "            # Repeat the single channel three times\n",
    "            resized_inputs = tf.keras.layers.Lambda(\n",
    "                lambda x: tf.repeat(x, 3, axis=-1)\n",
    "            )(inputs)\n",
    "        \n",
    "        # Resize to expected dimensions\n",
    "        resized_inputs = tf.keras.layers.Lambda(\n",
    "            lambda x: tf.image.resize(x, base_model.input_shape[1:3])\n",
    "        )(resized_inputs)\n",
    "    \n",
    "    # Pass through base model\n",
    "    x = base_model(resized_inputs, training=False)\n",
    "    \n",
    "    # Add classification head\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNetV2 model (without top layers)\n",
    "print(\"\\nLoading pre-trained MobileNetV2 model...\")\n",
    "mobilenet_base = MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# Create transfer learning model for CIFAR-10\n",
    "print(\"\\nCreating transfer learning model for CIFAR-10...\")\n",
    "mobilenet_cifar = create_transfer_learning_model(\n",
    "    base_model=mobilenet_base,\n",
    "    input_shape=cifar_input_shape,\n",
    "    num_classes=10,\n",
    "    freeze_base=True\n",
    ")\n",
    "\n",
    "# Train the transfer learning model on CIFAR-10\n",
    "print(\"\\nTraining transfer learning model on CIFAR-10...\")\n",
    "mobilenet_results, mobilenet_model = train_and_evaluate_model(\n",
    "    model=mobilenet_cifar,\n",
    "    train_data=(cifar_train, cifar_y_train),\n",
    "    val_data=(cifar_val, cifar_y_val),\n",
    "    test_data=(cifar_test, cifar_y_test),\n",
    "    model_name=\"MobileNetV2 Transfer (CIFAR-10)\",\n",
    "    epochs=10  # Faster convergence with transfer learning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning the Pre-trained Model\n",
    "\n",
    "Now, let's take our transfer learning approach a step further by fine-tuning the top layers of the pre-trained model. This allows the model to adapt its pre-learned features to our specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fine_tuned_model(base_model, input_shape, num_classes=10, unfreeze_layers=5):\n",
    "    \"\"\"\n",
    "    Create a fine-tuned model by unfreezing some layers of a pre-trained model.\n",
    "    \n",
    "    Args:\n",
    "        base_model: Pre-trained model to use as base\n",
    "        input_shape: Input shape for the model\n",
    "        num_classes: Number of output classes\n",
    "        unfreeze_layers: Number of top layers to unfreeze\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Freeze all layers initially\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Unfreeze the last few layers\n",
    "    for layer in base_model.layers[-unfreeze_layers:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Create input layer with required input shape\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Resize input if needed\n",
    "    resized_inputs = inputs\n",
    "    if input_shape != base_model.input_shape[1:]:\n",
    "        # Add preprocessing to match expected input\n",
    "        if input_shape[-1] == 1:  # Grayscale to RGB\n",
    "            resized_inputs = tf.keras.layers.Lambda(\n",
    "                lambda x: tf.repeat(x, 3, axis=-1)\n",
    "            )(inputs)\n",
    "        \n",
    "        # Resize to expected dimensions\n",
    "        resized_inputs = tf.keras.layers.Lambda(\n",
    "            lambda x: tf.image.resize(x, base_model.input_shape[1:3])\n",
    "        )(resized_inputs)\n",
    "    \n",
    "    # Pass through base model\n",
    "    x = base_model(resized_inputs, training=True)\n",
    "    \n",
    "    # Add classification head\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile model with lower learning rate for fine-tuning\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Count trainable vs non-trainable parameters\n",
    "    trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    non_trainable_params = np.sum([np.prod(v.shape) for v in model.non_trainable_weights])\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params:,}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fine-tuned model for CIFAR-10\n",
    "print(\"\\nCreating fine-tuned model for CIFAR-10...\")\n",
    "fine_tuned_cifar = create_fine_tuned_model(\n",
    "    base_model=mobilenet_base,\n",
    "    input_shape=cifar_input_shape,\n",
    "    num_classes=10,\n",
    "    unfreeze_layers=10\n",
    ")\n",
    "\n",
    "# Train the fine-tuned model on CIFAR-10\n",
    "print(\"\\nTraining fine-tuned model on CIFAR-10...\")\n",
    "fine_tuned_results, fine_tuned_model = train_and_evaluate_model(\n",
    "    model=fine_tuned_cifar,\n",
    "    train_data=(cifar_train, cifar_y_train),\n",
    "    val_data=(cifar_val, cifar_y_val),\n",
    "    test_data=(cifar_test, cifar_y_test),\n",
    "    model_name=\"MobileNetV2 Fine-tuned (CIFAR-10)\",\n",
    "    epochs=5,  # Fewer epochs to prevent overfitting\n",
    "    batch_size=32  # Smaller batch size for fine-tuning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Transfer Learning Results\n",
    "\n",
    "Let's compare the performance of our basic CNN with the transfer learning and fine-tuning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the transfer learning results\n",
    "transfer_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': result['model_name'],\n",
    "        'Accuracy (%)': result['test_accuracy'],\n",
    "        'Training Time (s)': result['training_time'],\n",
    "        'Inference Time (ms)': result['inference_time'],\n",
    "        'Parameters': result['total_params'],\n",
    "        'Trainable Parameters': result['trainable_params'],\n",
    "        'Accuracy/Million Params': result['accuracy_per_million_params']\n",
    "    }\n",
    "    for result in [cifar_cnn_results, mobilenet_results, fine_tuned_results]\n",
    "])\n",
    "\n",
    "print(\"\\nTransfer Learning Comparison:\")\n",
    "print(transfer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Model Predictions\n",
    "\n",
    "Let's visualize the predictions of our fine-tuned model on some test images to get a better understanding of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions from transfer learning model\n",
    "def display_predictions(model, images, true_labels, class_names, title, num_samples=5):\n",
    "    \"\"\"Display sample images with predictions.\"\"\"\n",
    "    indices = np.random.choice(range(len(images)), num_samples, replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.subplot(1, num_samples, i+1)\n",
    "        plt.imshow(images[idx])\n",
    "        \n",
    "        # Get prediction\n",
    "        pred = model.predict(np.expand_dims(images[idx], axis=0), verbose=0)\n",
    "        pred_class = np.argmax(pred)\n",
    "        true_class = true_labels[idx]\n",
    "        \n",
    "        # Color based on correct/incorrect\n",
    "        color = 'green' if pred_class == true_class else 'red'\n",
    "        \n",
    "        plt.title(f\"True: {class_names[true_class]}\\nPred: {class_names[pred_class]}\", color=color)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display predictions from the fine-tuned model\n",
    "print(\"\\nDisplaying predictions from fine-tuned model:\")\n",
    "display_predictions(\n",
    "    fine_tuned_model, \n",
    "    cifar_test, \n",
    "    cifar_y_test_raw, \n",
    "    cifar10_class_names, \n",
    "    \"Fine-tuned MobileNetV2 Predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 7: PERFORMANCE ANALYSIS\n",
    "\n",
    "Let's compare the performance of all the models we've implemented throughout this lab to draw conclusions about CNN architecture design and transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Combine all results (CNN architectures and transfer learning)\n",
    "all_results = mnist_architecture_results + [basic_cnn_results, cifar_cnn_results, mobilenet_results, fine_tuned_results]\n",
    "\n",
    "# Create comprehensive results table\n",
    "all_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': result['model_name'],\n",
    "        'Dataset': 'MNIST' if 'MNIST' in result['model_name'] else 'CIFAR-10',\n",
    "        'Accuracy (%)': result['test_accuracy'],\n",
    "        'Training Time (s)': result['training_time'],\n",
    "        'Inference Time (ms)': result['inference_time'],\n",
    "        'Parameters': result['total_params'],\n",
    "        'Trainable Params': result.get('trainable_params', result['total_params']),\n",
    "        'Params/Second': result['params_per_second'],\n",
    "        'Accuracy/Million Params': result['accuracy_per_million_params']\n",
    "    }\n",
    "    for result in all_results\n",
    "])\n",
    "\n",
    "print(\"\\nAll Models Comparison:\")\n",
    "print(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing CNN vs FCNN\n",
    "\n",
    "Let's compare our CNN models with a fully connected neural network (FCNN) to see the benefits of CNNs for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare CNN vs FCNN from Lab 2 (we'll simulate this here)\n",
    "fcnn_results = {\n",
    "    'model_name': 'Best FCNN (MNIST)',\n",
    "    'test_accuracy': 98.5,\n",
    "    'training_time': 45.0,\n",
    "    'inference_time': 0.06,\n",
    "    'total_params': 270000,\n",
    "    'params_per_second': 6000,\n",
    "    'accuracy_per_million_params': 365.0\n",
    "}\n",
    "\n",
    "# Find best CNN for MNIST\n",
    "best_mnist_cnn = all_df[all_df['Dataset'] == 'MNIST'].sort_values('Accuracy (%)', ascending=False).iloc[0]\n",
    "\n",
    "# Compare CNN vs FCNN\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': fcnn_results['model_name'],\n",
    "        'Accuracy (%)': fcnn_results['test_accuracy'],\n",
    "        'Training Time (s)': fcnn_results['training_time'],\n",
    "        'Inference Time (ms)': fcnn_results['inference_time'],\n",
    "        'Parameters': fcnn_results['total_params'],\n",
    "        'Accuracy/Million Params': fcnn_results['accuracy_per_million_params']\n",
    "    },\n",
    "    {\n",
    "        'Model': best_mnist_cnn['Model'],\n",
    "        'Accuracy (%)': best_mnist_cnn['Accuracy (%)'],\n",
    "        'Training Time (s)': best_mnist_cnn['Training Time (s)'],\n",
    "        'Inference Time (ms)': best_mnist_cnn['Inference Time (ms)'],\n",
    "        'Parameters': best_mnist_cnn['Parameters'],\n",
    "        'Accuracy/Million Params': best_mnist_cnn['Accuracy/Million Params']\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\nCNN vs FCNN Comparison:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Normalized metrics for better comparison (relative to each other)\n",
    "metrics = ['Accuracy (%)', 'Training Time (s)', 'Inference Time (ms)', 'Parameters', 'Accuracy/Million Params']\n",
    "models = comparison_df['Model'].tolist()\n",
    "\n",
    "# Normalize values\n",
    "normalized_df = comparison_df.copy()\n",
    "for metric in metrics:\n",
    "    if metric in ['Accuracy (%)', 'Accuracy/Million Params']:  # Higher is better\n",
    "        normalized_df[metric] = comparison_df[metric] / comparison_df[metric].max()\n",
    "    else:  # Lower is better\n",
    "        normalized_df[metric] = comparison_df[metric].min() / comparison_df[metric]\n",
    "\n",
    "# Plot normalized metrics\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(metrics))\n",
    "\n",
    "plt.bar(index, normalized_df.iloc[0][metrics], bar_width, label=models[0])\n",
    "plt.bar(index + bar_width, normalized_df.iloc[1][metrics], bar_width, label=models[1])\n",
    "\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Normalized Score (higher is better)')\n",
    "plt.title('CNN vs FCNN Comparison')\n",
    "plt.xticks(index + bar_width / 2, metrics, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the Best Models for Different Criteria\n",
    "\n",
    "Different applications have different requirements. Let's identify which models perform best according to various criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best model for different criteria\n",
    "best_accuracy = all_df.loc[all_df['Accuracy (%)'].idxmax()]\n",
    "print(f\"\\nBest Model for Accuracy: {best_accuracy['Model']} ({best_accuracy['Accuracy (%)']:.2f}%)\")\n",
    "\n",
    "fastest_inference = all_df.loc[all_df['Inference Time (ms)'].idxmin()]\n",
    "print(f\"Best Model for Inference Speed: {fastest_inference['Model']} ({fastest_inference['Inference Time (ms)']:.4f} ms)\")\n",
    "\n",
    "most_efficient = all_df.loc[all_df['Accuracy/Million Params'].idxmax()]\n",
    "print(f\"Most Parameter-Efficient Model: {most_efficient['Model']} ({most_efficient['Accuracy/Million Params']:.2f} accuracy/M params)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 3: CONVOLUTIONAL NEURAL NETWORKS (CNN) - PART E (OPTIONAL)\n",
    "## Feature Visualization\n",
    "\n",
    "In this section, we'll visualize what our CNN models are learning in their convolutional layers. This will help us understand how CNNs extract and process features from images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 8: FEATURE VISUALIZATION (OPTIONAL)\n",
    "\n",
    "One of the powerful aspects of CNNs is their ability to learn hierarchical features - from simple edges in early layers to complex shapes in deeper layers. Let's visualize these learned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VISUALIZING CNN FEATURES\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Convolutional Filters\n",
    "\n",
    "First, let's visualize the filters (kernels) in the convolutional layers. These filters are what the network uses to detect different patterns in the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_filters(model, layer_name=None):\n",
    "    \"\"\"\n",
    "    Visualize the filters/kernels in a convolutional layer.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        layer_name: Name of layer to visualize (if None, use first Conv2D layer)\n",
    "    \"\"\"\n",
    "    # Get the layer of interest\n",
    "    if layer_name is None:\n",
    "        # Find the first convolutional layer\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "                layer_name = layer.name\n",
    "                break\n",
    "    \n",
    "    # Get the weights\n",
    "    for layer in model.layers:\n",
    "        if layer.name == layer_name:\n",
    "            weights, biases = layer.get_weights()\n",
    "            break\n",
    "    \n",
    "    # Normalize weights for better visualization\n",
    "    weights_min, weights_max = np.min(weights), np.max(weights)\n",
    "    weights = (weights - weights_min) / (weights_max - weights_min)\n",
    "    \n",
    "    # Number of filters and their dimensions\n",
    "    n_filters = weights.shape[3]\n",
    "    \n",
    "    # Compute grid size\n",
    "    grid_size = int(np.ceil(np.sqrt(n_filters)))\n",
    "    \n",
    "    # Create figure with subplot grid\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    # Plot each filter\n",
    "    for i in range(n_filters):\n",
    "        if i < grid_size * grid_size:  # Ensure we don't exceed the grid\n",
    "            plt.subplot(grid_size, grid_size, i+1)\n",
    "            # Depending on the input shape, filters might have different channel dimensions\n",
    "            if weights.shape[2] == 1:  # Grayscale\n",
    "                plt.imshow(weights[:, :, 0, i], cmap='viridis')\n",
    "            else:  # RGB - take mean across channels for visualization\n",
    "                plt.imshow(np.mean(weights[:, :, :, i], axis=2), cmap='viridis')\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Filters in layer: {layer_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize filters from our trained CNN models\n",
    "print(\"\\nVisualizing filters from Basic CNN (MNIST):\")\n",
    "visualize_filters(basic_cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Feature Maps\n",
    "\n",
    "Now, let's visualize the feature maps (activations) produced by the convolutional layers when given a specific input image. This shows us what patterns the network is detecting in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_maps(model, image, layer_names=None):\n",
    "    \"\"\"\n",
    "    Visualize feature maps from convolutional layers for a given input image.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        image: Input image to visualize feature maps for\n",
    "        layer_names: List of layer names to visualize (if None, use all Conv2D layers)\n",
    "    \"\"\"\n",
    "    # If no layer names provided, find all convolutional layers\n",
    "    if layer_names is None:\n",
    "        layer_names = []\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "                layer_names.append(layer.name)\n",
    "    \n",
    "    # First, we need to get the activations directly using a function\n",
    "    # This approach works for both Sequential and Functional models\n",
    "    feature_maps = []\n",
    "    for layer_name in layer_names:\n",
    "        # Create a function that returns the output of the target layer\n",
    "        feature_extractor = tf.keras.models.Model(\n",
    "            inputs=model.inputs,\n",
    "            outputs=model.get_layer(layer_name).output\n",
    "        )\n",
    "        \n",
    "        # Get the feature map by passing the image through the model\n",
    "        feature_map = feature_extractor.predict(np.expand_dims(image, axis=0), verbose=0)\n",
    "        feature_maps.append((layer_name, feature_map))\n",
    "    \n",
    "    # Plot feature maps\n",
    "    for layer_name, feature_map in feature_maps:\n",
    "        # Get the feature map from the batch\n",
    "        feature_map = feature_map[0]\n",
    "        \n",
    "        # Number of features in this layer\n",
    "        n_features = feature_map.shape[-1]\n",
    "        \n",
    "        # Compute grid size\n",
    "        grid_size = int(np.ceil(np.sqrt(n_features)))\n",
    "        \n",
    "        # Limit to max 64 feature maps for visibility\n",
    "        display_features = min(n_features, 64)\n",
    "        display_grid_size = int(np.ceil(np.sqrt(display_features)))\n",
    "        \n",
    "        # Create figure with subplot grid\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        \n",
    "        # Plot each feature map\n",
    "        for i in range(display_features):\n",
    "            if i < display_grid_size * display_grid_size:  # Ensure we don't exceed the grid\n",
    "                plt.subplot(display_grid_size, display_grid_size, i+1)\n",
    "                plt.imshow(feature_map[:, :, i], cmap='viridis')\n",
    "                plt.axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Feature Maps in layer: {layer_name} (showing {display_features} of {n_features} features)')\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.95)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting and Visualizing a Sample Image\n",
    "\n",
    "Let's select a random image from our test set and visualize how the CNN processes it through its layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature maps for a sample image\n",
    "sample_idx = np.random.randint(0, len(mnist_test))\n",
    "sample_image = mnist_test[sample_idx]\n",
    "sample_label = np.argmax(mnist_y_test[sample_idx])\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(sample_image.squeeze(), cmap='gray')\n",
    "plt.title(f\"Sample Image (Digit: {sample_label})\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualizing feature maps for the sample image:\")\n",
    "visualize_feature_maps(basic_cnn_model, sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Feature Maps Across Different Architectures\n",
    "\n",
    "Now, let's compare how feature maps differ across our different CNN architectures. This will give us insight into how architectural choices affect feature learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activations in a different CNN architecture\n",
    "# Pick the DeepCNN model (index 1 in our architecture list)\n",
    "deep_cnn_model = mnist_architecture_models[1]\n",
    "deep_cnn_name = mnist_architecture_results[1]['model_name']\n",
    "\n",
    "print(f\"\\nVisualizing feature maps for {deep_cnn_name}:\")\n",
    "visualize_feature_maps(deep_cnn_model, sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on Feature Maps\n",
    "\n",
    "Let's analyze what we've observed in the visualizations:\n",
    "\n",
    "1. **First Layer Feature Maps**: The first convolutional layer typically detects low-level features like edges and textures. In the MNIST dataset, these often correspond to parts of digit strokes.\n",
    "\n",
    "2. **Deeper Layer Feature Maps**: As we move deeper into the network, feature maps become more abstract and specialized. Some neurons might activate specifically for loops (as in digits 6, 8, 9), while others might activate for vertical lines (as in digits 1, 4, 7).\n",
    "\n",
    "3. **Architectural Differences**: \n",
    "   - The DeepCNN model has more convolutional layers, allowing it to create more hierarchical representations\n",
    "   - Networks with more filters (like WideCNN) can capture a broader range of features at each level\n",
    "   - Smaller networks (like TinyCNN) must be more efficient with their limited capacity, often focusing on the most discriminative features\n",
    "\n",
    "4. **Feature Sparsity**: Notice that many feature maps show sparse activation - only a small portion of the map lights up for a given input. This is a sign of specialized feature detectors.\n",
    "\n",
    "5. **Complementary Features**: Different feature maps in the same layer often detect complementary features, working together to fully represent the input image.\n",
    "\n",
    "These visualizations help us understand why CNNs are so effective for image recognition tasks - they automatically learn a hierarchy of increasingly complex and task-relevant features directly from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION\n",
    "\n",
    "In this lab, we've explored Convolutional Neural Networks (CNNs) and their applications in image classification. We've implemented various CNN architectures, visualized learned features, experimented with transfer learning, and analyzed the computational efficiency of different models.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **CNN Architecture Design**:\n",
    "   - Deeper networks generally achieve higher accuracy but require more computation\n",
    "   - Wider networks (more filters per layer) can capture more diverse features\n",
    "   - Filter size affects the receptive field and the type of features detected\n",
    "   - Parameter efficiency is important for deployment on resource-constrained devices\n",
    "\n",
    "2. **CNN vs FCNN**:\n",
    "   - CNNs are significantly more parameter-efficient than FCNNs for image tasks\n",
    "   - CNNs can achieve higher accuracy with fewer parameters due to weight sharing and local connectivity\n",
    "   - CNNs naturally encode the spatial structure of images, which FCNNs lose\n",
    "\n",
    "3. **Feature Learning**:\n",
    "   - Early layers learn simple features like edges and textures\n",
    "   - Deeper layers combine these to detect more complex, task-specific patterns\n",
    "   - Visualizing filters and feature maps helps us understand what the network is learning\n",
    "\n",
    "4. **Transfer Learning**:\n",
    "   - Pre-trained models significantly boost performance on smaller datasets\n",
    "   - Fine-tuning adapts pre-learned features to specific tasks\n",
    "   - Transfer learning reduces training time and data requirements\n",
    "\n",
    "5. **Performance Analysis**:\n",
    "   - There's a trade-off between accuracy, model size, and inference speed\n",
    "   - Different applications may prioritize different metrics (accuracy, speed, efficiency)\n",
    "   - The best model depends on the specific constraints and requirements of the application\n",
    "\n",
    "These findings can guide your future work with CNN models, helping you make informed architectural decisions based on your specific needs and constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADDITIONAL CHALLENGES (Optional)\n",
    "\n",
    "If you complete the lab early or want to explore further, try these extensions:\n",
    "\n",
    "1. Implement data augmentation (rotation, flipping, etc.) to improve model performance\n",
    "2. Experiment with different pooling strategies (max vs. average)\n",
    "3. Try implementing a more complex CNN architecture like ResNet or Inception\n",
    "4. Apply your CNN models to a different dataset (e.g., CIFAR-100, STL-10)\n",
    "5. Implement Grad-CAM for visualizing which parts of the image are important for classification\n",
    "6. Explore model quantization to reduce model size and improve inference speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save results to Google Drive (optional)\n",
    "\n",
    "# # Mount Google Drive (optional, for saving results)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !mkdir -p \"/content/drive/My Drive/ML_Hardware_Course/Lab3\"\n",
    "# results_path = \"/content/drive/My Drive/ML_Hardware_Course/Lab3/cnn_results.csv\"\n",
    "# all_df.to_csv(results_path, index=False)\n",
    "# print(f\"Results saved to {results_path}\")\n",
    "\n",
    "# # Save the best model (optional)\n",
    "# best_model_path = \"/content/drive/My Drive/ML_Hardware_Course/Lab3/best_cnn_model.h5\"\n",
    "# best_idx = all_df['Accuracy (%)'].idxmax()\n",
    "# best_model_name = all_df.iloc[best_idx]['Model']\n",
    "\n",
    "# if \"Transfer\" in best_model_name or \"Fine-tuned\" in best_model_name:\n",
    "#     if \"Transfer\" in best_model_name:\n",
    "#         mobilenet_model.save(best_model_path)\n",
    "#     else:\n",
    "#         fine_tuned_model.save(best_model_path)\n",
    "# elif \"CIFAR\" in best_model_name:\n",
    "#     cifar_cnn_model.save(best_model_path)\n",
    "# else:  # MNIST architecture\n",
    "#     arch_idx = [i for i, r in enumerate(mnist_architecture_results) if r['model_name'] == best_model_name]\n",
    "#     if arch_idx:\n",
    "#         mnist_architecture_models[arch_idx[0]].save(best_model_path)\n",
    "#     else:\n",
    "#         basic_cnn_model.save(best_model_path)\n",
    "\n",
    "# print(f\"Best model saved to {best_model_path}\")\n",
    "\n",
    "print(\"\\nLab 3 completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
